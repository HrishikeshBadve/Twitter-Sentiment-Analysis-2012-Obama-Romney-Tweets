{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ec00867a-7c6d-49bf-accc-f948d3859e04"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "e305700f-495b-4a9c-c72a-8ffb460266d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "698e7631-906c-41e2-8f8c-57f0217bbb7a"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "from statistics import mean, stdev, median, mode\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from mittens import GloVe, Mittens\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import MaxPool3D, AveragePooling3D\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling3D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "a0b2d8da-0ebb-4d47-c6ca-1398fdd3a7ed"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BALelbZyNU02",
    "outputId": "c87872db-d3e0-44f8-b6a0-9fa931a3715e"
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "### Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pr_df = pd.read_csv(os.path.join(data_path, 'Combined Training Data.csv'), usecols = [1,2]).dropna()\n",
    "val_pr_df = pd.read_csv(os.path.join(data_path, 'Combined Validation Data.csv'), usecols = [1,2]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "AD9U00CVddNC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes obama</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>allow contraceptive say</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama take day study next debate know anything...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>say much good thing obama small business owner...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>debate romney via</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0                                          yes obama  Positive\n",
       "1                            allow contraceptive say   Neutral\n",
       "2  obama take day study next debate know anything...  Negative\n",
       "3  say much good thing obama small business owner...  Positive\n",
       "4                                  debate romney via  Positive"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>scary disturb story reveal real mitt romney</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>lady pay attention tn romney condemn single mo...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>barack obama number unemployed woman increase ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fact check support president romney romney con...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bad tom friedman obama weakness next four year...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0        scary disturb story reveal real mitt romney  Negative\n",
       "1  lady pay attention tn romney condemn single mo...  Negative\n",
       "2  barack obama number unemployed woman increase ...  Negative\n",
       "3  fact check support president romney romney con...  Negative\n",
       "4  bad tom friedman obama weakness next four year...  Negative"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train1_pr_df = pd.read_csv(os.path.join(data_path, 'Combined Training1 Data.csv'), usecols = [1,2]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debate job come back obama get as make corp ta...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>romney romney abrasive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sicken obama war woman tom halloran</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney business genius speculator play bi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day election day ready cheer president obama t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  debate job come back obama get as make corp ta...  Negative\n",
       "1                             romney romney abrasive  Negative\n",
       "2                sicken obama war woman tom halloran  Negative\n",
       "3  mitt romney business genius speculator play bi...  Negative\n",
       "4  day election day ready cheer president obama t...  Positive"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train1_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "FiqzWfYwa-ir"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "# embedded_sequences = embedding_layer(int_sequences_input)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.GlobalMaxPooling1D()(x)\n",
    "# x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "# model = keras.Model(int_sequences_input, preds)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "Nt4Q-LLu_Nh6"
   },
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    data = np.asarray(data)\n",
    "    temp = np.zeros((len(data),3))\n",
    "#     print(data[0])\n",
    "    for i in range(len(temp)):\n",
    "        if data[i] == 'Negative':\n",
    "            temp[i][2] = 1 ## Negative sentiment third neuron\n",
    "        elif data[i] == 'Neutral':\n",
    "            temp[i][1] = 1 ## Neutral sentiment second neuron  \n",
    "        else:\n",
    "            temp[i][0] = 1 ## Positive sentiment first neuron \n",
    "\n",
    "    return temp\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "ETvx1WoKfrfk"
   },
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    temp = []\n",
    "    for i in x:\n",
    "        m = np.argmax(i)\n",
    "        if m == 0:\n",
    "            temp.append('Positive')\n",
    "        elif m == 1:\n",
    "            temp.append('Neutral')\n",
    "        else:\n",
    "            temp.append('Negative')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmGshHslpZeH"
   },
   "source": [
    "## Building Glove Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "YU-SxaOZ_Nh9"
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or3hmQvdpZeJ"
   },
   "source": [
    "## Embedding Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "wBG3Kq7EpZeJ"
   },
   "outputs": [],
   "source": [
    "def emb_matrix(t,embeddings, we_dim):\n",
    "    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n",
    "    vocab_size = len(t.word_index) + 1    \n",
    "    embedding_matrix = zeros((vocab_size, we_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kT9ZAXi_NiC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgK6d3Qy_NiF"
   },
   "source": [
    "## Fine tuning the word embeddings of 300 dimensions using mittens library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqwJmPT-_NiF"
   },
   "source": [
    "## Used the code for finetuning from the following link:\n",
    "### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "HRwhIKRT_NiF"
   },
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "    #training_tokens\n",
    "\n",
    "    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "    print(len(oov))\n",
    "\n",
    "    corp_vocab = list(set(oov))\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "    trr =''\n",
    "    for i in training_tokens:\n",
    "        for j in i:\n",
    "            trr+= j\n",
    "            trr += ' '\n",
    "\n",
    "    # print(trr)\n",
    "    # print(z)\n",
    "    X = cv.fit_transform([trr])\n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    coocc_ar = Xc.toarray()\n",
    "\n",
    "    mittens_model = Mittens(n=200, max_iter=len(oov)+200)\n",
    "\n",
    "    new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "    return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB2Nd-Kr_NiH",
    "outputId": "4773e17a-16dd-4855-a490-a5fa416fa28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "Iteration 1340: loss: 0.035653963685035706"
     ]
    }
   ],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(train_pr_df)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spGvX_Cg_NiI",
    "outputId": "7efc2436-a554-40f2-8aa5-6a0932bf3d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir2jPLOS_NiK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3VcHx_CduFW"
   },
   "source": [
    "## Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "t9xXo3kBdnDc"
   },
   "outputs": [],
   "source": [
    "def cross_valid_cnn(X,y,epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "\n",
    "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        tokenise_tf = Tokenizer()\n",
    "        tokenise_tf.fit_on_texts(X_train1) \n",
    "    \n",
    "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
    "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
    "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "        int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "        embedded_sequences = embedding_layer(int_sequences_input)\n",
    "        x = layers.Conv1D(128, 3)(embedded_sequences)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(128, 3)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "        x = layers.Conv1D(128, 3)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        # x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "        model = tf.keras.Model(int_sequences_input, preds)\n",
    "        # print(model.summary())\n",
    "        # print(z)\n",
    "        model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        y_pred_temp = model.predict(validation_padded)\n",
    "        y_pred = pred(y_pred_temp)\n",
    "        f1_temp = f1_score(y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[2])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[0])\n",
    "\n",
    "        acc.append(round(accuracy_score(y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "ZQ_WOJe96xsa"
   },
   "outputs": [],
   "source": [
    "def model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(64, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(128, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "0-q5ICwzkjY8"
   },
   "outputs": [],
   "source": [
    "def model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(32, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(64, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "ixABj5x5kj33"
   },
   "outputs": [],
   "source": [
    "def model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(64, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "JebvbtfskGBZ"
   },
   "outputs": [],
   "source": [
    "# max_length = 31\n",
    "# epochs = 2\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "# we_dim = 200\n",
    "# par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "# print(par_dict)\n",
    "\n",
    "# tokenise_tf = Tokenizer()\n",
    "# tokenise_tf.fit_on_texts(obama_train_pr_df['Doc Text']) \n",
    "\n",
    "# encoded_train = tokenise_tf.texts_to_sequences(obama_train_pr_df['Doc Text'])\n",
    "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "# encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n",
    "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "# try: \n",
    "#   model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#   history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "# except:\n",
    "#   try: \n",
    "#     model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "#   except:\n",
    "\n",
    "#     model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "\n",
    "# y_pred_temp = model.predict(validation_padded)\n",
    "# y_pred = pred(y_pred_temp)\n",
    "# f1_temp = f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None)\n",
    "\n",
    "# f1_Positive = f1_temp[2]\n",
    "# f1_Neutral = f1_temp[1]\n",
    "# f1_Negative = f1_temp[0]\n",
    "\n",
    "# accuracy = round(accuracy_score(obama_val_pr_df['Sentiment'], y_pred),3) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print('\\n')\n",
    "# print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Cl2Z9PW8CEy",
    "outputId": "52564d69-79f7-460e-a7e1-35c4ad983a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_length': 47, 'batch_size': 128, 'learning_rate': 0.009717210069037124, 'epochs': 29}\n",
      "  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]WARNING:tensorflow:From C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "{'accuracy': 0.543, 'f1_pos': 0.499, 'f1_neu': 0.414, 'f1_neg': 0.641, 'eval_score': 0.561}\n",
      "{'max_length': 20, 'batch_size': 64, 'learning_rate': 0.00797270511254158, 'epochs': 22}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.491, 'f1_neu': 0.484, 'f1_neg': 0.641, 'eval_score': 0.563}\n",
      "{'max_length': 9, 'batch_size': 128, 'learning_rate': 0.004221481399783237, 'epochs': 5}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.434, 'f1_neu': 0.443, 'f1_neg': 0.62, 'eval_score': 0.526}\n",
      "{'max_length': 14, 'batch_size': 32, 'learning_rate': 0.0014695748005829833, 'epochs': 8}\n",
      "{'accuracy': 0.257, 'f1_pos': 0.409, 'f1_neu': 0.0, 'f1_neg': 0.0, 'eval_score': 0.222}\n",
      "{'max_length': 49, 'batch_size': 32, 'learning_rate': 0.006045597063473632, 'epochs': 19}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.463, 'f1_neu': 0.454, 'f1_neg': 0.651, 'eval_score': 0.556}\n",
      "{'max_length': 29, 'batch_size': 64, 'learning_rate': 0.0017783259281545683, 'epochs': 25}\n",
      "{'accuracy': 0.547, 'f1_pos': 0.52, 'f1_neu': 0.379, 'f1_neg': 0.644, 'eval_score': 0.57}\n",
      "{'max_length': 33, 'batch_size': 128, 'learning_rate': 0.0016836132152856698, 'epochs': 14}\n",
      "{'accuracy': 0.532, 'f1_pos': 0.492, 'f1_neu': 0.414, 'f1_neg': 0.623, 'eval_score': 0.549}\n",
      "{'max_length': 33, 'batch_size': 128, 'learning_rate': 0.0023172124485602887, 'epochs': 7}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.43, 'f1_neu': 0.489, 'f1_neg': 0.586, 'eval_score': 0.511}\n",
      "{'max_length': 48, 'batch_size': 128, 'learning_rate': 0.005345033716303853, 'epochs': 17}\n",
      "{'accuracy': 0.513, 'f1_pos': 0.484, 'f1_neu': 0.492, 'f1_neg': 0.551, 'eval_score': 0.516}\n",
      "{'max_length': 58, 'batch_size': 128, 'learning_rate': 0.007596245257795896, 'epochs': 20}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.49, 'f1_neu': 0.469, 'f1_neg': 0.574, 'eval_score': 0.527}\n",
      "{'max_length': 15, 'batch_size': 32, 'learning_rate': 0.004438333841081109, 'epochs': 10}\n",
      "{'accuracy': 0.257, 'f1_pos': 0.409, 'f1_neu': 0.0, 'f1_neg': 0.0, 'eval_score': 0.222}\n",
      "{'max_length': 29, 'batch_size': 64, 'learning_rate': 0.00388969747635341, 'epochs': 16}\n",
      "{'accuracy': 0.536, 'f1_pos': 0.495, 'f1_neu': 0.457, 'f1_neg': 0.616, 'eval_score': 0.549}\n",
      "{'max_length': 33, 'batch_size': 128, 'learning_rate': 0.004076107293925095, 'epochs': 24}\n",
      " 60%|██████    | 12/20 [09:39<06:39, 49.98s/trial, best loss: -0.57]"
     ]
    }
   ],
   "source": [
    "def objective_func_CNN(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "    print(par_dict)\n",
    "\n",
    "    tokenise_tf = Tokenizer()\n",
    "    tokenise_tf.fit_on_texts(train_pr_df['Doc Text']) \n",
    "\n",
    "    encoded_train = tokenise_tf.texts_to_sequences(train_pr_df['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(val_pr_df['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    try: \n",
    "        model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "        history = model.fit(training_padded, one_hot(train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "    except:\n",
    "        try:            \n",
    "            model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "            history = model.fit(training_padded, one_hot(train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        except:\n",
    "\n",
    "            model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "            history = model.fit(training_padded, one_hot(train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "\n",
    "    y_pred_temp = model.predict(validation_padded)\n",
    "    y_pred = pred(y_pred_temp)\n",
    "    f1_temp = np.round(f1_score(val_pr_df['Sentiment'], y_pred, average = None),3)\n",
    "\n",
    "    f1_Positive = f1_temp[2]\n",
    "    f1_Neutral = f1_temp[1]\n",
    "    f1_Negative = f1_temp[0]\n",
    "\n",
    "    accuracy = round(accuracy_score(val_pr_df['Sentiment'], y_pred),3) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print('\\n')\n",
    "    print(eval_dict)\n",
    "\n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.01)\n",
    "        }                  \n",
    "                                    \n",
    "we_dim = 200                               \n",
    "best_CNN = fmin(objective_func_CNN, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.564,\n",
       " 'f1_pos': 0.537,\n",
       " 'f1_neu': 0.299,\n",
       " 'f1_neg': 0.682,\n",
       " 'eval_score': 0.594}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'max_length': 48, 'batch_size': 64, 'learning_rate': 0.007428450405689178, 'epochs': 7}\n",
    "{'accuracy': 0.564, 'f1_pos': 0.537, 'f1_neu': 0.299, 'f1_neg': 0.682, 'eval_score': 0.594}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 31,\n",
       " 'batch_size': 128,\n",
       " 'learning_rate': 0.00043843544096825674,\n",
       " 'epochs': 7}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'max_length': 31, 'batch_size': 128, 'learning_rate': 0.00043843544096825674, 'epochs': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hfm558S9d2RH"
   },
   "outputs": [],
   "source": [
    "# max_length = 30\n",
    "# epochs = 20\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "# we_dim = 200\n",
    "\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "# f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "# print('\\n')\n",
    "# print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X0w0bGDBrC9x"
   },
   "outputs": [],
   "source": [
    "# def objective_func_CNN_CV(args):\n",
    "#     max_length = args['max_length']\n",
    "#     batch_size = args['batch_size']\n",
    "#     learning_rate = args['learning_rate']\n",
    "#     epochs = args['epochs']\n",
    "\n",
    "#     par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "#     print(par_dict)\n",
    "\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "#     print('\\n')\n",
    "#     print(eval_dict)\n",
    "\n",
    "#     return -(f1)\n",
    "\n",
    "# space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "#         'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "#          'epochs': hp.choice('epochs',range(5,30)), \n",
    "#          'learning_rate': hp.uniform('learning_rate', 0,0.01)\n",
    "#         }                  \n",
    "                                    \n",
    "# we_dim = 200                               \n",
    "# best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_CNN_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FhdoX1kd2Y4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldH6dPIM_NiL"
   },
   "source": [
    "## Custom F1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Q5Q1HVs_NiM"
   },
   "outputs": [],
   "source": [
    "def f1_value(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
