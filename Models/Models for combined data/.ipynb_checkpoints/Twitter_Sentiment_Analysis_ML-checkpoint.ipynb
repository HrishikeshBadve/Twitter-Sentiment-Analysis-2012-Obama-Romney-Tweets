{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ce569851-00f4-4d5e-885f-09017ba06d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "33c0b826-09cd-46a9-d064-b3d2e3f29dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "3daebdaa-b3bf-4e0e-db83-54f15377e157"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "e0d02290-c4fe-42de-a584-b148f9e7ea94"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BALelbZyNU02"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "96eAfNsDaiVd"
   },
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "kljiyU8TaCiF"
   },
   "outputs": [],
   "source": [
    "train_pr_df = pd.read_csv(os.path.join(data_path, 'Combined Training1 Data.csv'), usecols = [1,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mcLnGHdPaCiI",
    "outputId": "a08bc803-0a20-4a28-f538-77929db4d5df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>debate job come back obama get as make corp ta...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>romney romney abrasive</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sicken obama war woman tom halloran</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney business genius speculator play bi...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>day election day ready cheer president obama t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  debate job come back obama get as make corp ta...  Negative\n",
       "1                             romney romney abrasive  Negative\n",
       "2                sicken obama war woman tom halloran  Negative\n",
       "3  mitt romney business genius speculator play bi...  Negative\n",
       "4  day election day ready cheer president obama t...  Positive"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "XORvLV2iaCiM"
   },
   "outputs": [],
   "source": [
    "train_pr_df = train_pr_df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data into TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mXJwrtOtaCiQ"
   },
   "outputs": [],
   "source": [
    "def tf_idf(x):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(x)\n",
    "    #print(tfidf_matrix)\n",
    "    x1 = tfidf_matrix.toarray()\n",
    "#     print(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hM-Q2KTBaCiU"
   },
   "outputs": [],
   "source": [
    "train_pr_tfidf = tf_idf(train_pr_df['Doc Text'])\n",
    "# romney_train_pr_tfidf = tf_idf(romney_train_pr_df['Doc Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NimCgs87eUzF",
    "outputId": "c6ecaa8e-c556-4bb8-d907-fe29294689b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11117, 10120)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pr_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWPQASu9eUzG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm66hwR6eUzJ"
   },
   "source": [
    "### Cross Valid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "a6F0wDNPeUzJ"
   },
   "outputs": [],
   "source": [
    "def cross_valid(X,y,model):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(X_train1, temp)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Valid func with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_PCA(X,y,model, n):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        x_pca1 = pca.fit_transform(X_train1)\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(x_pca1, temp)\n",
    "        \n",
    "        x_pca_val = pca.transform(X_val)\n",
    "        y_pred = model.predict(x_pca_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhRyNUshSYd"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-PbiFcztf_NY"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP6ste5OD_9a",
    "outputId": "2a7a71e4-4f73-4130-ec7d-c6c80e0f44e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # clf = SVC(kernel=\"rbf\", gamma=1, class_weight='balanced')\n",
    "# #     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
    "# #     f1 = mean(error) + stdev(error) \n",
    "# #print(f1)\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# # print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "\n",
    "# print(eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "JoDEYnJDhX6a",
    "outputId": "4721cafd-2b1b-47fd-a178-a47f10f631cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.556, 'f1_pos': 0.525, 'f1_neu': 0.483, 'f1_neg': 0.629, 'eval_score': 0.57}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.53, 'f1_neu': 0.49, 'f1_neg': 0.636, 'eval_score': 0.576}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.514, 'f1_neu': 0.484, 'f1_neg': 0.622, 'eval_score': 0.562}\n",
      "{'accuracy': 0.556, 'f1_pos': 0.521, 'f1_neu': 0.489, 'f1_neg': 0.628, 'eval_score': 0.568}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.526, 'f1_neu': 0.487, 'f1_neg': 0.632, 'eval_score': 0.572}\n",
      "{'accuracy': 0.57, 'f1_pos': 0.546, 'f1_neu': 0.496, 'f1_neg': 0.639, 'eval_score': 0.585}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.521, 'f1_neu': 0.489, 'f1_neg': 0.63, 'eval_score': 0.569}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.522, 'f1_neu': 0.482, 'f1_neg': 0.628, 'eval_score': 0.568}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.535, 'f1_neu': 0.492, 'f1_neg': 0.635, 'eval_score': 0.578}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.508, 'f1_neu': 0.475, 'f1_neg': 0.62, 'eval_score': 0.558}\n",
      "100%|██████████| 10/10 [1:12:12<00:00, 433.28s/trial, best loss: -0.585]\n",
      "{'C': 0.7890784798886763}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR(args):\n",
    "\n",
    "    C = args['C']\n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(train_pr_tfidf,train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,10),\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1.8434857486520984, 'n': 300}                   \n",
      "{'accuracy': 0.541, 'f1_pos': 0.523, 'f1_neu': 0.476, 'f1_neg': 0.604, 'eval_score': 0.556}\n",
      "{'C': 1.5030580591896512, 'n': 900}                                 \n",
      "{'accuracy': 0.558, 'f1_pos': 0.54, 'f1_neu': 0.487, 'f1_neg': 0.624, 'eval_score': 0.574}\n",
      "{'C': 1.2327957870944484, 'n': 700}                                  \n",
      "{'accuracy': 0.555, 'f1_pos': 0.541, 'f1_neu': 0.484, 'f1_neg': 0.619, 'eval_score': 0.572}\n",
      "{'C': 1.4260381337579342, 'n': 200}                                  \n",
      "{'accuracy': 0.545, 'f1_pos': 0.52, 'f1_neu': 0.49, 'f1_neg': 0.604, 'eval_score': 0.556}\n",
      "{'C': 1.8566388055538825, 'n': 600}                                  \n",
      "{'accuracy': 0.552, 'f1_pos': 0.532, 'f1_neu': 0.481, 'f1_neg': 0.62, 'eval_score': 0.568}\n",
      "{'C': 1.401649559841088, 'n': 800}                                   \n",
      "{'accuracy': 0.554, 'f1_pos': 0.536, 'f1_neu': 0.482, 'f1_neg': 0.62, 'eval_score': 0.57}\n",
      "{'C': 1.5928811919880783, 'n': 600}                                  \n",
      "{'accuracy': 0.555, 'f1_pos': 0.535, 'f1_neu': 0.491, 'f1_neg': 0.617, 'eval_score': 0.569}\n",
      "{'C': 1.7511113959933775, 'n': 700}                                  \n",
      "{'accuracy': 0.552, 'f1_pos': 0.533, 'f1_neu': 0.481, 'f1_neg': 0.62, 'eval_score': 0.568}\n",
      "{'C': 1.5421121858352855, 'n': 100}                                  \n",
      "{'accuracy': 0.523, 'f1_pos': 0.5, 'f1_neu': 0.455, 'f1_neg': 0.59, 'eval_score': 0.538}\n",
      "{'C': 0.20614968226256036, 'n': 600}                                 \n",
      "{'accuracy': 0.554, 'f1_pos': 0.532, 'f1_neu': 0.482, 'f1_neg': 0.621, 'eval_score': 0.569}\n",
      "100%|██████████| 10/10 [24:25<00:00, 146.51s/trial, best loss: -0.574]\n",
      "{'C': 1.5030580591896512, 'n': 8}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR_PCA(args):\n",
    "\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    \n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    "\n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    par_dict = {'C': C, 'n': n}\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(par_dict)\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,2),\n",
    "         'n': hp.choice('n', np.arange(100,1000, step =100))\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR_PCA = fmin(objective_func_LR_PCA, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With Bayesian Optimization, different class weights and LR\n",
    "\n",
    "# def objective_func_LR(args):\n",
    "\n",
    "#     C = args['C']\n",
    "#     balance = args['balance']\n",
    "#     # multi_class = args['multi_class']\n",
    "#     # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "#     clf = LogisticRegression(C= C,class_weight=balance,n_jobs =-1)\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "#     eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "#     print(eval_dict)\n",
    "#     return -(eval_score)\n",
    "\n",
    "\n",
    "# space = {'C': hp.uniform('C', 0,10),\n",
    "#          'balance': hp.choice('balance', [{1:3, 2:1, 3:2}, {1:15, 2:1, 3:10}, {1:20, 2:5, 3:10}, {1:100, 2:10, 3:100}])\n",
    "#         # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "#         # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "#         #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "#         }                        \n",
    "                                \n",
    "                                \n",
    "# best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 1\n",
    "# n = 3\n",
    "# kernel = 'rbf'\n",
    "\n",
    "# par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "# print(par_dict)\n",
    "\n",
    "# clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print(eval_dict)\n",
    "# print('\\n')    \n",
    "# return -(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "58UghlAQeUzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.062326381967102384, 'n': 53, 'kernel': 'linear'}\n",
      "{'accuracy': 0.505, 'f1_pos': 0.379, 'f1_neu': 0.469, 'f1_neg': 0.586, 'eval_score': 0.49}\n",
      "{'C': 0.33829254293365374, 'n': 3, 'kernel': 'linear'}             \n",
      "{'accuracy': 0.45, 'f1_pos': 0.372, 'f1_neu': 0.381, 'f1_neg': 0.537, 'eval_score': 0.453}\n",
      "{'C': 0.5372641670533641, 'n': 53, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.526, 'f1_pos': 0.5, 'f1_neu': 0.451, 'f1_neg': 0.596, 'eval_score': 0.541}\n",
      "{'C': 0.3622603032203806, 'n': 353, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.564, 'f1_pos': 0.533, 'f1_neu': 0.491, 'f1_neg': 0.634, 'eval_score': 0.577}\n",
      "{'C': 0.7324611346380544, 'n': 103, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.538, 'f1_pos': 0.506, 'f1_neu': 0.479, 'f1_neg': 0.603, 'eval_score': 0.549}\n",
      "{'C': 0.24875299620690916, 'n': 403, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.563, 'f1_pos': 0.528, 'f1_neu': 0.483, 'f1_neg': 0.639, 'eval_score': 0.577}\n",
      "{'C': 0.49003910749639024, 'n': 3, 'kernel': 'linear'}               \n",
      "{'accuracy': 0.453, 'f1_pos': 0.38, 'f1_neu': 0.386, 'f1_neg': 0.538, 'eval_score': 0.457}\n",
      "{'C': 0.7761368882534679, 'n': 253, 'kernel': 'linear'}              \n",
      "{'accuracy': 0.541, 'f1_pos': 0.505, 'f1_neu': 0.499, 'f1_neg': 0.599, 'eval_score': 0.548}\n",
      "{'C': 0.5144011793377957, 'n': 103, 'kernel': 'linear'}              \n",
      "{'accuracy': 0.52, 'f1_pos': 0.478, 'f1_neu': 0.478, 'f1_neg': 0.579, 'eval_score': 0.526}\n",
      "{'C': 0.43396605928674536, 'n': 203, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.562, 'f1_pos': 0.533, 'f1_neu': 0.498, 'f1_neg': 0.628, 'eval_score': 0.574}\n",
      "{'C': 0.73511870214215, 'n': 353, 'kernel': 'linear'}                 \n",
      "{'accuracy': 0.55, 'f1_pos': 0.519, 'f1_neu': 0.502, 'f1_neg': 0.608, 'eval_score': 0.559}\n",
      "{'C': 0.23991304989555207, 'n': 303, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.563, 'f1_pos': 0.529, 'f1_neu': 0.485, 'f1_neg': 0.638, 'eval_score': 0.577}\n",
      "{'C': 0.09806563666354973, 'n': 353, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.545, 'f1_pos': 0.511, 'f1_neu': 0.451, 'f1_neg': 0.627, 'eval_score': 0.561}\n",
      "{'C': 0.11175878909204784, 'n': 3, 'kernel': 'linear'}                \n",
      "{'accuracy': 0.455, 'f1_pos': 0.346, 'f1_neu': 0.366, 'f1_neg': 0.559, 'eval_score': 0.453}\n",
      "{'C': 0.8680216873102348, 'n': 53, 'kernel': 'linear'}                \n",
      "{'accuracy': 0.511, 'f1_pos': 0.451, 'f1_neu': 0.47, 'f1_neg': 0.576, 'eval_score': 0.513}\n",
      "{'C': 0.14969858001491965, 'n': 103, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.527, 'f1_pos': 0.504, 'f1_neu': 0.44, 'f1_neg': 0.604, 'eval_score': 0.545}\n",
      "{'C': 0.11130816643742514, 'n': 353, 'kernel': 'linear'}              \n",
      "{'accuracy': 0.543, 'f1_pos': 0.48, 'f1_neu': 0.482, 'f1_neg': 0.62, 'eval_score': 0.548}\n",
      "{'C': 0.132220490706369, 'n': 103, 'kernel': 'rbf'}                   \n",
      "{'accuracy': 0.526, 'f1_pos': 0.501, 'f1_neu': 0.446, 'f1_neg': 0.6, 'eval_score': 0.542}\n",
      "{'C': 0.6121150908478982, 'n': 253, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.559, 'f1_pos': 0.532, 'f1_neu': 0.489, 'f1_neg': 0.628, 'eval_score': 0.573}\n",
      "{'C': 0.4159446835488646, 'n': 453, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.565, 'f1_pos': 0.541, 'f1_neu': 0.494, 'f1_neg': 0.633, 'eval_score': 0.58}\n",
      "100%|██████████| 20/20 [1:00:22<00:00, 181.11s/trial, best loss: -0.58] \n",
      "{'C': 0.4159446835488646, 'kernel': 1, 'n': 9}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and SVM\n",
    "\n",
    "def objective_func_SVM_PCA(args):\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    kernel = args['kernel']\n",
    "#     gamma = args['gamma']\n",
    "#     degree = args['degree']\n",
    "\n",
    "    par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "    print(par_dict)\n",
    " \n",
    "    clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,1),\n",
    "         'n': hp.choice('n', np.arange(3, 503, step =50)),\n",
    "        'kernel': hp.choice('kernel', ['linear', 'rbf']) \n",
    "#         'gamma': hp.choice('gamma',range(1,4)),\n",
    "#          'degree' : hp.choice('degree',range(1,4))\n",
    "        }\n",
    "                                \n",
    "                       \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_SVM_PCA = fmin(objective_func_SVM_PCA, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_classifier_SVM_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 26, 'metric': 'manhattan'}            \n",
      "{'accuracy': 0.354, 'f1_pos': 0.44, 'f1_neu': 0.227, 'f1_neg': 0.302, 'eval_score': 0.365}\n",
      "{'n_neighbors': 2, 'metric': 'euclidean'}                              \n",
      "{'accuracy': 0.429, 'f1_pos': 0.47, 'f1_neu': 0.42, 'f1_neg': 0.39, 'eval_score': 0.43}\n",
      "{'n_neighbors': 5, 'metric': 'manhattan'}                              \n",
      "{'accuracy': 0.411, 'f1_pos': 0.449, 'f1_neu': 0.39, 'f1_neg': 0.379, 'eval_score': 0.413}\n",
      "{'n_neighbors': 20, 'metric': 'euclidean'}                            \n",
      "{'accuracy': 0.51, 'f1_pos': 0.513, 'f1_neu': 0.402, 'f1_neg': 0.582, 'eval_score': 0.535}\n",
      "{'n_neighbors': 26, 'metric': 'manhattan'}                             \n",
      "{'accuracy': 0.351, 'f1_pos': 0.435, 'f1_neu': 0.239, 'f1_neg': 0.294, 'eval_score': 0.36}\n",
      "{'n_neighbors': 8, 'metric': 'euclidean'}                              \n",
      "{'accuracy': 0.492, 'f1_pos': 0.499, 'f1_neu': 0.419, 'f1_neg': 0.543, 'eval_score': 0.511}\n",
      "{'n_neighbors': 8, 'metric': 'euclidean'}                              \n",
      "{'accuracy': 0.497, 'f1_pos': 0.503, 'f1_neu': 0.427, 'f1_neg': 0.547, 'eval_score': 0.516}\n",
      "{'n_neighbors': 26, 'metric': 'manhattan'}                             \n",
      "{'accuracy': 0.35, 'f1_pos': 0.435, 'f1_neu': 0.228, 'f1_neg': 0.297, 'eval_score': 0.361}\n",
      "{'n_neighbors': 5, 'metric': 'manhattan'}                              \n",
      "{'accuracy': 0.411, 'f1_pos': 0.451, 'f1_neu': 0.392, 'f1_neg': 0.376, 'eval_score': 0.413}\n",
      "{'n_neighbors': 26, 'metric': 'manhattan'}                             \n",
      "{'accuracy': 0.355, 'f1_pos': 0.437, 'f1_neu': 0.239, 'f1_neg': 0.302, 'eval_score': 0.365}\n",
      "100%|██████████| 10/10 [1:46:30<00:00, 639.08s/trial, best loss: -0.535]\n",
      "{'metric': 0, 'n_neighbors': 6}\n"
     ]
    }
   ],
   "source": [
    "# WIth Bayesian Optimization and KNN\n",
    "def objective_func_KNN(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']  \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(train_pr_tfidf,train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(2,32, step =3)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"])\n",
    "        }\n",
    "\n",
    "best_classifier_KNN = fmin(objective_func_KNN, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 8, 'n': 53, 'metric': 'euclidean'}    \n",
      "{'accuracy': 0.481, 'f1_pos': 0.438, 'f1_neu': 0.441, 'f1_neg': 0.539, 'eval_score': 0.486}\n",
      "{'n_neighbors': 5, 'n': 23, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.465, 'f1_pos': 0.431, 'f1_neu': 0.417, 'f1_neg': 0.527, 'eval_score': 0.474}\n",
      "{'n_neighbors': 3, 'n': 33, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.462, 'f1_pos': 0.434, 'f1_neu': 0.387, 'f1_neg': 0.534, 'eval_score': 0.477}\n",
      "{'n_neighbors': 5, 'n': 43, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.474, 'f1_pos': 0.437, 'f1_neu': 0.43, 'f1_neg': 0.535, 'eval_score': 0.482}\n",
      "{'n_neighbors': 6, 'n': 93, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.476, 'f1_pos': 0.439, 'f1_neu': 0.439, 'f1_neg': 0.529, 'eval_score': 0.481}\n",
      "{'n_neighbors': 9, 'n': 63, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.487, 'f1_pos': 0.435, 'f1_neu': 0.446, 'f1_neg': 0.55, 'eval_score': 0.491}\n",
      "{'n_neighbors': 5, 'n': 73, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.467, 'f1_pos': 0.435, 'f1_neu': 0.437, 'f1_neg': 0.515, 'eval_score': 0.472}\n",
      "{'n_neighbors': 7, 'n': 13, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.473, 'f1_pos': 0.413, 'f1_neu': 0.415, 'f1_neg': 0.546, 'eval_score': 0.477}\n",
      "{'n_neighbors': 2, 'n': 103, 'metric': 'euclidean'}                 \n",
      "{'accuracy': 0.415, 'f1_pos': 0.441, 'f1_neu': 0.423, 'f1_neg': 0.379, 'eval_score': 0.412}\n",
      "{'n_neighbors': 6, 'n': 13, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.471, 'f1_pos': 0.429, 'f1_neu': 0.408, 'f1_neg': 0.541, 'eval_score': 0.48}\n",
      "{'n_neighbors': 8, 'n': 103, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.472, 'f1_pos': 0.431, 'f1_neu': 0.452, 'f1_neg': 0.515, 'eval_score': 0.473}\n",
      "{'n_neighbors': 3, 'n': 63, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.465, 'f1_pos': 0.431, 'f1_neu': 0.406, 'f1_neg': 0.532, 'eval_score': 0.476}\n",
      "{'n_neighbors': 3, 'n': 23, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.464, 'f1_pos': 0.428, 'f1_neu': 0.392, 'f1_neg': 0.541, 'eval_score': 0.478}\n",
      "{'n_neighbors': 5, 'n': 83, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.471, 'f1_pos': 0.444, 'f1_neu': 0.437, 'f1_neg': 0.519, 'eval_score': 0.478}\n",
      "{'n_neighbors': 1, 'n': 73, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.453, 'f1_pos': 0.403, 'f1_neu': 0.407, 'f1_neg': 0.518, 'eval_score': 0.458}\n",
      "100%|██████████| 15/15 [06:24<00:00, 25.63s/trial, best loss: -0.491]\n",
      "{'metric': 1, 'n': 6, 'n_neighbors': 8}\n"
     ]
    }
   ],
   "source": [
    "# WIth PCA , Bayesian Optimization and KNN\n",
    "def objective_func_KNN_PCA(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']\n",
    "    n = args['n']    \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'n': n, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,10, step =1)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"]),\n",
    "        'n': hp.choice('n', np.arange(3,123, step =10))}\n",
    "\n",
    "best_classifier_KNN_PCA = fmin(objective_func_KNN_PCA, space, algo=tpe.suggest, max_evals=15)\n",
    "print(best_classifier_KNN_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-z9cSEeUzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.387, 'f1_pos': 0.44, 'f1_neu': 0.346, 'f1_neg': 0.36, 'eval_score': 0.396}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(train_pr_tfidf,train_pr_df['Sentiment'], clf) \n",
    "eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 153}                                            \n",
      "{'accuracy': 0.446, 'f1_pos': 0.39, 'f1_neu': 0.478, 'f1_neg': 0.44, 'eval_score': 0.425}\n",
      "{'n': 253}                                                          \n",
      "{'accuracy': 0.446, 'f1_pos': 0.404, 'f1_neu': 0.47, 'f1_neg': 0.444, 'eval_score': 0.431}\n",
      "{'n': 153}                                                          \n",
      "{'accuracy': 0.448, 'f1_pos': 0.394, 'f1_neu': 0.476, 'f1_neg': 0.447, 'eval_score': 0.43}\n",
      "{'n': 3}                                                            \n",
      "{'accuracy': 0.458, 'f1_pos': 0.303, 'f1_neu': 0.175, 'f1_neg': 0.61, 'eval_score': 0.457}\n",
      "{'n': 553}                                                          \n",
      "{'accuracy': 0.428, 'f1_pos': 0.416, 'f1_neu': 0.431, 'f1_neg': 0.434, 'eval_score': 0.426}\n",
      "{'n': 803}                                                          \n",
      "{'accuracy': 0.388, 'f1_pos': 0.403, 'f1_neu': 0.37, 'f1_neg': 0.389, 'eval_score': 0.393}\n",
      "{'n': 853}                                                           \n",
      "{'accuracy': 0.381, 'f1_pos': 0.401, 'f1_neu': 0.359, 'f1_neg': 0.377, 'eval_score': 0.386}\n",
      "{'n': 553}                                                           \n",
      "{'accuracy': 0.428, 'f1_pos': 0.414, 'f1_neu': 0.427, 'f1_neg': 0.441, 'eval_score': 0.428}\n",
      "{'n': 203}                                                           \n",
      "{'accuracy': 0.448, 'f1_pos': 0.405, 'f1_neu': 0.477, 'f1_neg': 0.44, 'eval_score': 0.431}\n",
      "{'n': 553}                                                           \n",
      "{'accuracy': 0.428, 'f1_pos': 0.416, 'f1_neu': 0.434, 'f1_neg': 0.432, 'eval_score': 0.425}\n",
      "{'n': 153}                                                            \n",
      "{'accuracy': 0.446, 'f1_pos': 0.392, 'f1_neu': 0.478, 'f1_neg': 0.442, 'eval_score': 0.427}\n",
      "{'n': 203}                                                            \n",
      "{'accuracy': 0.454, 'f1_pos': 0.403, 'f1_neu': 0.481, 'f1_neg': 0.453, 'eval_score': 0.437}\n",
      "{'n': 553}                                                           \n",
      "{'accuracy': 0.424, 'f1_pos': 0.416, 'f1_neu': 0.434, 'f1_neg': 0.42, 'eval_score': 0.42}\n",
      "{'n': 403}                                                           \n",
      "{'accuracy': 0.448, 'f1_pos': 0.419, 'f1_neu': 0.467, 'f1_neg': 0.445, 'eval_score': 0.437}\n",
      "{'n': 303}                                                           \n",
      "{'accuracy': 0.445, 'f1_pos': 0.404, 'f1_neu': 0.469, 'f1_neg': 0.443, 'eval_score': 0.431}\n",
      "{'n': 3}                                                             \n",
      "{'accuracy': 0.456, 'f1_pos': 0.3, 'f1_neu': 0.172, 'f1_neg': 0.61, 'eval_score': 0.455}\n",
      "{'n': 53}                                                            \n",
      "{'accuracy': 0.437, 'f1_pos': 0.392, 'f1_neu': 0.475, 'f1_neg': 0.415, 'eval_score': 0.415}\n",
      "{'n': 103}                                                           \n",
      "{'accuracy': 0.44, 'f1_pos': 0.386, 'f1_neu': 0.472, 'f1_neg': 0.434, 'eval_score': 0.42}\n",
      "{'n': 953}                                                           \n",
      "{'accuracy': 0.371, 'f1_pos': 0.399, 'f1_neu': 0.354, 'f1_neg': 0.352, 'eval_score': 0.374}\n",
      "{'n': 753}                                                           \n",
      "{'accuracy': 0.395, 'f1_pos': 0.403, 'f1_neu': 0.382, 'f1_neg': 0.396, 'eval_score': 0.398}\n",
      "100%|██████████| 20/20 [30:10<00:00, 90.54s/trial, best loss: -0.457] \n"
     ]
    }
   ],
   "source": [
    "# With PCA and Naive bayes\n",
    "def objective_func_NB_PCA(args):\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = {'n': n}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n': hp.choice('n', np.arange(3,1023, step =50))}\n",
    "\n",
    "best_classifier_NB_PCA = fmin(objective_func_NB_PCA, space, algo=tpe.suggest, max_evals=20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uqUrTlhaCiX",
    "outputId": "46220e5f-4754-4177-f6df-fc32cf0fa883"
   },
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnu2mBLwaCie"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "r-6Q8dRwaCig",
    "outputId": "b42504a9-9d98-49fd-e286-297bb9fcd020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 440, 'max_depth': 30, 'min_samples_split': 7, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.488, 'f1_pos': 0.255, 'f1_neu': 0.147, 'f1_neg': 0.631, 'eval_score': 0.458}\n",
      "{'n_estimators': 560, 'max_depth': 72, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.548, 'f1_pos': 0.446, 'f1_neu': 0.331, 'f1_neg': 0.663, 'eval_score': 0.552}\n",
      "{'n_estimators': 40, 'max_depth': 62, 'min_samples_split': 3, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.373, 'f1_neu': 0.289, 'f1_neg': 0.655, 'eval_score': 0.518}\n",
      "{'n_estimators': 520, 'max_depth': 52, 'min_samples_split': 5, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.384, 'f1_neu': 0.265, 'f1_neg': 0.654, 'eval_score': 0.522}\n",
      "{'n_estimators': 20, 'max_depth': 74, 'min_samples_split': 3, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.399, 'f1_neu': 0.306, 'f1_neg': 0.655, 'eval_score': 0.528}\n",
      "{'n_estimators': 280, 'max_depth': 39, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.391, 'f1_neu': 0.244, 'f1_neg': 0.649, 'eval_score': 0.522}\n",
      "{'n_estimators': 420, 'max_depth': 81, 'min_samples_split': 5, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.48, 'f1_neu': 0.38, 'f1_neg': 0.67, 'eval_score': 0.571}\n",
      "{'n_estimators': 380, 'max_depth': 77, 'min_samples_split': 10, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.433, 'f1_neu': 0.325, 'f1_neg': 0.664, 'eval_score': 0.547}\n",
      "{'n_estimators': 460, 'max_depth': 58, 'min_samples_split': 10, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.512, 'f1_pos': 0.345, 'f1_neu': 0.216, 'f1_neg': 0.646, 'eval_score': 0.501}\n",
      "{'n_estimators': 580, 'max_depth': 74, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.456, 'f1_neu': 0.335, 'f1_neg': 0.665, 'eval_score': 0.557}\n",
      "{'n_estimators': 240, 'max_depth': 50, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.386, 'f1_neu': 0.254, 'f1_neg': 0.652, 'eval_score': 0.521}\n",
      "{'n_estimators': 560, 'max_depth': 52, 'min_samples_split': 7, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.528, 'f1_pos': 0.382, 'f1_neu': 0.268, 'f1_neg': 0.653, 'eval_score': 0.521}\n",
      "{'n_estimators': 420, 'max_depth': 51, 'min_samples_split': 4, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.397, 'f1_neu': 0.283, 'f1_neg': 0.657, 'eval_score': 0.529}\n",
      "{'n_estimators': 140, 'max_depth': 44, 'min_samples_split': 5, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.521, 'f1_pos': 0.37, 'f1_neu': 0.243, 'f1_neg': 0.65, 'eval_score': 0.514}\n",
      "{'n_estimators': 260, 'max_depth': 77, 'min_samples_split': 7, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.386, 'f1_neu': 0.274, 'f1_neg': 0.654, 'eval_score': 0.522}\n",
      "{'n_estimators': 160, 'max_depth': 87, 'min_samples_split': 8, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.422, 'f1_neu': 0.31, 'f1_neg': 0.658, 'eval_score': 0.539}\n",
      "{'n_estimators': 260, 'max_depth': 28, 'min_samples_split': 5, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.491, 'f1_pos': 0.265, 'f1_neu': 0.156, 'f1_neg': 0.632, 'eval_score': 0.463}\n",
      "{'n_estimators': 120, 'max_depth': 90, 'min_samples_split': 6, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.519, 'f1_pos': 0.375, 'f1_neu': 0.242, 'f1_neg': 0.649, 'eval_score': 0.514}\n",
      "{'n_estimators': 160, 'max_depth': 69, 'min_samples_split': 7, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.387, 'f1_neu': 0.285, 'f1_neg': 0.655, 'eval_score': 0.524}\n",
      "{'n_estimators': 300, 'max_depth': 71, 'min_samples_split': 8, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.432, 'f1_neu': 0.312, 'f1_neg': 0.658, 'eval_score': 0.543}\n",
      "{'n_estimators': 580, 'max_depth': 85, 'min_samples_split': 2, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.443, 'f1_neu': 0.349, 'f1_neg': 0.663, 'eval_score': 0.552}\n",
      "{'n_estimators': 420, 'max_depth': 81, 'min_samples_split': 11, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.56, 'f1_pos': 0.48, 'f1_neu': 0.371, 'f1_neg': 0.667, 'eval_score': 0.569}\n",
      "{'n_estimators': 420, 'max_depth': 81, 'min_samples_split': 11, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.49, 'f1_neu': 0.377, 'f1_neg': 0.67, 'eval_score': 0.575}\n",
      "{'n_estimators': 80, 'max_depth': 46, 'min_samples_split': 11, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.537, 'f1_pos': 0.412, 'f1_neu': 0.291, 'f1_neg': 0.657, 'eval_score': 0.535}\n",
      "{'n_estimators': 480, 'max_depth': 81, 'min_samples_split': 11, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.481, 'f1_neu': 0.37, 'f1_neg': 0.67, 'eval_score': 0.571}\n",
      "100%|██████████| 25/25 [1:56:43<00:00, 280.15s/trial, best loss: -0.575]\n",
      "{'max_depth': 61, 'min_samples_leaf': 0, 'min_samples_split': 9, 'n_estimators': 20}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "\n",
    "    par_dict = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(train_pr_tfidf,train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,601, step =20)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF = fmin(objective_func_RF, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "AI0VHzOleUzi",
    "outputId": "de0a927c-95c9-439d-d3ff-06cd57c0aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 163, 'n_estimators': 300, 'max_depth': 53, 'min_samples_split': 4, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.439, 'f1_neu': 0.403, 'f1_neg': 0.657, 'eval_score': 0.549}\n",
      "{'n': 123, 'n_estimators': 320, 'max_depth': 33, 'min_samples_split': 9, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.443, 'f1_neu': 0.399, 'f1_neg': 0.657, 'eval_score': 0.55}\n",
      "{'n': 103, 'n_estimators': 300, 'max_depth': 84, 'min_samples_split': 7, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.459, 'f1_neu': 0.424, 'f1_neg': 0.659, 'eval_score': 0.558}\n",
      "{'n': 123, 'n_estimators': 40, 'max_depth': 94, 'min_samples_split': 3, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.448, 'f1_neu': 0.42, 'f1_neg': 0.648, 'eval_score': 0.547}\n",
      "{'n': 123, 'n_estimators': 440, 'max_depth': 87, 'min_samples_split': 9, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.454, 'f1_neu': 0.407, 'f1_neg': 0.656, 'eval_score': 0.554}\n",
      "{'n': 3, 'n_estimators': 580, 'max_depth': 100, 'min_samples_split': 10, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.492, 'f1_pos': 0.406, 'f1_neu': 0.374, 'f1_neg': 0.594, 'eval_score': 0.497}\n",
      "{'n': 43, 'n_estimators': 120, 'max_depth': 91, 'min_samples_split': 6, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.463, 'f1_neu': 0.409, 'f1_neg': 0.644, 'eval_score': 0.55}\n",
      "{'n': 3, 'n_estimators': 440, 'max_depth': 57, 'min_samples_split': 6, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.494, 'f1_pos': 0.41, 'f1_neu': 0.357, 'f1_neg': 0.602, 'eval_score': 0.502}\n",
      "{'n': 123, 'n_estimators': 580, 'max_depth': 90, 'min_samples_split': 6, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.457, 'f1_neu': 0.413, 'f1_neg': 0.659, 'eval_score': 0.557}\n",
      "{'n': 63, 'n_estimators': 20, 'max_depth': 29, 'min_samples_split': 11, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.528, 'f1_pos': 0.437, 'f1_neu': 0.403, 'f1_neg': 0.63, 'eval_score': 0.532}\n",
      "{'n': 143, 'n_estimators': 280, 'max_depth': 80, 'min_samples_split': 11, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.437, 'f1_neu': 0.392, 'f1_neg': 0.659, 'eval_score': 0.548}\n",
      "{'n': 183, 'n_estimators': 320, 'max_depth': 72, 'min_samples_split': 8, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.442, 'f1_neu': 0.39, 'f1_neg': 0.658, 'eval_score': 0.55}\n",
      "{'n': 63, 'n_estimators': 420, 'max_depth': 61, 'min_samples_split': 11, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.465, 'f1_neu': 0.402, 'f1_neg': 0.654, 'eval_score': 0.556}\n",
      "{'n': 143, 'n_estimators': 20, 'max_depth': 95, 'min_samples_split': 6, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.522, 'f1_pos': 0.437, 'f1_neu': 0.41, 'f1_neg': 0.619, 'eval_score': 0.526}\n",
      "{'n': 3, 'n_estimators': 560, 'max_depth': 38, 'min_samples_split': 10, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.494, 'f1_pos': 0.413, 'f1_neu': 0.376, 'f1_neg': 0.595, 'eval_score': 0.501}\n",
      "{'n': 123, 'n_estimators': 460, 'max_depth': 60, 'min_samples_split': 7, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.451, 'f1_neu': 0.405, 'f1_neg': 0.655, 'eval_score': 0.552}\n",
      "{'n': 183, 'n_estimators': 120, 'max_depth': 58, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.548, 'f1_pos': 0.446, 'f1_neu': 0.405, 'f1_neg': 0.653, 'eval_score': 0.549}\n",
      "{'n': 83, 'n_estimators': 440, 'max_depth': 49, 'min_samples_split': 6, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.451, 'f1_neu': 0.398, 'f1_neg': 0.653, 'eval_score': 0.55}\n",
      "{'n': 3, 'n_estimators': 60, 'max_depth': 62, 'min_samples_split': 11, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.491, 'f1_pos': 0.416, 'f1_neu': 0.366, 'f1_neg': 0.594, 'eval_score': 0.5}\n",
      "{'n': 43, 'n_estimators': 260, 'max_depth': 36, 'min_samples_split': 9, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.472, 'f1_neu': 0.415, 'f1_neg': 0.649, 'eval_score': 0.557}\n",
      "{'n': 103, 'n_estimators': 260, 'max_depth': 84, 'min_samples_split': 7, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.448, 'f1_neu': 0.401, 'f1_neg': 0.658, 'eval_score': 0.552}\n",
      "{'n': 103, 'n_estimators': 80, 'max_depth': 90, 'min_samples_split': 2, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.539, 'f1_pos': 0.454, 'f1_neu': 0.416, 'f1_neg': 0.639, 'eval_score': 0.544}\n",
      "{'n': 43, 'n_estimators': 160, 'max_depth': 36, 'min_samples_split': 5, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.547, 'f1_pos': 0.476, 'f1_neu': 0.415, 'f1_neg': 0.645, 'eval_score': 0.556}\n",
      "{'n': 23, 'n_estimators': 360, 'max_depth': 36, 'min_samples_split': 7, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.46, 'f1_neu': 0.42, 'f1_neg': 0.64, 'eval_score': 0.547}\n",
      "{'n': 103, 'n_estimators': 300, 'max_depth': 84, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.463, 'f1_neu': 0.406, 'f1_neg': 0.657, 'eval_score': 0.558}\n",
      "{'n': 103, 'n_estimators': 300, 'max_depth': 84, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.455, 'f1_neu': 0.406, 'f1_neg': 0.655, 'eval_score': 0.553}\n",
      "{'n': 103, 'n_estimators': 400, 'max_depth': 84, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.462, 'f1_neu': 0.418, 'f1_neg': 0.655, 'eval_score': 0.557}\n",
      "{'n': 103, 'n_estimators': 380, 'max_depth': 70, 'min_samples_split': 4, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.446, 'f1_neu': 0.407, 'f1_neg': 0.66, 'eval_score': 0.553}\n",
      "{'n': 163, 'n_estimators': 300, 'max_depth': 56, 'min_samples_split': 3, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.459, 'f1_neu': 0.412, 'f1_neg': 0.662, 'eval_score': 0.559}\n",
      "{'n': 163, 'n_estimators': 500, 'max_depth': 46, 'min_samples_split': 3, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.447, 'f1_neu': 0.4, 'f1_neg': 0.659, 'eval_score': 0.553}\n",
      "100%|██████████| 30/30 [28:28<00:00, 56.95s/trial, best loss: -0.559]\n",
      "{'max_depth': 36, 'min_samples_leaf': 1, 'min_samples_split': 1, 'n': 8, 'n_estimators': 14}\n"
     ]
    }
   ],
   "source": [
    "# With PCA,  Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF_PCA(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = { 'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,601, step =20)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF_PCA = fmin(objective_func_RF_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_RF_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "TKP0NEj6eUzk"
   },
   "outputs": [],
   "source": [
    "# # WITH LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from math import sqrt\n",
    "# lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "# x_lda = lda.fit_transform(train_pr_tfidf,train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "NQsPlUENeUzl",
    "outputId": "7a8a84e7-88c0-4bab-bf29-611d0cc98d98"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "#     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda, temp, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzCfxZ1GeUzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "K5Y5ahpseUzp"
   },
   "outputs": [],
   "source": [
    "# obama_train2_pr_tfidf, obama_val_pr_tfidf, obama_train2_pr_class, obama_val_pr_class = train_test_split(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "4SUBRUJ8eUzq"
   },
   "outputs": [],
   "source": [
    "# x_lda2 = lda.fit_transform(train_pr_tfidf,train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "6cz5e2KzeUzs",
    "outputId": "9d0f0738-440f-4c25-dcf2-79fc3f0dce22"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "# #     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda2, obama_train2_pr_class, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "pm0j2MPOeUzt"
   },
   "outputs": [],
   "source": [
    "# x_val_lda = lda.transform(obama_val_pr_tfidf)\n",
    "# x_val_lda_df = pd.DataFrame(data = x_val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "GkTc0libeUzy",
    "outputId": "e52d6004-e6f8-46bd-863b-90bb91bba6be"
   },
   "outputs": [],
   "source": [
    "# c = lambda b : 1 if b == 0 else b\n",
    "# d = lambda b : 2 if b <= 1 else b\n",
    "# bc = RandomForestClassifier(max_depth = c(best_classifier['max_depth']), min_samples_leaf = c(best_classifier['min_samples_leaf']), min_samples_split = d(best_classifier['min_samples_split'])\n",
    "#                             ,n_estimators = best_classifier['n_estimators'])\n",
    "\n",
    "# bc.fit(x_lda2, obama_train2_pr_class)\n",
    "\n",
    "# y_pred = bc.predict(x_val_lda)\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "JDPpRJ91eUz0",
    "outputId": "7338ec62-8947-4d33-a1ef-f2e2eee8843d"
   },
   "outputs": [],
   "source": [
    "# lda_clf = lda.fit(obama_train2_pr_tfidf, obama_train2_pr_class)\n",
    "# y_pred_lda_clf = lda_clf.predict(obama_val_pr_tfidf)\n",
    "# # y_pred_lda_clf\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred_lda_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBsD7WYFeUz4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "0UB056v0eUz6"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'goss', 'n_estimators': 900, 'max_depth': 9, 'learning_rate': 0.0005302970688930043, 'subsample': 0.9333333333333333, 'min_split_gain': 5}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.478, 'f1_neu': 0.396, 'f1_neg': 0.588, 'eval_score': 0.524}\n",
      "{'boosting_type': 'dart', 'n_estimators': 900, 'max_depth': 7, 'learning_rate': 0.15031426232480716, 'subsample': 0.8666666666666667, 'min_split_gain': 5}\n",
      "{'accuracy': 0.524, 'f1_pos': 0.503, 'f1_neu': 0.439, 'f1_neg': 0.597, 'eval_score': 0.541}\n",
      "{'boosting_type': 'goss', 'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.19809633629294132, 'subsample': 0.9333333333333333, 'min_split_gain': 0}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.503, 'f1_neu': 0.453, 'f1_neg': 0.576, 'eval_score': 0.532}\n",
      "{'boosting_type': 'goss', 'n_estimators': 750, 'max_depth': 5, 'learning_rate': 0.13876934680274372, 'subsample': 0.8, 'min_split_gain': 4}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.503, 'f1_neu': 0.46, 'f1_neg': 0.59, 'eval_score': 0.539}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 650, 'max_depth': 7, 'learning_rate': 0.15275175555036027, 'subsample': 0.8, 'min_split_gain': 5}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.5, 'f1_neu': 0.438, 'f1_neg': 0.602, 'eval_score': 0.543}\n",
      "{'boosting_type': 'dart', 'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.0391792232658363, 'subsample': 1.0, 'min_split_gain': 3}\n",
      "{'accuracy': 0.537, 'f1_pos': 0.509, 'f1_neu': 0.465, 'f1_neg': 0.605, 'eval_score': 0.55}\n",
      "{'boosting_type': 'dart', 'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.007823350019091823, 'subsample': 0.8666666666666667, 'min_split_gain': 2}\n",
      "{'accuracy': 0.493, 'f1_pos': 0.449, 'f1_neu': 0.314, 'f1_neg': 0.602, 'eval_score': 0.515}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.10489869227477588, 'subsample': 0.8666666666666667, 'min_split_gain': 5}\n",
      "{'accuracy': 0.515, 'f1_pos': 0.484, 'f1_neu': 0.424, 'f1_neg': 0.593, 'eval_score': 0.531}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 800, 'max_depth': 7, 'learning_rate': 0.14179768474640841, 'subsample': 0.8666666666666667, 'min_split_gain': 0}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.515, 'f1_neu': 0.479, 'f1_neg': 0.605, 'eval_score': 0.554}\n",
      "{'boosting_type': 'goss', 'n_estimators': 450, 'max_depth': 4, 'learning_rate': 0.031313972588206784, 'subsample': 0.8, 'min_split_gain': 2}\n",
      "{'accuracy': 0.535, 'f1_pos': 0.509, 'f1_neu': 0.471, 'f1_neg': 0.599, 'eval_score': 0.548}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 800, 'max_depth': 3, 'learning_rate': 0.1074396962169735, 'subsample': 0.8666666666666667, 'min_split_gain': 0}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.518, 'f1_neu': 0.484, 'f1_neg': 0.606, 'eval_score': 0.556}\n",
      "{'boosting_type': 'goss', 'n_estimators': 400, 'max_depth': 5, 'learning_rate': 0.11940997175409723, 'subsample': 0.8666666666666667, 'min_split_gain': 2}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.51, 'f1_neu': 0.469, 'f1_neg': 0.589, 'eval_score': 0.543}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 200, 'max_depth': 4, 'learning_rate': 0.12315444383888284, 'subsample': 0.8666666666666667, 'min_split_gain': 3}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.508, 'f1_neu': 0.466, 'f1_neg': 0.61, 'eval_score': 0.552}\n",
      "{'boosting_type': 'dart', 'n_estimators': 800, 'max_depth': 9, 'learning_rate': 0.1355008904802522, 'subsample': 0.8, 'min_split_gain': 0}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.516, 'f1_neu': 0.488, 'f1_neg': 0.605, 'eval_score': 0.555}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 800, 'max_depth': 8, 'learning_rate': 0.05104715696706785, 'subsample': 0.8666666666666667, 'min_split_gain': 0}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.519, 'f1_neu': 0.491, 'f1_neg': 0.602, 'eval_score': 0.555}\n",
      "{'boosting_type': 'dart', 'n_estimators': 550, 'max_depth': 6, 'learning_rate': 0.18631689794513873, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.521, 'f1_neu': 0.491, 'f1_neg': 0.604, 'eval_score': 0.557}\n",
      "{'boosting_type': 'goss', 'n_estimators': 550, 'max_depth': 3, 'learning_rate': 0.01879123080463785, 'subsample': 0.8666666666666667, 'min_split_gain': 1}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.495, 'f1_neu': 0.46, 'f1_neg': 0.601, 'eval_score': 0.542}\n",
      "{'boosting_type': 'dart', 'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.047584684709338236, 'subsample': 0.8, 'min_split_gain': 0}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.494, 'f1_neu': 0.445, 'f1_neg': 0.613, 'eval_score': 0.547}\n",
      "{'boosting_type': 'dart', 'n_estimators': 300, 'max_depth': 4, 'learning_rate': 0.008450240401183117, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.447, 'f1_neu': 0.342, 'f1_neg': 0.613, 'eval_score': 0.522}\n",
      "{'boosting_type': 'dart', 'n_estimators': 650, 'max_depth': 4, 'learning_rate': 0.04934376192283956, 'subsample': 0.8666666666666667, 'min_split_gain': 1}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.5, 'f1_neu': 0.451, 'f1_neg': 0.602, 'eval_score': 0.544}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.18816788412269736, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.547, 'f1_pos': 0.523, 'f1_neu': 0.493, 'f1_neg': 0.606, 'eval_score': 0.559}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.19944728090782932, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.529, 'f1_neu': 0.489, 'f1_neg': 0.609, 'eval_score': 0.562}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.17621807060035952, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.521, 'f1_neu': 0.488, 'f1_neg': 0.603, 'eval_score': 0.556}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.08053250193961002, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.524, 'f1_neu': 0.485, 'f1_neg': 0.608, 'eval_score': 0.559}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.08108788220222118, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.507, 'f1_neu': 0.456, 'f1_neg': 0.606, 'eval_score': 0.549}\n",
      "100%|██████████| 25/25 [09:38<00:00, 23.16s/trial, best loss: -0.562]\n",
      "{'boosting_type': 0, 'learning_rate': 0.19944728090782932, 'max_depth': 3, 'min_split_gain': 1, 'n_estimators': 9, 'subsample': 3}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "    subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "\n",
    "    par_dict = {'boosting_type': boosting_type, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': subsample, 'min_split_gain': min_split_gain}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=subsample, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(train_pr_tfidf,train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,1001, step =50)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "     'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB = fmin(objective_func_LGB, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 83, 'n_estimators': 150, 'max_depth': 6, 'learning_rate': 0.1960623520083565, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.463, 'f1_neu': 0.436, 'f1_neg': 0.58, 'eval_score': 0.516}\n",
      "{'n': 83, 'n_estimators': 750, 'max_depth': 5, 'learning_rate': 0.05088249025808522, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.509, 'f1_neu': 0.474, 'f1_neg': 0.615, 'eval_score': 0.556}\n",
      "{'n': 83, 'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.022378320037396683, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.512, 'f1_neu': 0.454, 'f1_neg': 0.603, 'eval_score': 0.549}\n",
      "{'n': 3, 'n_estimators': 250, 'max_depth': 4, 'learning_rate': 0.17885938403990365, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.451, 'f1_pos': 0.425, 'f1_neu': 0.398, 'f1_neg': 0.508, 'eval_score': 0.461}\n",
      "{'n': 63, 'n_estimators': 650, 'max_depth': 3, 'learning_rate': 0.04678839325729607, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.528, 'f1_pos': 0.499, 'f1_neu': 0.465, 'f1_neg': 0.593, 'eval_score': 0.54}\n",
      "{'n': 43, 'n_estimators': 650, 'max_depth': 7, 'learning_rate': 0.10363954744352838, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.5, 'f1_neu': 0.45, 'f1_neg': 0.598, 'eval_score': 0.541}\n",
      "{'n': 163, 'n_estimators': 950, 'max_depth': 7, 'learning_rate': 0.051553357125775336, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.509, 'f1_neu': 0.467, 'f1_neg': 0.599, 'eval_score': 0.547}\n",
      "{'n': 83, 'n_estimators': 650, 'max_depth': 4, 'learning_rate': 0.1784068265365361, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.513, 'f1_neu': 0.478, 'f1_neg': 0.605, 'eval_score': 0.553}\n",
      "{'n': 143, 'n_estimators': 850, 'max_depth': 9, 'learning_rate': 0.10184868773996253, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.489, 'f1_neu': 0.47, 'f1_neg': 0.65, 'eval_score': 0.567}\n",
      "{'n': 103, 'n_estimators': 750, 'max_depth': 7, 'learning_rate': 0.16697574568398832, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.515, 'f1_pos': 0.472, 'f1_neu': 0.442, 'f1_neg': 0.592, 'eval_score': 0.526}\n",
      "{'n': 43, 'n_estimators': 650, 'max_depth': 8, 'learning_rate': 0.034978355358574964, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.539, 'f1_pos': 0.506, 'f1_neu': 0.467, 'f1_neg': 0.609, 'eval_score': 0.551}\n",
      "{'n': 3, 'n_estimators': 50, 'max_depth': 5, 'learning_rate': 0.18907673937448877, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.485, 'f1_pos': 0.441, 'f1_neu': 0.392, 'f1_neg': 0.57, 'eval_score': 0.499}\n",
      "{'n': 3, 'n_estimators': 250, 'max_depth': 3, 'learning_rate': 0.1946909770424945, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.485, 'f1_pos': 0.433, 'f1_neu': 0.39, 'f1_neg': 0.574, 'eval_score': 0.497}\n",
      "{'n': 143, 'n_estimators': 750, 'max_depth': 6, 'learning_rate': 0.07293032219233501, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.498, 'f1_neu': 0.473, 'f1_neg': 0.632, 'eval_score': 0.56}\n",
      "{'n': 103, 'n_estimators': 350, 'max_depth': 4, 'learning_rate': 0.058824949736071774, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.512, 'f1_neu': 0.475, 'f1_neg': 0.602, 'eval_score': 0.551}\n",
      "{'n': 23, 'n_estimators': 50, 'max_depth': 6, 'learning_rate': 0.10294238424634197, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.521, 'f1_pos': 0.494, 'f1_neu': 0.455, 'f1_neg': 0.587, 'eval_score': 0.534}\n",
      "{'n': 163, 'n_estimators': 650, 'max_depth': 7, 'learning_rate': 0.06803423580491501, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.515, 'f1_neu': 0.478, 'f1_neg': 0.608, 'eval_score': 0.555}\n",
      "{'n': 103, 'n_estimators': 950, 'max_depth': 7, 'learning_rate': 0.044610810232090015, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.498, 'f1_neu': 0.463, 'f1_neg': 0.594, 'eval_score': 0.54}\n",
      "{'n': 183, 'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.06232669078055613, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.506, 'f1_neu': 0.471, 'f1_neg': 0.648, 'eval_score': 0.572}\n",
      "{'n': 103, 'n_estimators': 350, 'max_depth': 3, 'learning_rate': 0.01638212752586492, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.499, 'f1_neu': 0.472, 'f1_neg': 0.587, 'eval_score': 0.538}\n",
      "{'n': 143, 'n_estimators': 850, 'max_depth': 9, 'learning_rate': 0.12268297043542031, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.486, 'f1_neu': 0.467, 'f1_neg': 0.649, 'eval_score': 0.565}\n",
      "{'n': 183, 'n_estimators': 450, 'max_depth': 8, 'learning_rate': 0.12958048820805979, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.515, 'f1_neu': 0.472, 'f1_neg': 0.608, 'eval_score': 0.555}\n",
      "{'n': 123, 'n_estimators': 850, 'max_depth': 9, 'learning_rate': 0.08213464538123944, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.485, 'f1_neu': 0.473, 'f1_neg': 0.653, 'eval_score': 0.567}\n",
      "{'n': 183, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.13319556487580236, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.488, 'f1_neu': 0.47, 'f1_neg': 0.651, 'eval_score': 0.567}\n",
      "{'n': 183, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.14894803818120472, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.494, 'f1_neu': 0.473, 'f1_neg': 0.649, 'eval_score': 0.569}\n",
      "{'n': 183, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.1531404187427698, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.518, 'f1_neu': 0.472, 'f1_neg': 0.605, 'eval_score': 0.554}\n",
      "{'n': 183, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.005051840001673913, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.509, 'f1_neu': 0.478, 'f1_neg': 0.628, 'eval_score': 0.563}\n",
      "{'n': 183, 'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.1470254376193649, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.503, 'f1_neu': 0.466, 'f1_neg': 0.611, 'eval_score': 0.551}\n",
      "{'n': 183, 'n_estimators': 250, 'max_depth': 8, 'learning_rate': 0.11470769331242009, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.517, 'f1_neu': 0.481, 'f1_neg': 0.619, 'eval_score': 0.562}\n",
      "{'n': 23, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.08327189632297687, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.521, 'f1_pos': 0.458, 'f1_neu': 0.445, 'f1_neg': 0.604, 'eval_score': 0.528}\n",
      "{'n': 63, 'n_estimators': 450, 'max_depth': 5, 'learning_rate': 0.08700674044861503, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.506, 'f1_neu': 0.463, 'f1_neg': 0.595, 'eval_score': 0.544}\n",
      "{'n': 123, 'n_estimators': 150, 'max_depth': 8, 'learning_rate': 0.14844365243636917, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.523, 'f1_pos': 0.466, 'f1_neu': 0.45, 'f1_neg': 0.603, 'eval_score': 0.531}\n",
      "{'n': 183, 'n_estimators': 250, 'max_depth': 6, 'learning_rate': 0.16264770739564266, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.503, 'f1_neu': 0.481, 'f1_neg': 0.639, 'eval_score': 0.567}\n",
      "{'n': 183, 'n_estimators': 150, 'max_depth': 5, 'learning_rate': 0.03527834933700798, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.523, 'f1_neu': 0.476, 'f1_neg': 0.602, 'eval_score': 0.555}\n",
      "{'n': 83, 'n_estimators': 550, 'max_depth': 8, 'learning_rate': 0.11625736660223017, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.506, 'f1_neu': 0.469, 'f1_neg': 0.597, 'eval_score': 0.545}\n",
      "100%|██████████| 35/35 [46:34<00:00, 79.85s/trial, best loss: -0.572]\n",
      "{'boosting_type': 0, 'learning_rate': 0.06232669078055613, 'max_depth': 5, 'min_split_gain': 0, 'n': 9, 'n_estimators': 2}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB_PCA(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "#     subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "    n = args['n']\n",
    "    \n",
    "    par_dict = {'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': 1, 'min_split_gain': min_split_gain} \n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=1, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(train_pr_tfidf,train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,1001, step =100)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "#      'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB_PCA = fmin(objective_func_LGB_PCA, space, algo=tpe.suggest, max_evals=35)\n",
    "print(best_classifier_LGB_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt3WGISKa9_U"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "romney_train_pr_tfidf_smote, romney_train_pr_class = sm.fit_sample(romney_train_pr_tfidf, romney_train_pr_df['Sentiment'] )\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "obama_train_pr_tfidf_smote, obama_train_pr_class = sm.fit_sample(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
