{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"Twitter_Sentiment_Analysis_CNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"78PhOL0jpZdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604769927544,"user_tz":360,"elapsed":27501,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"033f0595-d70b-4f3c-ea47-34d7becad712"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AEM1ZS6vpu6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604769927545,"user_tz":360,"elapsed":27495,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"ef306ad4-e569-4e1e-9179-acb9f2d8ddb8"},"source":["%cd /content/drive/My Drive/Research Project"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QsHo75fupasE"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","# import src\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ePRqI5M3qPi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604769932335,"user_tz":360,"elapsed":32282,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"098c399e-d879-4d81-fe10-8437995bee3d"},"source":["pip install -U mittens"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting mittens\n","  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.18.5)\n","Installing collected packages: mittens\n","Successfully installed mittens-0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hgktrAVlpZdb"},"source":["from numpy import asarray\n","from numpy import zeros\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","\n","from statistics import mean, stdev, median, mode\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from mittens import GloVe, Mittens\n","from hyperopt import fmin, tpe, hp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W16zYA4bumpz"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.models import load_model\n","\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import layers\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Conv3D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.layers import MaxPool3D, AveragePooling3D\n","\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import AveragePooling3D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO6oNPy7px_h"},"source":["# pip install autocorrect"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BALelbZyNU02","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604769936316,"user_tz":360,"elapsed":36257,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"86a9b35c-26b6-4cf0-b705-7374b783f2d1"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"w8gGSNnE_Ngv"},"source":["import tensorflow_addons as tfa\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n","import tensorflow.keras.backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8nyt2ltYpZd5"},"source":["### Getting Data"]},{"cell_type":"code","metadata":{"id":"j06_FWdeump7"},"source":["# data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NdQPWPwMump9"},"source":["data_path = r'/content/drive/My Drive/Research Project/Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-I9YTbJ1ump-"},"source":["romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training Data.csv'), usecols = [1,2]).dropna()\n","romney_val_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Validation Data.csv'), usecols = [1,2]).dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AD9U00CVddNC","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1604769936777,"user_tz":360,"elapsed":36707,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"e7624fcd-6cf4-4fbe-d2db-62a0db959912"},"source":["romney_train_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>believe romney able work w bipartisim gov brin...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ask romney past decade tax end obama debate pr...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>conclusion romney get own</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>register democrat nothing give reason change v...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>aunt buy theiphone tell need vote romne shecray</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  believe romney able work w bipartisim gov brin...  Positive\n","1  ask romney past decade tax end obama debate pr...  Positive\n","2                          conclusion romney get own  Positive\n","3  register democrat nothing give reason change v...  Positive\n","4    aunt buy theiphone tell need vote romne shecray   Neutral"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"-4LNaGHqumqC","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1604769936933,"user_tz":360,"elapsed":36857,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"c005eeef-0f0e-4eb8-935d-2e331388b828"},"source":["romney_val_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>vote romney waystogetshot</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>waystogetshot voting romney</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>romney go stand china afraid go view</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>swing state poll woman push romney lead via</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>least job come back romney nba team outsource ...</td>\n","      <td>Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0                          vote romney waystogetshot   Neutral\n","1                        waystogetshot voting romney  Negative\n","2               romney go stand china afraid go view   Neutral\n","3        swing state poll woman push romney lead via   Neutral\n","4  least job come back romney nba team outsource ...  Negative"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"H2dIAGCPumqE"},"source":["romney_train1_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2]).dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkmE5CIKumqG","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1604769937121,"user_tz":360,"elapsed":37038,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"7d6c94c5-8c63-46bc-88d8-4d7d4fb20544"},"source":["romney_train1_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>insidious mitt romney bain help philip morris ...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mean like romney cheat primary</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>mitt romney still believe black president</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>romney tax plan deserve nd look secret one dif...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hope romney debate prepped people last time</td>\n","      <td>Positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  insidious mitt romney bain help philip morris ...  Negative\n","1                     mean like romney cheat primary  Negative\n","2          mitt romney still believe black president  Negative\n","3  romney tax plan deserve nd look secret one dif...  Negative\n","4        hope romney debate prepped people last time  Positive"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"9ZNsce8mumqI"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qJBsX8hHumqK"},"source":["\n","## Building CNN Models"]},{"cell_type":"code","metadata":{"id":"FiqzWfYwa-ir"},"source":["\n","\n","# int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","# embedded_sequences = embedding_layer(int_sequences_input)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n","# x = layers.MaxPooling1D(5)(x)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n","# x = layers.MaxPooling1D(5)(x)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n","# x = layers.GlobalMaxPooling1D()(x)\n","# x = layers.Dense(128, activation=\"relu\")(x)\n","# x = layers.Dropout(0.5)(x)\n","# preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n","# model = keras.Model(int_sequences_input, preds)\n","# model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt4Q-LLu_Nh6"},"source":["def one_hot(data):\n","    data = np.asarray(data)\n","    temp = np.zeros((len(data),3))\n","#     print(data[0])\n","    for i in range(len(temp)):\n","        if data[i] == 'Negative':\n","            temp[i][2] = 1 ## Negative sentiment third neuron\n","        elif data[i] == 'Neutral':\n","            temp[i][1] = 1 ## Neutral sentiment second neuron  \n","        else:\n","            temp[i][0] = 1 ## Positive sentiment first neuron \n","\n","    return temp\n","    \n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETvx1WoKfrfk"},"source":["def pred(x):\n","    temp = []\n","    for i in x:\n","        m = np.argmax(i)\n","        if m == 0:\n","            temp.append('Positive')\n","        elif m == 1:\n","            temp.append('Neutral')\n","        else:\n","            temp.append('Negative')\n","    return temp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmGshHslpZeH"},"source":["## Building Glove Dictionary"]},{"cell_type":"code","metadata":{"id":"YU-SxaOZ_Nh9"},"source":["embeddings = {}\n","with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n","    for line in file:\n","        values = line.split()\n","        word = values[0]\n","        vector = asarray(values[1:], dtype='float32')\n","        embeddings[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or3hmQvdpZeJ"},"source":["## Embedding Matrix Function"]},{"cell_type":"code","metadata":{"id":"wBG3Kq7EpZeJ"},"source":["def emb_matrix(t,embeddings, we_dim):\n","    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n","    vocab_size = len(t.word_index) + 1    \n","    embedding_matrix = zeros((vocab_size, we_dim))\n","    for word, i in t.word_index.items():\n","        embedding_vector = embeddings.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kT9ZAXi_NiC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgK6d3Qy_NiF"},"source":["## Fine tuning the word embeddings of 300 dimensions using mittens library\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VqwJmPT-_NiF"},"source":["## Used the code for finetuning from the following link:\n","### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"]},{"cell_type":"code","metadata":{"id":"HRwhIKRT_NiF"},"source":["def finetune(training): \n","    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n","    #training_tokens\n","\n","    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n","    print(len(oov))\n","\n","    corp_vocab = list(set(oov))\n","\n","    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n","    trr =''\n","    for i in training_tokens:\n","        for j in i:\n","            trr+= j\n","            trr += ' '\n","\n","    # print(trr)\n","    # print(z)\n","    X = cv.fit_transform([trr])\n","    Xc = (X.T * X)\n","    Xc.setdiag(0)\n","    coocc_ar = Xc.toarray()\n","\n","    mittens_model = Mittens(n=200, max_iter=len(oov)+200)\n","\n","    new_embeddings = mittens_model.fit(\n","      coocc_ar,\n","      vocab=corp_vocab,\n","      initial_embedding_dict= embeddings)\n","\n","    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n","    return training_tokens, new_embeddings\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB2Nd-Kr_NiH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604770029691,"user_tz":360,"elapsed":129592,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"71cab24e-faf4-4203-b608-356dc611d7e6"},"source":["embeddings2= embeddings.copy()\n","\n","training_tokens, new_embeddings = finetune(romney_train_pr_df)\n","embeddings2.update(new_embeddings)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["573\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["Iteration 770: loss: 0.011159378103911877"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"spGvX_Cg_NiI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604770029692,"user_tz":360,"elapsed":129588,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"5fbb1914-4617-4a52-a280-8d4ca6cdd44e"},"source":["oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n","print(len(oov2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ir2jPLOS_NiK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3VcHx_CduFW"},"source":["## Using CNN"]},{"cell_type":"code","metadata":{"id":"t9xXo3kBdnDc"},"source":["def cross_valid_cnn(X,y,epochs,batch_size,max_length, learning_rate, we_dim):\n","    f1_Positive  =[]\n","    f1_Neutral =[]\n","    f1_Negative =[]\n","    acc =[]\n","    cv = KFold(n_splits=5,shuffle=True)\n","    for train_index, val_index in cv.split(X):\n","    #     print(\"Train Index: \", train_index, \"\\n\")\n","    #     print(\"Test Index: \", test_index)\n","    #     print(X.iloc[train_index])\n","    #     print(f)\n","\n","        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n","        tokenise_tf = Tokenizer()\n","        tokenise_tf.fit_on_texts(X_train1) \n","    \n","        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n","        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n","\n","        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n","        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","        embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","        int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","        embedded_sequences = embedding_layer(int_sequences_input)\n","        x = layers.Conv1D(128, 3)(embedded_sequences)\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU()(x)\n","        x = layers.MaxPooling1D(2)(x)\n","        x = layers.Conv1D(128, 3)(x)\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU()(x)\n","        x = layers.MaxPooling1D(2)(x)\n","\n","        x = layers.Conv1D(128, 3)(x)\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU()(x)\n","        x = layers.GlobalMaxPooling1D()(x)\n","        # x = layers.MaxPooling1D(2)(x)\n","\n","        x = layers.Flatten()(x)\n","        x = layers.Dense(128, activation=\"relu\")(x)\n","        x = layers.Dropout(0.2)(x)\n","        x = layers.Dense(128, activation=\"relu\")(x)\n","        x = layers.Dropout(0.2)(x)\n","        preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","        model = tf.keras.Model(int_sequences_input, preds)\n","        # print(model.summary())\n","        # print(z)\n","        model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        y_pred_temp = model.predict(validation_padded)\n","        y_pred = pred(y_pred_temp)\n","        f1_temp = f1_score(y_val, y_pred, average = None)\n","\n","        f1_Positive.append(f1_temp[2])\n","        f1_Neutral.append(f1_temp[1])\n","        f1_Negative.append(f1_temp[0])\n","\n","        acc.append(round(accuracy_score(y_val, y_pred),3))\n","\n","    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZQ_WOJe96xsa"},"source":["def model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(64, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    x = layers.Conv1D(128, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","\n","    x = layers.Conv1D(256, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","    # x = layers.MaxPooling1D(2)(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-q5ICwzkjY8"},"source":["def model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(32, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    x = layers.Conv1D(64, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixABj5x5kj33"},"source":["def model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(64, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JebvbtfskGBZ"},"source":["# max_length = 31\n","# epochs = 2\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","# par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","# print(par_dict)\n","\n","# tokenise_tf = Tokenizer()\n","# tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text']) \n","\n","# encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n","# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n","\n","# encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n","# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","# try: \n","#   model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#   history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","# except:\n","#   try: \n","#     model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","#   except:\n","\n","#     model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","\n","# y_pred_temp = model.predict(validation_padded)\n","# y_pred = pred(y_pred_temp)\n","# f1_temp = f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None)\n","\n","# f1_Positive = f1_temp[2]\n","# f1_Neutral = f1_temp[1]\n","# f1_Negative = f1_temp[0]\n","\n","# accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3) \n","# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n","# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","# print('\\n')\n","# print(eval_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Cl2Z9PW8CEy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1604772781175,"user_tz":360,"elapsed":1161790,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"e32c741e-af18-4033-ad64-83b01d06759b"},"source":["def objective_func_CNN(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","\n","    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","    print(par_dict)\n","\n","    tokenise_tf = Tokenizer()\n","    tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text']) \n","\n","    encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n","    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n","\n","    encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n","    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","    try: \n","        model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","        history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","    except:\n","        try:            \n","            model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","            history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        except:\n","\n","            model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","            history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","\n","    y_pred_temp = model.predict(validation_padded)\n","    y_pred = pred(y_pred_temp)\n","    f1_temp = np.round(f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None),3)\n","\n","    f1_Positive = f1_temp[2]\n","    f1_Neutral = f1_temp[1]\n","    f1_Negative = f1_temp[0]\n","\n","    accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3) \n","    eval_score = round(mean([0.4*accuracy, 1.6*f1_Positive]),3)   \n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","    print('\\n')\n","    print(eval_dict)\n","\n","    return -(eval_score)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","        }                  \n","                                    \n","we_dim = 200                               \n","best_CNN = fmin(objective_func_CNN, space, algo=tpe.suggest, max_evals=20)\n","print(best_CNN)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'max_length': 15, 'batch_size': 32, 'learning_rate': 0.0027327740726435803, 'epochs': 16}\n","{'accuracy': 0.189, 'f1_pos': 0.318, 'f1_neu': 0.0, 'f1_neg': 0.0, 'eval_score': 0.292}\n","{'max_length': 59, 'batch_size': 128, 'learning_rate': 0.005185465135720898, 'epochs': 25}\n","{'accuracy': 0.504, 'f1_pos': 0.444, 'f1_neu': 0.44, 'f1_neg': 0.584, 'eval_score': 0.456}\n","{'max_length': 59, 'batch_size': 64, 'learning_rate': 0.007631126012370996, 'epochs': 24}\n","{'accuracy': 0.539, 'f1_pos': 0.442, 'f1_neu': 0.403, 'f1_neg': 0.658, 'eval_score': 0.461}\n","{'max_length': 43, 'batch_size': 64, 'learning_rate': 0.00908746835652521, 'epochs': 15}\n","{'accuracy': 0.581, 'f1_pos': 0.35, 'f1_neu': 0.365, 'f1_neg': 0.723, 'eval_score': 0.396}\n","{'max_length': 55, 'batch_size': 128, 'learning_rate': 0.0026277353339704346, 'epochs': 6}\n","{'accuracy': 0.475, 'f1_pos': 0.435, 'f1_neu': 0.431, 'f1_neg': 0.535, 'eval_score': 0.443}\n","{'max_length': 35, 'batch_size': 128, 'learning_rate': 0.007686654539422726, 'epochs': 11}\n","{'accuracy': 0.577, 'f1_pos': 0.444, 'f1_neu': 0.425, 'f1_neg': 0.699, 'eval_score': 0.471}\n","{'max_length': 29, 'batch_size': 64, 'learning_rate': 0.0004279930460137649, 'epochs': 8}\n","{'accuracy': 0.517, 'f1_pos': 0.407, 'f1_neu': 0.341, 'f1_neg': 0.645, 'eval_score': 0.429}\n","{'max_length': 52, 'batch_size': 64, 'learning_rate': 0.002162576047297713, 'epochs': 24}\n","{'accuracy': 0.508, 'f1_pos': 0.448, 'f1_neu': 0.377, 'f1_neg': 0.619, 'eval_score': 0.46}\n","{'max_length': 22, 'batch_size': 32, 'learning_rate': 0.0015064883870171152, 'epochs': 12}\n","{'accuracy': 0.573, 'f1_pos': 0.422, 'f1_neu': 0.394, 'f1_neg': 0.703, 'eval_score': 0.452}\n","{'max_length': 51, 'batch_size': 64, 'learning_rate': 0.001611981011793252, 'epochs': 15}\n","{'accuracy': 0.526, 'f1_pos': 0.487, 'f1_neu': 0.377, 'f1_neg': 0.63, 'eval_score': 0.495}\n","{'max_length': 56, 'batch_size': 64, 'learning_rate': 0.0026778294358729715, 'epochs': 27}\n","{'accuracy': 0.536, 'f1_pos': 0.421, 'f1_neu': 0.386, 'f1_neg': 0.654, 'eval_score': 0.444}\n","{'max_length': 31, 'batch_size': 32, 'learning_rate': 0.003383627093303141, 'epochs': 5}\n","{'accuracy': 0.525, 'f1_pos': 0.422, 'f1_neu': 0.468, 'f1_neg': 0.609, 'eval_score': 0.443}\n","{'max_length': 48, 'batch_size': 32, 'learning_rate': 0.000637174954641766, 'epochs': 27}\n","{'accuracy': 0.517, 'f1_pos': 0.426, 'f1_neu': 0.32, 'f1_neg': 0.647, 'eval_score': 0.444}\n","{'max_length': 10, 'batch_size': 64, 'learning_rate': 0.004110026856671702, 'epochs': 5}\n","{'accuracy': 0.529, 'f1_pos': 0.327, 'f1_neu': 0.465, 'f1_neg': 0.637, 'eval_score': 0.367}\n","{'max_length': 49, 'batch_size': 32, 'learning_rate': 0.0018091817081472361, 'epochs': 17}\n","{'accuracy': 0.499, 'f1_pos': 0.369, 'f1_neu': 0.428, 'f1_neg': 0.607, 'eval_score': 0.395}\n","{'max_length': 31, 'batch_size': 32, 'learning_rate': 0.005579625391373141, 'epochs': 18}\n","{'accuracy': 0.554, 'f1_pos': 0.472, 'f1_neu': 0.317, 'f1_neg': 0.688, 'eval_score': 0.488}\n","{'max_length': 35, 'batch_size': 64, 'learning_rate': 0.008712702146244103, 'epochs': 5}\n","{'accuracy': 0.595, 'f1_pos': 0.455, 'f1_neu': 0.223, 'f1_neg': 0.73, 'eval_score': 0.483}\n","{'max_length': 58, 'batch_size': 32, 'learning_rate': 0.003948695205431789, 'epochs': 29}\n","{'accuracy': 0.548, 'f1_pos': 0.442, 'f1_neu': 0.406, 'f1_neg': 0.673, 'eval_score': 0.463}\n","{'max_length': 55, 'batch_size': 64, 'learning_rate': 0.009886112045508738, 'epochs': 26}\n","{'accuracy': 0.534, 'f1_pos': 0.416, 'f1_neu': 0.461, 'f1_neg': 0.634, 'eval_score': 0.44}\n","{'max_length': 31, 'batch_size': 32, 'learning_rate': 0.008617452899422114, 'epochs': 23}\n","{'accuracy': 0.573, 'f1_pos': 0.438, 'f1_neu': 0.251, 'f1_neg': 0.72, 'eval_score': 0.465}\n","100%|██████████| 20/20 [19:21<00:00, 58.07s/it, best loss: -0.495]\n","{'batch_size': 1, 'epochs': 10, 'learning_rate': 0.001611981011793252, 'max_length': 47}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mt-GTpvAumqn"},"source":["{'max_length': 31, 'batch_size': 128, 'learning_rate': 0.00043843544096825674, 'epochs': 7}\n","{'accuracy': 0.581, 'f1_pos': 0.474, 'f1_neu': 0.26, 'f1_neg': 0.727, 'eval_score': 0.594}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hfm558S9d2RH"},"source":["# max_length = 30\n","# epochs = 20\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","\n","# accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n","# f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","# print('\\n')\n","# print(eval_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0w0bGDBrC9x"},"source":["# def objective_func_CNN_CV(args):\n","#     max_length = args['max_length']\n","#     batch_size = args['batch_size']\n","#     learning_rate = args['learning_rate']\n","#     epochs = args['epochs']\n","\n","#     par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","#     print(par_dict)\n","\n","#     accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n","#     f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","#     print('\\n')\n","#     print(eval_dict)\n","\n","#     return -(f1)\n","\n","# space = {'max_length': hp.choice('max_length',range(4,60)),  \n","#         'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","#          'epochs': hp.choice('epochs',range(5,30)), \n","#          'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","#         }                  \n","                                    \n","# we_dim = 200                               \n","# best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=20)\n","# print(best_CNN_CV)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FhdoX1kd2Y4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldH6dPIM_NiL"},"source":["## Custom F1 value"]},{"cell_type":"code","metadata":{"id":"9Q5Q1HVs_NiM"},"source":["def f1_value(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":null,"outputs":[]}]}