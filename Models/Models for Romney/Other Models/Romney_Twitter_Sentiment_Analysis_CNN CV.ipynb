{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Twitter_Sentiment_Analysis_CNN CV.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"78PhOL0jpZdY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169648941,"user_tz":360,"elapsed":15453,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"bb217b26-f1c9-4ea0-d3fb-0cc5d2dc64a6"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AEM1ZS6vpu6b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169648943,"user_tz":360,"elapsed":15448,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"daebff28-2032-4dcb-96a0-b367d33d911c"},"source":["%cd /content/drive/My Drive/Research Project"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QsHo75fupasE"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","# import src\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ePRqI5M3qPi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169653248,"user_tz":360,"elapsed":19747,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"80bfd8d8-2f23-4724-889f-13f26ebc83b2"},"source":["pip install -U mittens"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting mittens\n","  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.18.5)\n","Installing collected packages: mittens\n","Successfully installed mittens-0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hgktrAVlpZdb"},"source":["from numpy import asarray\n","from numpy import zeros\n","\n","from sklearn.utils import shuffle\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","\n","from statistics import mean, stdev, median, mode\n","\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","from mittens import GloVe, Mittens\n","from hyperopt import fmin, tpe, hp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GUN7Yj7nuVbI"},"source":["import tensorflow as tf\n","\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.models import load_model\n","\n","from tensorflow.keras.layers import Embedding\n","from tensorflow.keras import layers\n","\n","from tensorflow.keras.layers import Input\n","from tensorflow.keras.layers import Conv3D\n","from tensorflow.keras.layers import BatchNormalization\n","from tensorflow.keras.layers import LeakyReLU\n","from tensorflow.keras.layers import PReLU\n","from tensorflow.keras.layers import MaxPool3D, AveragePooling3D\n","\n","from tensorflow.keras.layers import Dropout\n","from tensorflow.keras.layers import AveragePooling3D\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO6oNPy7px_h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169659099,"user_tz":360,"elapsed":25590,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"1c2ef1b3-76e9-49af-ce08-0b55c908cabd"},"source":["pip install autocorrect"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting autocorrect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n","\r\u001b[K     |▌                               | 10kB 32.6MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 16.1MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 13.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 13.1MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 9.0MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 9.3MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 10.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 9.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 276kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 317kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 358kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 552kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 8.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 593kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 8.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 8.7MB/s \n","\u001b[?25hBuilding wheels for collected packages: autocorrect\n","  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=e2e9cd54d14b06e359e01f09bb456224cb8998d8ee0c391f8cb4dacb853eff5b\n","  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n","Successfully built autocorrect\n","Installing collected packages: autocorrect\n","Successfully installed autocorrect-2.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BALelbZyNU02","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606169659399,"user_tz":360,"elapsed":25886,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"bbe9579e-1290-4aec-fabd-bf80d3e7c2f3"},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"w8gGSNnE_Ngv"},"source":["import tensorflow_addons as tfa\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n","import tensorflow.keras.backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8nyt2ltYpZd5"},"source":["### Getting Data"]},{"cell_type":"code","metadata":{"id":"brNOJOiruVbR"},"source":["# data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBEabTTCuVbT"},"source":["data_path = r'/content/drive/My Drive/Research Project/Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cKN2GayGuVbV"},"source":["train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training Data.csv'), usecols = [1,2]).dropna()\n","val_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Validation Data.csv'), usecols = [1,2]).dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AD9U00CVddNC","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606169661275,"user_tz":360,"elapsed":27751,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"5e23c245-affd-43f8-e99d-c6230ec8199f"},"source":["train_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>believe romney able work w bipartisim gov brin...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ask romney past decade tax end obama debate pr...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>conclusion romney get own</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>register democrat nothing give reason change v...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>aunt buy theiphone tell need vote romne shecray</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  believe romney able work w bipartisim gov brin...  Positive\n","1  ask romney past decade tax end obama debate pr...  Positive\n","2                          conclusion romney get own  Positive\n","3  register democrat nothing give reason change v...  Positive\n","4    aunt buy theiphone tell need vote romne shecray   Neutral"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"bTPn_w6huVbZ","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606169661276,"user_tz":360,"elapsed":27746,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"fb24ce3d-32e7-4173-ee98-8a16e27883db"},"source":["val_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>vote romney waystogetshot</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>waystogetshot voting romney</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>romney go stand china afraid go view</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>swing state poll woman push romney lead via</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>least job come back romney nba team outsource ...</td>\n","      <td>Negative</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0                          vote romney waystogetshot   Neutral\n","1                        waystogetshot voting romney  Negative\n","2               romney go stand china afraid go view   Neutral\n","3        swing state poll woman push romney lead via   Neutral\n","4  least job come back romney nba team outsource ...  Negative"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"O3oY-bDquVbb"},"source":["train1_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2]).dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u98CSNUXuVbd","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1606169661277,"user_tz":360,"elapsed":27739,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"62474a46-f897-4723-c9ab-fbc2b3697c19"},"source":["train1_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>insidious mitt romney bain help philip morris ...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mean like romney cheat primary</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>mitt romney still believe black president</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>romney tax plan deserve nd look secret one dif...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>hope romney debate prepped people last time</td>\n","      <td>Positive</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  insidious mitt romney bain help philip morris ...  Negative\n","1                     mean like romney cheat primary  Negative\n","2          mitt romney still believe black president  Negative\n","3  romney tax plan deserve nd look secret one dif...  Negative\n","4        hope romney debate prepped people last time  Positive"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"k6_TpKyMuVbe"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ef3GV9SzuVbg"},"source":["\n","## Building CNN Models"]},{"cell_type":"code","metadata":{"id":"FiqzWfYwa-ir"},"source":["\n","\n","# int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","# embedded_sequences = embedding_layer(int_sequences_input)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n","# x = layers.MaxPooling1D(5)(x)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n","# x = layers.MaxPooling1D(5)(x)\n","# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n","# x = layers.GlobalMaxPooling1D()(x)\n","# x = layers.Dense(128, activation=\"relu\")(x)\n","# x = layers.Dropout(0.5)(x)\n","# preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n","# model = keras.Model(int_sequences_input, preds)\n","# model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt4Q-LLu_Nh6"},"source":["def one_hot(data):\n","    data = np.asarray(data)\n","    temp = np.zeros((len(data),3))\n","#     print(data[0])\n","    for i in range(len(temp)):\n","        if data[i] == 'Negative':\n","            temp[i][2] = 1 ## Negative sentiment third neuron\n","        elif data[i] == 'Neutral':\n","            temp[i][1] = 1 ## Neutral sentiment second neuron  \n","        else:\n","            temp[i][0] = 1 ## Positive sentiment first neuron \n","\n","    return temp\n","    \n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETvx1WoKfrfk"},"source":["def pred(x):\n","    temp = []\n","    for i in x:\n","        m = np.argmax(i)\n","        if m == 0:\n","            temp.append('Positive')\n","        elif m == 1:\n","            temp.append('Neutral')\n","        else:\n","            temp.append('Negative')\n","    return temp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmGshHslpZeH"},"source":["## Building Glove Dictionary"]},{"cell_type":"code","metadata":{"id":"YU-SxaOZ_Nh9"},"source":["embeddings = {}\n","with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n","    for line in file:\n","        values = line.split()\n","        word = values[0]\n","        vector = asarray(values[1:], dtype='float32')\n","        embeddings[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or3hmQvdpZeJ"},"source":["## Embedding Matrix Function"]},{"cell_type":"code","metadata":{"id":"wBG3Kq7EpZeJ"},"source":["def emb_matrix(t,embeddings, we_dim):\n","    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n","    vocab_size = len(t.word_index) + 1    \n","    embedding_matrix = zeros((vocab_size, we_dim))\n","    for word, i in t.word_index.items():\n","        embedding_vector = embeddings.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kT9ZAXi_NiC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgK6d3Qy_NiF"},"source":["## Fine tuning the word embeddings of 300 dimensions using mittens library\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VqwJmPT-_NiF"},"source":["## Used the code for finetuning from the following link:\n","### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"]},{"cell_type":"code","metadata":{"id":"HRwhIKRT_NiF"},"source":["def finetune(training): \n","    training_tokens = [word_tokenize(i) for i in training]\n","    #training_tokens\n","\n","    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n","    # print(len(oov))\n","\n","    corp_vocab = list(set(oov))\n","\n","    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n","    trr =''\n","    for i in training_tokens:\n","        for j in i:\n","            trr+= j\n","            trr += ' '\n","\n","    # print(trr)\n","    # print(z)\n","    X = cv.fit_transform([trr])\n","    Xc = (X.T * X)\n","    Xc.setdiag(0)\n","    coocc_ar = Xc.toarray()\n","\n","    mittens_model = Mittens(n=200, max_iter=len(oov)+200,  display_progress = 0)\n","\n","    new_embeddings = mittens_model.fit(\n","      coocc_ar,\n","      vocab=corp_vocab,\n","      initial_embedding_dict= embeddings)\n","\n","    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n","    return training_tokens, new_embeddings\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB2Nd-Kr_NiH"},"source":["# embeddings2= embeddings.copy()\n","\n","# training_tokens, new_embeddings = finetune(train1_pr_df)\n","# embeddings2.update(new_embeddings)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"spGvX_Cg_NiI"},"source":["# oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n","# print(len(oov2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ir2jPLOS_NiK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"V3VcHx_CduFW"},"source":["## Using CNN"]},{"cell_type":"code","metadata":{"id":"ZQ_WOJe96xsa"},"source":["def model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(64, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    x = layers.Conv1D(128, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","\n","    x = layers.Conv1D(256, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","    # x = layers.MaxPooling1D(2)(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-q5ICwzkjY8"},"source":["def model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(32, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.MaxPooling1D(2)(x)\n","    x = layers.Conv1D(64, 3)(x)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ixABj5x5kj33"},"source":["def model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","\n","    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n","    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n","    embedded_sequences = embedding_layer(int_sequences_input)\n","\n","    x = layers.Conv1D(64, 3)(embedded_sequences)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU()(x)\n","    x = layers.GlobalMaxPooling1D()(x)\n","\n","    x = layers.Flatten()(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    x = layers.Dense(128, activation=\"tanh\")(x)\n","    x = layers.Dropout(0.2)(x)\n","    preds = layers.Dense(3, activation=\"softmax\")(x)\n","\n","    model = tf.keras.Model(int_sequences_input, preds)\n","    # print(model.summary())\n","    # print(z)\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JebvbtfskGBZ"},"source":["# max_length = 31\n","# epochs = 2\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","# par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","# print(par_dict)\n","\n","# tokenise_tf = Tokenizer()\n","# tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text']) \n","\n","# encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n","# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n","\n","# encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n","# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","# try: \n","#   model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#   history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","# except:\n","#   try: \n","#     model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","#   except:\n","\n","#     model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","\n","# y_pred_temp = model.predict(validation_padded)\n","# y_pred = pred(y_pred_temp)\n","# f1_temp = f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None)\n","\n","# f1_Positive = f1_temp[2]\n","# f1_Neutral = f1_temp[1]\n","# f1_Negative = f1_temp[0]\n","\n","# accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3) \n","# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n","# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","# print('\\n')\n","# print(eval_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bha0yomFR2jh"},"source":["def cross_valid_cnn(X,y,epochs,batch_size,max_length, learning_rate, we_dim):\n","    # print('dfd')\n","    f1_Positive  =[]\n","    f1_Neutral =[]\n","    f1_Negative =[]\n","    eval =[]\n","    acc =[]\n","    cv = KFold(n_splits=5,shuffle=True)\n","    for train_index, val_index in cv.split(X):\n","    #     print(\"Train Index: \", train_index, \"\\n\")\n","    #     print(\"Test Index: \", test_index)\n","    #     print(X.iloc[train_index])\n","    #     print(f)\n","\n","        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n","        tokenise_tf = Tokenizer()\n","        tokenise_tf.fit_on_texts(X_train1) \n","    \n","        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n","        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","        # print(X_train1['Doc Text'])\n","\n","        embeddings2= embeddings.copy()\n","\n","        training_tokens, new_embeddings = finetune(X_train1)\n","        embeddings2.update(new_embeddings)\n","\n","        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2, we_dim)\n","\n","        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n","        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","        # print('dfd')\n","        try: \n","            model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","            history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","        except:\n","            try:            \n","                model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","                history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","            except:\n","\n","                model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","                history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        y_pred_temp = model.predict(validation_padded)\n","        y_pred = pred(y_pred_temp)\n","        f1_temp = f1_score(y_val, y_pred, average = None)\n","\n","        f1_Positive.append(f1_temp[2])\n","        f1_Neutral.append(f1_temp[1])\n","        f1_Negative.append(f1_temp[0])\n","\n","        accuracy = round(accuracy_score(y_val, y_pred),3)\n","        acc.append(accuracy)\n","        eval_score = round(mean([0.4*accuracy, 1.6*f1_temp[2]]),3)\n","        eval.append(eval_score)\n","        # print(eval_score)\n","\n","    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3), round(mean(eval),3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wF3qUEsLVtVF"},"source":["# accuracy, f1_Positive, f1_Neutral, f1_Negative, eval_score = cross_valid_cnn(train1_pr_df['Doc Text'],train1_pr_df['Sentiment'], 2,32,20, 0.001, 200) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4uP1sfAXYlYf"},"source":["# print(dfaddf)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ks7U1CTVSiWT"},"source":["# def objective_func_CNN_CV(args):\n","#     max_length = args['max_length']\n","#     batch_size = args['batch_size']\n","#     learning_rate = args['learning_rate']\n","#     epochs = args['epochs']\n","\n","#     par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","#     print(par_dict)\n","\n","#     accuracy, f1_Positive, f1_Neutral, f1_Negative, eval_score = cross_valid_cnn(train1_pr_df['Doc Text'],train1_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)  \n","#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","#     print('\\n')\n","#     print(eval_dict)\n","\n","#     return -(eval_score)\n","\n","# space = {'max_length': hp.choice('max_length',range(5,65,5)),  \n","#         'batch_size': hp.choice('batch_size', [16, 32, 64]),\n","#          'epochs': hp.choice('epochs',range(5,30)), \n","#          'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","#         }                  \n","                                    \n","# we_dim = 200                               \n","# best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=15)\n","# print(best_CNN_CV)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jAbr3Ba5Sws_","executionInfo":{"status":"ok","timestamp":1606169722214,"user_tz":360,"elapsed":88645,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"949cad6c-8861-48cd-b38f-efc4d22d7bcf"},"source":["{'max_length': 30, 'batch_size': 64, 'learning_rate': 0.006651858136703583, 'epochs': 6}\n","{'accuracy': 0.562, 'f1_pos': 0.453, 'f1_neu': 0.334, 'f1_neg': 0.688, 'eval_score': 0.475}"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'accuracy': 0.562,\n"," 'eval_score': 0.475,\n"," 'f1_neg': 0.688,\n"," 'f1_neu': 0.334,\n"," 'f1_pos': 0.453}"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8acEWEOeMMfy","executionInfo":{"status":"ok","timestamp":1606172243019,"user_tz":360,"elapsed":2609446,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"8c1507da-eea5-4b4a-e1c3-8f46ac4f5f4d"},"source":["def objective_func_CNN_CV(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","\n","    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","    print(par_dict)\n","\n","    accuracy, f1_Positive, f1_Neutral, f1_Negative, eval_score = cross_valid_cnn(train1_pr_df['Doc Text'],train1_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)  \n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","    print('\\n')\n","    print(eval_dict)\n","\n","    return -(eval_score)\n","\n","space = {'max_length': hp.choice('max_length',range(5,50,3)),  \n","        'batch_size': hp.choice('batch_size', [16, 32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30,5)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","        }                  \n","                                    \n","we_dim = 200                               \n","best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=20)\n","print(best_CNN_CV)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'max_length': 5, 'batch_size': 128, 'learning_rate': 0.0003505742782358701, 'epochs': 25}\n","  0%|          | 0/20 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n","  warnings.warn('An interactive session is already active. This can '\n","\n"],"name":"stderr"},{"output_type":"stream","text":["{'accuracy': 0.513, 'f1_pos': 0.355, 'f1_neu': 0.384, 'f1_neg': 0.635, 'eval_score': 0.387}\n","{'max_length': 29, 'batch_size': 32, 'learning_rate': 0.0020142655620911942, 'epochs': 15}\n","{'accuracy': 0.547, 'f1_pos': 0.438, 'f1_neu': 0.369, 'f1_neg': 0.667, 'eval_score': 0.46}\n","{'max_length': 11, 'batch_size': 32, 'learning_rate': 0.0014052014031745786, 'epochs': 15}\n","{'accuracy': 0.502, 'f1_pos': 0.374, 'f1_neu': 0.41, 'f1_neg': 0.609, 'eval_score': 0.4}\n","{'max_length': 35, 'batch_size': 16, 'learning_rate': 0.0024411433717213228, 'epochs': 25}\n","{'accuracy': 0.546, 'f1_pos': 0.417, 'f1_neu': 0.426, 'f1_neg': 0.657, 'eval_score': 0.443}\n","{'max_length': 41, 'batch_size': 128, 'learning_rate': 0.002373136781668488, 'epochs': 25}\n","{'accuracy': 0.531, 'f1_pos': 0.381, 'f1_neu': 0.405, 'f1_neg': 0.642, 'eval_score': 0.411}\n","{'max_length': 47, 'batch_size': 16, 'learning_rate': 0.0034076308465073304, 'epochs': 5}\n","{'accuracy': 0.54, 'f1_pos': 0.448, 'f1_neu': 0.267, 'f1_neg': 0.673, 'eval_score': 0.467}\n","{'max_length': 38, 'batch_size': 16, 'learning_rate': 0.00044999513414845874, 'epochs': 5}\n","{'accuracy': 0.563, 'f1_pos': 0.404, 'f1_neu': 0.382, 'f1_neg': 0.687, 'eval_score': 0.436}\n","{'max_length': 29, 'batch_size': 128, 'learning_rate': 0.007874587054244436, 'epochs': 10}\n","{'accuracy': 0.537, 'f1_pos': 0.41, 'f1_neu': 0.314, 'f1_neg': 0.668, 'eval_score': 0.436}\n","{'max_length': 35, 'batch_size': 64, 'learning_rate': 0.001790133458998955, 'epochs': 10}\n","{'accuracy': 0.539, 'f1_pos': 0.431, 'f1_neu': 0.379, 'f1_neg': 0.661, 'eval_score': 0.453}\n","{'max_length': 29, 'batch_size': 32, 'learning_rate': 0.005271112804657224, 'epochs': 5}\n","{'accuracy': 0.548, 'f1_pos': 0.446, 'f1_neu': 0.368, 'f1_neg': 0.659, 'eval_score': 0.466}\n","{'max_length': 32, 'batch_size': 16, 'learning_rate': 0.007742296380659407, 'epochs': 25}\n","{'accuracy': 0.529, 'f1_pos': 0.335, 'f1_neu': 0.333, 'f1_neg': 0.665, 'eval_score': 0.373}\n","{'max_length': 44, 'batch_size': 32, 'learning_rate': 0.0025635346066440056, 'epochs': 20}\n","{'accuracy': 0.543, 'f1_pos': 0.417, 'f1_neu': 0.382, 'f1_neg': 0.66, 'eval_score': 0.442}\n","{'max_length': 11, 'batch_size': 64, 'learning_rate': 0.0056463982325873205, 'epochs': 20}\n","{'accuracy': 0.514, 'f1_pos': 0.368, 'f1_neu': 0.409, 'f1_neg': 0.626, 'eval_score': 0.397}\n","{'max_length': 23, 'batch_size': 16, 'learning_rate': 0.00786309897213667, 'epochs': 20}\n","{'accuracy': 0.52, 'f1_pos': 0.454, 'f1_neu': 0.367, 'f1_neg': 0.632, 'eval_score': 0.467}\n","{'max_length': 38, 'batch_size': 32, 'learning_rate': 0.007808426851790823, 'epochs': 20}\n","{'accuracy': 0.552, 'f1_pos': 0.352, 'f1_neu': 0.36, 'f1_neg': 0.678, 'eval_score': 0.392}\n","{'max_length': 35, 'batch_size': 16, 'learning_rate': 0.00704557869767637, 'epochs': 20}\n","{'accuracy': 0.535, 'f1_pos': 0.236, 'f1_neu': 0.135, 'f1_neg': 0.703, 'eval_score': 0.296}\n","{'max_length': 29, 'batch_size': 32, 'learning_rate': 0.002167908172271187, 'epochs': 10}\n","{'accuracy': 0.534, 'f1_pos': 0.423, 'f1_neu': 0.424, 'f1_neg': 0.64, 'eval_score': 0.445}\n","{'max_length': 23, 'batch_size': 32, 'learning_rate': 0.008225556371935554, 'epochs': 20}\n","{'accuracy': 0.545, 'f1_pos': 0.446, 'f1_neu': 0.401, 'f1_neg': 0.652, 'eval_score': 0.466}\n","{'max_length': 32, 'batch_size': 32, 'learning_rate': 0.005168838692030022, 'epochs': 5}\n","{'accuracy': 0.542, 'f1_pos': 0.429, 'f1_neu': 0.382, 'f1_neg': 0.66, 'eval_score': 0.452}\n","{'max_length': 11, 'batch_size': 32, 'learning_rate': 0.0066812325543129415, 'epochs': 10}\n","{'accuracy': 0.537, 'f1_pos': 0.423, 'f1_neu': 0.358, 'f1_neg': 0.663, 'eval_score': 0.446}\n","100%|██████████| 20/20 [42:00<00:00, 126.04s/it, best loss: -0.467]\n","{'batch_size': 0, 'epochs': 0, 'learning_rate': 0.0034076308465073304, 'max_length': 14}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-wJ3UZJbbfvB"},"source":["{'max_length': 47, 'batch_size': 16, 'learning_rate': 0.0034076308465073304, 'epochs': 5}\n","{'accuracy': 0.54, 'f1_pos': 0.448, 'f1_neu': 0.267, 'f1_neg': 0.673, 'eval_score': 0.467}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRq_N21KTjFS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mJezvCEVTjIC","colab":{"base_uri":"https://localhost:8080/","height":163},"executionInfo":{"status":"error","timestamp":1606172243446,"user_tz":360,"elapsed":2609865,"user":{"displayName":"Kalyan Kumar Paladugula","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBypR7Wk5RJqa7mv464W6F2Ab5FPdsDmSKVqfY=s64","userId":"13683835722741163781"}},"outputId":"a048212d-4828-4fc0-8347-11362f93e87b"},"source":["print(dasfad)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-36-680d4d84f232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdasfad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'dasfad' is not defined"]}]},{"cell_type":"code","metadata":{"id":"7Cl2Z9PW8CEy"},"source":["def objective_func_CNN(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","\n","    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","    print(par_dict)\n","\n","    tokenise_tf = Tokenizer()\n","    tokenise_tf.fit_on_texts(obama_train_pr_df['Doc Text']) \n","\n","    encoded_train = tokenise_tf.texts_to_sequences(obama_train_pr_df['Doc Text'])\n","    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n","\n","    encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n","    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","    try: \n","        model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","        history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","    except:\n","        try:            \n","            model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","            history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        except:\n","\n","            model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n","            history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","\n","\n","    y_pred_temp = model.predict(validation_padded)\n","    y_pred = pred(y_pred_temp)\n","    f1_temp = np.round(f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None),3)\n","\n","    f1_Positive = f1_temp[2]\n","    f1_Neutral = f1_temp[1]\n","    f1_Negative = f1_temp[0]\n","\n","    accuracy = round(accuracy_score(obama_val_pr_df['Sentiment'], y_pred),3) \n","    eval_score = round(mean([0.4*accuracy, 1.6*f1_temp[0]]),3)  \n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n","    print('\\n')\n","    print(eval_dict)\n","\n","    return -(eval_score)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","        }                  \n","                                    \n","we_dim = 200                               \n","best_CNN = fmin(objective_func_CNN, space, algo=tpe.suggest, max_evals=20)\n","print(best_CNN)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5tL5l1v_w4am"},"source":["{'max_length': 31, 'batch_size': 128, 'learning_rate': 0.00043843544096825674, 'epochs': 7}\n","{'accuracy': 0.547, 'f1_pos': 0.604, 'f1_neu': 0.403, 'f1_neg': 0.595, 'eval_score': 0.582}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mUA2gZj7w2UY"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rDsRALSruVb9"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hfm558S9d2RH"},"source":["# max_length = 30\n","# epochs = 20\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","\n","# accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n","# f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","# print('\\n')\n","# print(eval_dict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0w0bGDBrC9x"},"source":["# def objective_func_CNN_CV(args):\n","#     max_length = args['max_length']\n","#     batch_size = args['batch_size']\n","#     learning_rate = args['learning_rate']\n","#     epochs = args['epochs']\n","\n","#     par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n","#     print(par_dict)\n","\n","#     accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n","#     f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","#     print('\\n')\n","#     print(eval_dict)\n","\n","#     return -(f1)\n","\n","# space = {'max_length': hp.choice('max_length',range(4,60)),  \n","#         'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","#          'epochs': hp.choice('epochs',range(5,30)), \n","#          'learning_rate': hp.uniform('learning_rate', 0,0.01)\n","#         }                  \n","                                    \n","# we_dim = 200                               \n","# best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=20)\n","# print(best_CNN_CV)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5FhdoX1kd2Y4"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldH6dPIM_NiL"},"source":["## Custom F1 value"]},{"cell_type":"code","metadata":{"id":"9Q5Q1HVs_NiM"},"source":["def f1_value(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":null,"outputs":[]}]}