{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Twitter_Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "78PhOL0jpZdY",
        "outputId": "5413773b-ed97-4fd9-cb73-cfe1fa0ac798",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEM1ZS6vpu6b",
        "outputId": "c640479e-c834-41d0-86ae-21a5cac7326d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd /content/drive/My Drive/Research Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Research Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsHo75fupasE"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "# import src\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ePRqI5M3qPi",
        "outputId": "b6811039-ea10-4720-cdfa-792f3ae6ea1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "pip install -U mittens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mittens\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.18.5)\n",
            "Installing collected packages: mittens\n",
            "Successfully installed mittens-0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgktrAVlpZdb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
        "from statistics import mean, stdev, median, mode\n",
        "# With PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# SVM\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "# KNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from mittens import GloVe, Mittens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO6oNPy7px_h",
        "outputId": "466390ee-8cc6-4508-8ef6-99aeaaffc6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "pip install autocorrect"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting autocorrect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n",
            "\u001b[K     |████████████████████████████████| 624kB 3.4MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: autocorrect\n",
            "  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=fe2029f7d2ed3a032b47b95002fff396518e955b30b535b7379d75f9e5a3da53\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n",
            "Successfully built autocorrect\n",
            "Installing collected packages: autocorrect\n",
            "Successfully installed autocorrect-2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXIBsqXjpZdd"
      },
      "source": [
        "\n",
        "import nltk\n",
        "# nltk.download()\n",
        "import string\n",
        "from nltk.tokenize import word_tokenize\n",
        "import random\n",
        "import pickle\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "from autocorrect import Speller\n",
        "# from pycontractions import Contractions\n",
        "\n",
        "# from spellchecker import SpellChecker\n",
        "\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem.wordnet import WordNetLemmatizer \n",
        "\n",
        "from hyperopt import fmin, tpe, hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BALelbZyNU02",
        "outputId": "86dd8973-c4d1-4bc1-b937-42ce7d623fd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lim7c1rpZdf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8gGSNnE_Ngv"
      },
      "source": [
        "import tensorflow_addons as tfa\n",
        "\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
        "import tensorflow.keras.backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUnMZe9zpZdh"
      },
      "source": [
        "# nltk.download('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCe7BteZpZdj"
      },
      "source": [
        "# # Load your favorite word2vec model\n",
        "# cont = Contractions('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
        "# text = \"we're\"\n",
        "# text = list(cont.expand_texts([text], precise=True))[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBMsohUhpZdl"
      },
      "source": [
        "def conv_dataframes(pos_path, neg_path):\n",
        "    with open(pos_path,'r',encoding='latin1') as f:\n",
        "        data_p = f.readlines()\n",
        "#     print(data_p[11])\n",
        "    with open(neg_path,'r',encoding='latin1') as f:\n",
        "        data_n = f.readlines()\n",
        "    pos_data = shuffle(pd.DataFrame(data_p, columns = [\"Doc Text\"]))\n",
        "#     pos_data['Sentiment'] = 1\n",
        "#     pos_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
        "    neg_data = shuffle(pd.DataFrame(data_n, columns = [\"Doc Text\"]))\n",
        "#     neg_data['Sentiment'] = -1\n",
        "#     neg_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
        "    return pos_data, neg_data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hr-TspWpZdn"
      },
      "source": [
        "## The code for pos tagging and lemmatize sentence is fron the following link:\n",
        " ### https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmfT_d2dpZdn"
      },
      "source": [
        "# function to convert nltk tag to wordnet tag\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJso8xCYpZdq"
      },
      "source": [
        "def lemmatize_sentence(sentence):\n",
        "    #tokenize the sentence and find the POS tag for each token\n",
        "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
        "    #tuple of (token, wordnet_tag)\n",
        "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
        "#     print(wordnet_tagged)\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in wordnet_tagged:\n",
        "        if tag is None:\n",
        "            #if there is no available tag, append the token as is\n",
        "            lemmatized_sentence.append(word)\n",
        "        else:        \n",
        "            #else use the tag to lemmatize the token\n",
        "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
        "#         if tag is not None:\n",
        "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
        "\n",
        "    return \" \".join(lemmatized_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eioAEw_bpZdr"
      },
      "source": [
        "# print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
        "# print(lemmatizer.lemmatize(\"loving\")) #loving\n",
        "# print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
        "# print(lemmatize_sentence(\"I am loving it\")) #I be love it"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0yxaZ3DHpZdt"
      },
      "source": [
        "def text_preprocessing(data):\n",
        "    \n",
        "    # This method replaces two or more consecutive letters with the same character to something shorter. For example, gooooooood becomes good.\n",
        "    def replaceTwoOrMore(s):\n",
        "        #look for 2 or more repetitions of character\n",
        "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
        "        return pattern.sub(r\"\\1\\1\", s)\n",
        "\n",
        "    # This method converts camel cased words into space delimited words.\n",
        "    # For example: ThisIsASentence will be changed to This Is A Sentence\n",
        "    def convertCamelCase(word):\n",
        "        return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",word)\n",
        "\n",
        "    # Read a flat file containing some abbreviations and their expansions in pipe separated format\n",
        "    # Use these abbreviations to replace text in the tweets as part of Preprocessing\n",
        "    \n",
        "    def readAbbrFile(abb_path):\n",
        "        global abbr_dict\n",
        "        abbr_dict ={}\n",
        "        f = open(abb_path)\n",
        "        lines = f.readlines()\n",
        "        f.close()\n",
        "        for i in lines:\n",
        "            tmp = i.split('|')\n",
        "            abbr_dict[tmp[0]] = tmp[1]\n",
        "\n",
        "        return abbr_dict\n",
        "  \n",
        "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
        "    # and replaces the key with the corresponding value\n",
        "    def replaceAbbr(s):\n",
        "#         temp =[]\n",
        "#         for word in s.split():\n",
        "#             if word.lower() in abbr_dict.keys():\n",
        "# #                 print('t')                \n",
        "#                 temp.append(abbr_dict[word.lower()])\n",
        "#             else:\n",
        "#                 temp.append(word)\n",
        "        temp = \" \".join([abbr_dict[word.lower()] if word.lower() in abbr_dict.keys() else word for word in s.split()])\n",
        "        return temp\n",
        "    #end\n",
        "\n",
        "    def readcontractions(contra_path):\n",
        "        global contra_dict\n",
        "        contra_dict ={}\n",
        "        f = open(contra_path)\n",
        "        lines = f.readlines()\n",
        "        f.close()\n",
        "        for i in lines:\n",
        "            try: \n",
        "                tmp = i.replace('\"', '').replace(',', '').replace('\\n', ' ').split(':')\n",
        "                contra_dict[tmp[0]] = tmp[1]\n",
        "            except:\n",
        "                print(tmp)\n",
        "                print(z)\n",
        "\n",
        "        return contra_dict\n",
        "    \n",
        "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
        "    # and replaces the key with the corresponding value\n",
        "    def replacecontra(s):\n",
        "        temp = \" \".join([contra_dict[word.lower()] if word.lower() in contra_dict.keys() else word for word in s.split()])\n",
        "        return temp\n",
        "    #end    \n",
        "    \n",
        "\n",
        "    abb_path = os.path.join(data_path,\"abbrevations.txt\")\n",
        "    abbr_dict = readAbbrFile(abb_path)\n",
        "    \n",
        "    contra_path = os.path.join(data_path,\"contractions.txt\")\n",
        "    contra_dict = readcontractions(contra_path)\n",
        "    \n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: x.lower())\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceAbbr(x))\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replacecontra(x))\n",
        "\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('@\\w+', ' ') #remove usernames\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[<>!#@$:.,%\\?-]+', ' ') #remove punctuation and special characters\n",
        "    \n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n\\n', ' ')\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n', ' ') # removing new line characters\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[^\\w\\s]',' ')\n",
        "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+[a-zA-Z]\\s+',' ')\n",
        "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\^[a-zA-Z]\\s+',' ')\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+',' ')  \n",
        "\n",
        "    \n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceTwoOrMore(x))\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: convertCamelCase(x))\n",
        "\n",
        "    # Remove stop words from text\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_list]))\n",
        "    \n",
        "#     data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(y) for y in x.split()]))\n",
        "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: lemmatize_sentence(x))\n",
        "\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7duAi2av_Ng-",
        "outputId": "5fca638e-81fe-4951-cfed-4204a00ea6e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "st = 'k k  k'\n",
        "st.replace('\\s+','')\n",
        "st.replace('\\n\\n', '')\n",
        "st.replace('[^\\w\\s]','')\n",
        "st.replace('\\s+[a-zA-Z]\\s+','')\n",
        "st.replace('\\^[a-zA-Z]\\s+','')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'k k  k'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DgyAcMZRpZdv"
      },
      "source": [
        "def create_vocab(data1, data2):\n",
        "    temp1 = text_preprocessing(data1)\n",
        "    temp2 = text_preprocessing(data2)\n",
        "#     temp_pos2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_pos['Doc Text']])\n",
        "#     temp_neg2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_neg['Doc Text']]) \n",
        "#     temp_pos[\"Doc Text Tokens\"] = temp_pos2 \n",
        "#     temp_neg[\"Doc Text Tokens\"] = temp_neg2\n",
        "    return temp1, temp2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcIgGnBHpZd1",
        "outputId": "8619d070-433c-4f6f-cf8b-7a42c1c836cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'\n",
        "trainFile = os.path.join(data_path,\"training-Obama-Romney-tweets.xlsx\")\n",
        "obama_train_temp = pd.read_excel(trainFile, sheet_name = 'Obama', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
        "romney_train_temp = pd.read_excel(trainFile, sheet_name = 'Romney', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-255fb4ac6532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training-Obama-Romney-tweets.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobama_train_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Obama'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mromney_train_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Romney'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kalya\\\\OneDrive - University of Illinois at Chicago\\\\!UIC\\\\!Semesters\\\\3rd Sem\\\\CS 583 Data Mining and Text Mining\\\\Research Project\\\\Data/training-Obama-Romney-tweets.xlsx'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aoo2NCbGpZd3",
        "outputId": "ff9514b3-a449-4c9e-a214-d3e144fe9ed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "obama_train_temp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
              "1  Question: If <e>Romney</e> and <e>Obama</e> ha...         2\n",
              "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
              "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...         2\n",
              "4  @Hollivan @hereistheanswer  Youre missing the ...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ESw9o5h7_NhH",
        "outputId": "e7e18e93-a3d4-41a0-e0ea-63aa4cfb1ce2"
      },
      "source": [
        "romney_train_temp.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
              "1  Senior <e>Romney</e> Advisor Claims <e>Obama</...         2\n",
              "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
              "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
              "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3fSFxgW_NhI"
      },
      "source": [
        "## Removing datapoints with mixed sentiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-c-9PNk_NhJ",
        "outputId": "e41582e4-b4bd-46ca-f0b3-14240470614c"
      },
      "source": [
        "obama_train = obama_train_temp[obama_train_temp['Sentiment'] .isin((1,-1,0))]\n",
        "obama_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>I was raised as a Democrat  left the party yea...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
              "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
              "4  @Hollivan @hereistheanswer  Youre missing the ...         0\n",
              "6  I was raised as a Democrat  left the party yea...        -1\n",
              "7  The <e>Obama camp</e> can't afford to lower ex...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-cpEizR_NhL",
        "outputId": "4d0a45c9-2f1f-416b-9ed6-004dc1fb8b51"
      },
      "source": [
        "romney_train = romney_train_temp[romney_train_temp['Sentiment'] .isin((1,-1,0))]\n",
        "romney_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
              "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
              "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
              "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1\n",
              "5  Hope <e>Romney</e> debate prepped w/ the same ...         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ0Xu4VZ_NhN"
      },
      "source": [
        "# Dropping missing values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6WQ-4zw_NhO"
      },
      "source": [
        "obama_train = obama_train.dropna()\n",
        "romney_train = romney_train.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epYfA3Vg_NhQ"
      },
      "source": [
        "## Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gW_pnLsm_NhQ",
        "outputId": "8c8f3c85-5317-474d-a0a8-a82f5c280779"
      },
      "source": [
        "obama_train_DA = obama_train['Sentiment'].value_counts()\n",
        "obama_train_DA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    1922\n",
              " 0    1895\n",
              " 1    1653\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RE1FsDeu_NhT",
        "outputId": "273ddcd9-24eb-4537-9c26-4b2c4fab26ef"
      },
      "source": [
        "plt.bar(height = obama_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 3 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1_gyDU9_NhV",
        "outputId": "4ca273ed-1f9e-4d6e-c9fb-a55f743de3fb"
      },
      "source": [
        "romney_train_DA = romney_train['Sentiment'].value_counts()\n",
        "romney_train_DA"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1    2893\n",
              " 0    1680\n",
              " 1    1075\n",
              "Name: Sentiment, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alvh5jk_NhW",
        "outputId": "aeb017c0-982e-4669-aab5-f8781e9a77ee"
      },
      "source": [
        "plt.bar(height = romney_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 3 artists>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LagE7x_6_NhY"
      },
      "source": [
        "# Romney data is very imbalanced\n",
        "# So, its better to select the best model with respect to F1 Score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05yRKXM4_NhY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2j9EdPb9_Nha"
      },
      "source": [
        "## Preprocessing the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qsh4tjZk_Nhb"
      },
      "source": [
        "# hm_lines = 5331\n",
        "\n",
        "# tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
        "# spell = Speller(lang='en')\n",
        "# # spell = SpellChecker()\n",
        "\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "stop_list = stopwords.words('english')\n",
        "stop_list.extend(['rt', 'retweet', 'e'])\n",
        "\n",
        "lemmatizer = WordNetLemmatizer() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZD3LoJ1_Nhe"
      },
      "source": [
        "obama_train_pr, romney_train_pr = create_vocab(obama_train, romney_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dt3d1UQV_Nhg",
        "outputId": "7e1f41e2-e776-4a8b-a822-74d9076321af"
      },
      "source": [
        "obama_train_pr.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>miss point afraid understand big picture dont ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>raise democrat leave party year ago lifetime n...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>obama camp can not afford low expectation toni...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  kirkpatrick wear baseball cap embroider obama ...         0\n",
              "2  obama debate cracker as cracker tonight tune t...         1\n",
              "4  miss point afraid understand big picture dont ...         0\n",
              "6  raise democrat leave party year ago lifetime n...        -1\n",
              "7  obama camp can not afford low expectation toni...         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dIaA7p2_Nhh"
      },
      "source": [
        "obama_train_pr['Sentiment'] = obama_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvFTC2P0_Nhj",
        "outputId": "db43eeb5-5682-47b0-b9d7-08261a27422a"
      },
      "source": [
        "obama_train_pr.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>miss point afraid understand big picture dont ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>raise democrat leave party year ago lifetime n...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>obama camp can not afford low expectation toni...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
              "2  obama debate cracker as cracker tonight tune t...  Positive\n",
              "4  miss point afraid understand big picture dont ...   Neutral\n",
              "6  raise democrat leave party year ago lifetime n...  Negative\n",
              "7  obama camp can not afford low expectation toni...   Neutral"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngjoZsf1_Nhk",
        "outputId": "f4faedde-ebea-4669-c4ae-8b3167a116da"
      },
      "source": [
        "romney_train_pr.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>insidious mitt romney bain help philip morris ...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mean like romney cheat primary</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mitt romney still believe black president</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hope romney debate prepped people last time</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  insidious mitt romney bain help philip morris ...        -1\n",
              "2                     mean like romney cheat primary        -1\n",
              "3          mitt romney still believe black president        -1\n",
              "4  romney tax plan deserve nd look secret one dif...        -1\n",
              "5        hope romney debate prepped people last time         1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BX_1EBR3_Nhm"
      },
      "source": [
        "romney_train_pr['Sentiment'] = romney_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT1k9oGj_Nho",
        "outputId": "432925e6-6ad6-4ed8-ac06-4e63bf4f003d"
      },
      "source": [
        "romney_train_pr.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>insidious mitt romney bain help philip morris ...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>mean like romney cheat primary</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mitt romney still believe black president</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>hope romney debate prepped people last time</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  insidious mitt romney bain help philip morris ...  Negative\n",
              "2                     mean like romney cheat primary  Negative\n",
              "3          mitt romney still believe black president  Negative\n",
              "4  romney tax plan deserve nd look secret one dif...  Negative\n",
              "5        hope romney debate prepped people last time  Positive"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBTlpT5I_Nhp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nyt2ltYpZd5"
      },
      "source": [
        "\n",
        "## Building Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dugwt7lTaCiA"
      },
      "source": [
        "# obama_train_pr.to_csv(os.path.join(data_path, 'Obama Training1 Data.csv'))\n",
        "# romney_train_pr.to_csv(os.path.join(data_path, 'Romney Training1 Data.csv'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AD9U00CVddNC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "96eAfNsDaiVd"
      },
      "source": [
        "data_path = r'/content/drive/My Drive/Research Project/Data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kljiyU8TaCiF"
      },
      "source": [
        "obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2])\n",
        "romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcLnGHdPaCiI",
        "outputId": "445a12c2-1b51-441f-c617-bf1b07a69e9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "obama_train_pr_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Doc Text</th>\n",
              "      <th>Sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
              "      <td>Positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>miss point afraid understand big picture dont ...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>raise democrat leave party year ago lifetime n...</td>\n",
              "      <td>Negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>obama camp can not afford low expectation toni...</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            Doc Text Sentiment\n",
              "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
              "1  obama debate cracker as cracker tonight tune t...  Positive\n",
              "2  miss point afraid understand big picture dont ...   Neutral\n",
              "3  raise democrat leave party year ago lifetime n...  Negative\n",
              "4  obama camp can not afford low expectation toni...   Neutral"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XORvLV2iaCiM"
      },
      "source": [
        "obama_train_pr_df = obama_train_pr_df.dropna()\n",
        "romney_train_pr_df = romney_train_pr_df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXJwrtOtaCiQ"
      },
      "source": [
        "def tf_idf(x):\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = vectorizer.fit_transform(x)\n",
        "    #print(tfidf_matrix)\n",
        "    x1 = tfidf_matrix.toarray()\n",
        "#     print(x1)\n",
        "    return x1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM-Q2KTBaCiU"
      },
      "source": [
        "obama_train_pr_tfidf = tf_idf(obama_train_pr_df['Doc Text'])\n",
        "romney_train_pr_tfidf = tf_idf(romney_train_pr_df['Doc Text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uqUrTlhaCiX",
        "outputId": "42f9cd8a-4faa-4cae-b3f9-0dd4e32502d4"
      },
      "source": [
        "# With PCA, Bayesian Optimization, and SVM\n",
        "\n",
        "def objective_func(args):\n",
        "    C = args['C']\n",
        "    pca = PCA(n_components=args['n'])\n",
        "    kernel = args['kernel']\n",
        "    gamma = args['gamma']\n",
        "    degree = args['degree']\n",
        "    x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
        "\n",
        " \n",
        "    clf = SVC(C= C, kernel = kernel, gamma=gamma, degree=degree)\n",
        "    temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
        "#     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
        "#     f1 = mean(error) + stdev(error) \n",
        "    #print(f1)\n",
        "    temp_f1 = cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro')\n",
        "    f1 = mean(temp_f1)\n",
        "    return -(f1)\n",
        "space = {'C': hp.uniform('C', 0.1,20),\n",
        "         'n': hp.choice('n', np.arange(3,10, step =1)),\n",
        "        'kernel': hp.choice('kernel', ['poly', 'rbf', 'sigmoid']), \n",
        "        'gamma': hp.choice('gamma',range(1,4)),\n",
        "         'degree' : hp.choice('degree',range(1,4))}\n",
        "                                \n",
        "                                \n",
        "                                \n",
        "best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
        "print(best_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:15<00:00,  3.76s/trial, best loss: -0.47506789074000444]\n",
            "{'C': 10.988358566458993, 'degree': 0, 'gamma': 2, 'kernel': 1, 'n': 6}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05g1elYAaCib",
        "outputId": "c73d2b74-8451-41e8-fce8-da119f7d4493"
      },
      "source": [
        "# With PCA, Bayesian Optimization, and SVM\n",
        "\n",
        "def objective_func(args):\n",
        "    C = args['C']\n",
        "\n",
        "    kernel = args['kernel']\n",
        "    gamma = args['gamma']\n",
        "    degree = args['degree']\n",
        "    \n",
        "#     pca = PCA(n_components=args['n'])\n",
        "#     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
        "\n",
        " \n",
        "    clf = SVC(C= C, kernel = kernel, gamma=gamma, degree=degree)\n",
        "#     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
        "#     f1 = mean(error) + stdev(error) \n",
        "    #print(f1)\n",
        "    temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
        "    temp_f1 = cross_val_score(clf, obama_train_pr_tfidf, temp, cv = 5, scoring = 'f1_macro')\n",
        "    f1 = mean(temp_f1)\n",
        "    return -(f1)\n",
        "space = {'C': hp.uniform('C', 0.1,20),\n",
        "#          'n': hp.choice('n', np.arange(3,10, step =1)),\n",
        "        'kernel': hp.choice('kernel', ['poly', 'rbf', 'sigmoid']), \n",
        "        'gamma': hp.choice('gamma',range(1,4)),\n",
        "         'degree' : hp.choice('degree',range(1,4))}\n",
        "                                \n",
        "                                \n",
        "                                \n",
        "best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
        "print(best_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnu2mBLwaCie"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-6Q8dRwaCig",
        "outputId": "b42504a9-9d98-49fd-e286-297bb9fcd020"
      },
      "source": [
        "# With Bayesian Optimization, and Random Forest\n",
        "\n",
        "def objective_func(args):\n",
        "    n_estimators = args['n_estimators']\n",
        "    max_depth = args['max_depth']\n",
        "    min_samples_split = args['min_samples_split']\n",
        "    min_samples_leaf = args['min_samples_leaf']\n",
        "#     pca = PCA(n_components=args['n'])\n",
        "#     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
        "\n",
        " \n",
        "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
        "    \n",
        "    temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
        "    temp_f1 = cross_val_score(clf, obama_train_pr_tfidf, temp, cv = 5, scoring = 'f1_macro')\n",
        "    f1 = mean(temp_f1)\n",
        "    return -(f1)\n",
        "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
        "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
        "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
        "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
        "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
        "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
        "#         'n': hp.choice('n', np.arange(3,20, step =1))\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "                                \n",
        "best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
        "print(best_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 30/30 [09:58<00:00, 19.95s/trial, best loss: -0.552838844792619] \n",
            "{'max_depth': 44, 'min_samples_leaf': 0, 'min_samples_split': 9, 'n_estimators': 48}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yPNE0fHcaCil",
        "outputId": "c7e0ce82-fe4e-424f-9700-0ddc4b76dea3"
      },
      "source": [
        "pca2 = PCA(n_components=3)\n",
        "x_pca2 = pca2.fit_transform(obama_train_pr_tfidf)\n",
        "gb = GaussianNB()\n",
        "print(cross_val_score(gb, obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], cv = 5, scoring = 'f1_macro').mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.44473092670012715\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4An0oQ0XaCio"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHIHUE1laCiv",
        "outputId": "17b057c5-e21f-4374-b803-8203df34db0d"
      },
      "source": [
        "# WIth PCA , Bayesian Optimization and KNN\n",
        "def objective_func(args):\n",
        "    n_neighbors = args['n_neighbors']\n",
        "    metric = args['metric']\n",
        "    pca = PCA(n_components=args['n'])\n",
        "    x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
        "\n",
        "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
        "    temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
        "    temp_f1 = cross_val_score(clf, obama_train_pr_tfidf, temp, cv = 5, scoring = 'f1_macro')\n",
        "    f1 = mean(temp_f1)\n",
        "    return -(f1)\n",
        "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,10, step =1)),\n",
        "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"]),\n",
        "        'n': hp.choice('n', np.arange(3,10, step =1))}\n",
        "\n",
        "best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
        "print(best_classifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 20%|██        | 6/30 [16:41<1:06:47, 166.99s/trial, best loss: -0.47666691430168934]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-30-d60b61969293>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m         'n': hp.choice('n', np.arange(3,10, step =1))}\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mbest_classifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_classifier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-30-d60b61969293>\u001b[0m in \u001b[0;36mobjective_func\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNeighborsClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_neighbors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobama_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Positive'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Negative'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mtemp_f1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobama_train_pr_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f1_macro'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[0mf1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemp_f1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    388\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    391\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 236\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1005\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1006\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1007\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1008\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    833\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    834\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 835\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    752\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 754\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    755\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 209\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    210\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    588\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 256\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    542\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 544\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    545\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    546\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer)\u001b[0m\n\u001b[0;32m    589\u001b[0m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m         \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m     error_msg = (\"scoring must return a number, got %s (%s) \"\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m     85\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_BaseScorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                 score = scorer._score(cached_call, estimator,\n\u001b[1;32m---> 87\u001b[1;33m                                       *args, **kwargs)\n\u001b[0m\u001b[0;32m     88\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m                 \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \"\"\"\n\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 205\u001b[1;33m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmethod_caller\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"predict\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\u001b[0m in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[1;34m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    661\u001b[0m                 delayed_query(\n\u001b[0;32m    662\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n\u001b[1;32m--> 663\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgen_even_slices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    664\u001b[0m             )\n\u001b[0;32m    665\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1015\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1017\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1018\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    908\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 909\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    910\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    561\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 562\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    563\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    564\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tt3WGISKa9_U"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBZvoZJna-OH"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv3D\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import PReLU\n",
        "from tensorflow.keras.layers import MaxPool3D, AveragePooling3D\n",
        "\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import AveragePooling3D\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpvgL7SUc9gr",
        "outputId": "c121405c-6f7d-4f05-d9fa-92e639ccdc43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "max_length = 40\n",
        "epochs = 2\n",
        "batch_size = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(obama_train_pr_df['Doc Text'])   \n",
        "encoded_train = tokenise_tf.texts_to_sequences(obama_train_pr_df['Doc Text'])\n",
        "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
        "validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-15c44c5430ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtokenise_tf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtokenise_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobama_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mencoded_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenise_tf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobama_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtraining_padded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'post'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pre'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'obama_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Pw9UHJjceM-"
      },
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras import layers\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FiqzWfYwa-ir",
        "outputId": "87113577-d80e-4414-c0ce-513c2e8c2b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "\n",
        "\n",
        "int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "embedded_sequences = embedding_layer(int_sequences_input)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(5)(x)\n",
        "x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
        "x = layers.GlobalMaxPooling1D()(x)\n",
        "x = layers.Dense(128, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
        "model = keras.Model(int_sequences_input, preds)\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-afbc5d216656>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mint_sequences_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0membedded_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_sequences_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_sequences\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'embedding_layer' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yGej2wra-vR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-qVvdKva-ru"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z2zMn62a-K1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj3NyfQ2a-Gk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qour-cwraCix"
      },
      "source": [
        "max_length = 40\n",
        "epochs = 2\n",
        "batch_size = 200\n",
        "learning_rate = 0.001\n",
        "\n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(obama_train['Doc Text'])   \n",
        "encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
        "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
        "validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True)\n",
        "y_pred_temp = model.predict(validation_padded)\n",
        "y_pred = pred(y_pred_temp)\n",
        "f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n",
        "print(f1)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4qdpS83daOj"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nt4Q-LLu_Nh6"
      },
      "source": [
        "def one_hot(data):\n",
        "    data = np.asarray(data)\n",
        "    temp = np.zeros((len(data),3))\n",
        "#     print(data[0])\n",
        "    for i in range(len(temp)):\n",
        "        if data[i] == 'Negative':\n",
        "            temp[i][2] = 1 ## Negative sentiment third neuron\n",
        "        elif data[i] == 'Neutral':\n",
        "            temp[i][1] = 1 ## Neutral sentiment second neuron  \n",
        "        else:\n",
        "            temp[i][0] = 1 ## Positive sentiment first neuron \n",
        "\n",
        "    return temp\n",
        "    \n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETvx1WoKfrfk"
      },
      "source": [
        "def pred(x):\n",
        "    temp = []\n",
        "    for i in x:\n",
        "        m = np.argmax(i)\n",
        "        if m == 0:\n",
        "            temp.append('Positive')\n",
        "        elif m == 1:\n",
        "            temp.append('Neutral')\n",
        "        else:\n",
        "            temp.append('Negative')\n",
        "    return temp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmGshHslpZeH"
      },
      "source": [
        "## Building Glove Dictionary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU-SxaOZ_Nh9"
      },
      "source": [
        "embeddings = {}\n",
        "with open(os.path.join(data_path,\"glove.6B.300d.txt\"), 'r', encoding=\"utf-8\") as file:\n",
        "    for line in file:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        vector = asarray(values[1:], dtype='float32')\n",
        "        embeddings[word] = vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "or3hmQvdpZeJ"
      },
      "source": [
        "## Embedding Matrix Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBG3Kq7EpZeJ"
      },
      "source": [
        "def emb_matrix(t,embeddings):\n",
        "    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n",
        "    vocab_size = len(t.word_index) + 1    \n",
        "    embedding_matrix = zeros((vocab_size, 300))\n",
        "    for word, i in t.word_index.items():\n",
        "        embedding_vector = embeddings.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector\n",
        "    return embedding_matrix, vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kT9ZAXi_NiC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgK6d3Qy_NiF"
      },
      "source": [
        "## Fine tuning the word embeddings of 300 dimensions using mittens library\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqwJmPT-_NiF"
      },
      "source": [
        "## Used the code for finetuning from the following link:\n",
        "### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HRwhIKRT_NiF"
      },
      "source": [
        "def finetune(training): \n",
        "    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
        "    #training_tokens\n",
        "\n",
        "    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
        "    print(len(oov))\n",
        "\n",
        "    corp_vocab = list(set(oov))\n",
        "\n",
        "    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "    trr =''\n",
        "    for i in training_tokens:\n",
        "        for j in i:\n",
        "            trr+= j\n",
        "            trr += ' '\n",
        "\n",
        "    # print(trr)\n",
        "    # print(z)\n",
        "    X = cv.fit_transform([trr])\n",
        "    Xc = (X.T * X)\n",
        "    Xc.setdiag(0)\n",
        "    coocc_ar = Xc.toarray()\n",
        "\n",
        "    mittens_model = Mittens(n=300, max_iter=1000)\n",
        "\n",
        "    new_embeddings = mittens_model.fit(\n",
        "      coocc_ar,\n",
        "      vocab=corp_vocab,\n",
        "      initial_embedding_dict= embeddings)\n",
        "\n",
        "    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
        "    return training_tokens, new_embeddings\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MB2Nd-Kr_NiH",
        "outputId": "bbfe8258-07f7-4ad0-8235-5fed0541af35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "embeddings2= embeddings.copy()\n",
        "\n",
        "training_tokens, new_embeddings = finetune(obama_train_pr_df)\n",
        "embeddings2.update(new_embeddings)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1978\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "Iteration 1000: loss: 1.488498568534851"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spGvX_Cg_NiI",
        "outputId": "f3e4d3b5-5bf1-4481-c379-ab809dcb096f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
        "print(len(oov2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir2jPLOS_NiK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V3VcHx_CduFW"
      },
      "source": [
        "## Using CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9xXo3kBdnDc"
      },
      "source": [
        "def cross_valid_cnn(X,y,epochs,batch_size,max_length, learning_rate):\n",
        "    f1  =[]\n",
        "    accuracy =[]\n",
        "    cv = KFold(n_splits=5,shuffle=True)\n",
        "    for train_index, val_index in cv.split(X):\n",
        "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
        "    #     print(\"Test Index: \", test_index)\n",
        "    #     print(X.iloc[train_index])\n",
        "    #     print(f)\n",
        "\n",
        "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
        "        tokenise_tf = Tokenizer()\n",
        "        tokenise_tf.fit_on_texts(X_train1) \n",
        "    \n",
        "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
        "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
        "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "        embedding_layer = Embedding(vocab_size, 300, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
        "        int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
        "        embedded_sequences = embedding_layer(int_sequences_input)\n",
        "        x = layers.Conv1D(128, 3)(embedded_sequences)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "        x = layers.Conv1D(128, 3)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = layers.MaxPooling1D(2)(x)\n",
        "\n",
        "        x = layers.Conv1D(128, 3)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = LeakyReLU()(x)\n",
        "        x = layers.GlobalMaxPooling1D()(x)\n",
        "        # x = layers.MaxPooling1D(2)(x)\n",
        "\n",
        "        x = layers.Flatten()(x)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        x = layers.Dense(128, activation=\"relu\")(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        preds = layers.Dense(3, activation=\"softmax\")(x)\n",
        "\n",
        "        model = tf.keras.Model(int_sequences_input, preds)\n",
        "        # print(model.summary())\n",
        "        # print(z)\n",
        "        model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
        "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
        "        y_pred_temp = model.predict(validation_padded)\n",
        "        y_pred = pred(y_pred_temp)\n",
        "        f1.append(f1_score(y_val, y_pred, average = 'macro'))\n",
        "        accuracy.append(accuracy_score(y_val, y_pred))\n",
        "\n",
        "#         print(temp(y_pred, y_val).head())\n",
        "    return mean(f1), mean(accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hfm558S9d2RH",
        "outputId": "d472d123-6cb7-4a4c-b3d7-ddd0a53c495c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "max_length = 30\n",
        "epochs = 25\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "f1, accuracy= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate)\n",
        "print(f1)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5296465581470925\n",
            "0.53345521023766\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0w0bGDBrC9x",
        "outputId": "f46a7cfa-353f-4923-cdc6-e208ea85d963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    \n",
        "    epochs = args['epochs']\n",
        "\n",
        "    f1, accuracy= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate)\n",
        "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
        "#     f1 = history.history['f1_value'][-1]\n",
        "   \n",
        "    return -round(f1,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,30)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(10,30)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "                               \n",
        "best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
        "print(best_vanilla_rnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [00:43<?, ?it/s, best loss: ?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-91aa98a69114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mbest_vanilla_rnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_vanilla_rnn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[1;32m    405\u001b[0m                     show_progressbar=show_progressbar)\n\u001b[1;32m    406\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    225\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                         \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m    139\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    842\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 844\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-58-91aa98a69114>\u001b[0m in \u001b[0;36mobjective_func\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mcross_valid_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;31m# loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     f1 = history.history['f1_value'][-1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-55-aa03df5be433>\u001b[0m in \u001b[0;36mcross_valid_cnn\u001b[0;34m(X, y, epochs, batch_size, max_length, learning_rate)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m# print(z)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0my_pred_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred_temp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 809\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m    667\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    268\u001b[0m       \u001b[0;31m# Collecting and resetting metrics has non-zero cost and will needlessly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m       \u001b[0;31m# slow down model.predict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_v1.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    996\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_training_eval_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 998\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1000\u001b[0m     \u001b[0;31m# Reset metrics on all the distributed (cloned) models.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m     \"\"\"\n\u001b[0;32m--> 247\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3599\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3600\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3601\u001b[0;31m         \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m(op_input_list)\u001b[0m\n\u001b[1;32m    628\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m   1063\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    956\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 958\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    959\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1181\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1182\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FhdoX1kd2Y4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldH6dPIM_NiL"
      },
      "source": [
        "## Custom F1 value"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Q5Q1HVs_NiM"
      },
      "source": [
        "def f1_value(y_true, y_pred): #taken from old keras source code\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
        "    return f1_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VHQfx6SpZeN"
      },
      "source": [
        "## Building Vanilla RNN, LSTM, and GRU models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sNbzJtMWpZeN"
      },
      "source": [
        "def model_vanilla_rnn(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(Bidirectional(SimpleRNN(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
        "    model.add(Dropout(Dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
        "#     print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D69lMKSLpZeP"
      },
      "source": [
        "def model_lstm(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(Bidirectional(LSTM(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
        "    model.add(Dropout(Dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
        "#     print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDL6HFQYpZeQ"
      },
      "source": [
        "def model_gru(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(Bidirectional(GRU(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
        "    model.add(Dropout(Dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
        "#     print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42FDB3zmpZeS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u5oc67yzQ2-Z"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dw6cg12SpZeU"
      },
      "source": [
        "# u = {}\n",
        "# u['max_len'] = max_length\n",
        "# u['batch'] = batch_size\n",
        "# u['l_rate'] = learning_rate\n",
        "# u['epochs'] = epochs\n",
        "\n",
        "# fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "# ax[0].plot(history.history['loss'], label='Training')\n",
        "# ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "# ax[0].legend(loc='upper left')\n",
        "# ax[0].title.set_text('Loss plot for the combination ' + str(u)) \n",
        "\n",
        "# ax[1].plot(history.history['accuracy'], label='Training')\n",
        "# ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "# ax[1].legend(loc='upper left')\n",
        "# ax[1].title.set_text('Accuracy plot for the combination ' + str(u))    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_K4Y7dUt_NiX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CxFvBFuD_NiZ"
      },
      "source": [
        "# class Metrics(Callback):\n",
        "#     def __init__(self, validation):   \n",
        "#         super(Metrics, self).__init__()\n",
        "#         self.validation = validation    \n",
        "            \n",
        "#         print('validation shape', len(self.validation[0]))\n",
        "        \n",
        "#     def on_train_begin(self, logs={}):        \n",
        "#         self.val_f1s = []\n",
        "#         self.val_recalls = []\n",
        "#         self.val_precisions = []\n",
        "     \n",
        "#     def on_epoch_end(self, epoch, logs={}):\n",
        "#         val_targ = self.validation[1]   \n",
        "#         val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n",
        "    \n",
        "#         val_f1 = f1_score(val_targ, val_predict)\n",
        "#         val_recall = recall_score(val_targ, val_predict)         \n",
        "#         val_precision = precision_score(val_targ, val_predict)\n",
        "        \n",
        "#         self.val_f1s.append(round(val_f1, 6))\n",
        "#         self.val_recalls.append(round(val_recall, 6))\n",
        "#         self.val_precisions.append(round(val_precision, 6))\n",
        " \n",
        "#         print(f' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNl5R_SO_Nia"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnDrYVDC_Nib"
      },
      "source": [
        "## Building models for obama tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLDi0elvjHkC"
      },
      "source": [
        "## Training the Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBp3ntnMpZeX"
      },
      "source": [
        "## Using Hyperopt library to tune the Hyperparameters. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTx5aKtXaCkP"
      },
      "source": [
        "def cross_valid(X,y,epochs,batch_size,max_length, learning_rate):\n",
        "    f1  =[]\n",
        "    cv = KFold(n_splits=5,shuffle=True)\n",
        "    for train_index, val_index in cv.split(X):\n",
        "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
        "    #     print(\"Test Index: \", test_index)\n",
        "    #     print(X.iloc[train_index])\n",
        "    #     print(f)\n",
        "\n",
        "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
        "        tokenise_tf = Tokenizer()\n",
        "        tokenise_tf.fit_on_texts(X_train1) \n",
        "    \n",
        "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
        "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
        "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "        model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)        \n",
        "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
        "        y_pred_temp = model.predict(validation_padded)\n",
        "        y_pred = pred(y_pred_temp)\n",
        "        f1.append(f1_score(y_val, y_pred, average = 'macro'))\n",
        "#         print(temp(y_pred, y_val).head())\n",
        "    return mean(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r6qsr1PKaCkS"
      },
      "source": [
        "max_length = 8\n",
        "epochs = 20\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "f1= cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate)\n",
        "f1\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6PBOzWHwaCkU",
        "outputId": "c8feb4ef-a425-4940-cf1a-256ed7de101d"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    f1= cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate)\n",
        "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
        "#     f1 = history.history['f1_value'][-1]\n",
        "   \n",
        "    return -round(f1,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,20)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(10,30)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "                               \n",
        "best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
        "print(best_vanilla_rnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/20 [06:17<?, ?trial/s, best loss=?]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-83-ad0e97606cef>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mbest_vanilla_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_vanilla_rnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-83-ad0e97606cef>\u001b[0m in \u001b[0;36mobjective_func\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mf1\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mcross_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Doc Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m     \u001b[1;31m# loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#     f1 = history.history['f1_value'][-1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-82-00df857e6629>\u001b[0m in \u001b[0;36mcross_valid\u001b[1;34m(X, y, epochs, batch_size, max_length, learning_rate)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0madam_optimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_vanilla_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m         \u001b[0my_pred_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m   def evaluate(self,\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3825\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n",
            "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhZTgQGmaCkW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am8LqqY96ghg",
        "outputId": "dc3efc76-9e4e-40fa-dc3e-de60f686d3fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
        "    y_pred_temp = model.predict(validation_padded)\n",
        "    y_pred = pred(y_pred_temp)\n",
        "    f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n",
        "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
        "#     f1 = history.history['f1_value'][-1]\n",
        "   \n",
        "    return -round(f1,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,20)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(10,30)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(obama_train['Doc Text'])                                \n",
        "best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
        "print(best_vanilla_rnn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [29:00<00:00, 87.03s/it, best loss: -0.55] \n",
            "{'batch_size': 1, 'epochs': 1, 'learning_rate': 0.0018559993912219612, 'max_length': 3}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfD4GJ3tNQbo",
        "outputId": "676ee689-d0f4-4169-a819-990b7d3e9d5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "max_length = 5\n",
        "epochs = 5\n",
        "batch_size = 64\n",
        "learning_rate = 0.0007711038235329859\n",
        "\n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(obama_train['Doc Text'])   \n",
        "encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
        "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
        "validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True)\n",
        "y_pred_temp = model.predict(validation_padded)\n",
        "y_pred = pred(y_pred_temp)\n",
        "acc= accuracy_score(obama_val['Sentiment'], y_pred)\n",
        "print(acc)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 4376 samples\n",
            "Epoch 1/5\n",
            "4376/4376 [==============================] - 2s 431us/sample - loss: 8.9841\n",
            "Epoch 2/5\n",
            "4376/4376 [==============================] - 2s 418us/sample - loss: 6.2251\n",
            "Epoch 3/5\n",
            "4376/4376 [==============================] - 2s 417us/sample - loss: 4.1734\n",
            "Epoch 4/5\n",
            "4376/4376 [==============================] - 2s 424us/sample - loss: 2.7909\n",
            "Epoch 5/5\n",
            "4376/4376 [==============================] - 2s 428us/sample - loss: 2.0136\n",
            "0.49177330895795246\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpDDkemAhldQ"
      },
      "source": [
        "# For vanilla rnn, the best hyper parameters are:\n",
        "# 'batch_size': 128, 'epochs': 6, 'learning_rate': 0.0003, 'max_length': 68"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ougNDoQpZea"
      },
      "source": [
        "# {'batch_size': 0, 'epochs': 7, 'max_length': 59, 'padd': 0, 'trunc': 0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JhmM1-BpZed"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN74XEQLpZef"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_lstm(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = round(history.history['val_accuracy'][-1],2)\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "    return -round(accuracy,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }             \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_lstm = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_lstm)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6vO3cE8h-Np"
      },
      "source": [
        "# For LSTM rnn, the best hyper parameters are:\n",
        "# 'batch_size': 128, 'epochs': 10, 'learning_rate': 0.002, 'max_length': 50"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf5oOkX5h_15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lgt4uCrYpZeh"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_gru(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round((accuracy),2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_gru = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_gru)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "illGwOwGiO_y"
      },
      "source": [
        "# For GRU rnn, the best hyper parameters are:\n",
        "# 'batch_size': 64, 'epochs': 7, 'learning_rate': 0.0006, 'max_length': 11"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTooPqJ3pZej"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SmX0YNPUQDjL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FagU-LZrpZeu"
      },
      "source": [
        "# max_length = 40\n",
        "# epochs = 15\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "# encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "# model = model_vanilla_rnn(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "# history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "# accuracy = history.history['val_accuracy'][-1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vbdqiyy_ibn4"
      },
      "source": [
        "# Experimenting with the Network Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dmf1gvnwjj0E"
      },
      "source": [
        "## Using the Final \fhidden state as the output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffW7HsAY_SD_"
      },
      "source": [
        "## max_length = 40\n",
        "## epochs = 15\n",
        "## batch_size = 64\n",
        "## learning_rate = 0.001"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whLYqoGw_K4q"
      },
      "source": [
        "\n",
        "def par_selec(model_r, max_length, nnh):\n",
        "  # max_length = 40\n",
        "  epochs = 15\n",
        "  batch_size = 64\n",
        "  learning_rate = 0.001\n",
        "\n",
        "  tokenise_tf = Tokenizer()\n",
        "  tokenise_tf.fit_on_texts(training['Doc Text']) \n",
        "\n",
        "  encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "  training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "  embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "  encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "  validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "\n",
        "  adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "  model = model_r(embedding_matrix, nnh, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "  history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "  accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "  u = {}\n",
        "  u['max_len'] = max_length\n",
        "  u['batch'] = batch_size\n",
        "  u['l_rate'] = learning_rate\n",
        "  u['epochs'] = epochs\n",
        "\n",
        "  fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "  ax[0].plot(history.history['loss'], label='Training')\n",
        "  ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "  ax[0].legend(loc='upper left')\n",
        "  ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "  ax[1].plot(history.history['accuracy'], label='Training')\n",
        "  ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "  ax[1].legend(loc='upper left')\n",
        "  ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "  plt.show()\n",
        "\n",
        "  print('Accuracy for ' + str(nnh) + ' hidden units ' + str(max_length) + ' sequence length: ' + str(accuracy))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qoSZZjwb7TRx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDJxWgnn7Tw8"
      },
      "source": [
        "## Changing the no of hidden units\n",
        "## Keeping max_length = 40\n",
        "## epochs = 15\n",
        "## batch_size = 64\n",
        "## learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Yn9-A557TxB"
      },
      "source": [
        "par_selec(model_vanilla_rnn, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5ATsHkx7TxS"
      },
      "source": [
        "par_selec(model_lstm, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wa9_OGRP7TxY"
      },
      "source": [
        "par_selec(model_gru, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fs3tQI087Txe"
      },
      "source": [
        "## Doubling the no of hidden units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44Tox3Py7Txe"
      },
      "source": [
        "par_selec(model_vanilla_rnn, 40, 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cClcEyhU7Txl"
      },
      "source": [
        "### For Vanilla RNN,  With double the no of hidden units, the accuracy reduced from 60 to 56 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZrghPzq7Txm"
      },
      "source": [
        "par_selec(model_lstm, 40, 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9v5oT1L7Txr"
      },
      "source": [
        "### For LSTM, With double the no of hidden units, the accuracy slightly increased from 72 to 73 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_GUX6NVE7Tx2"
      },
      "source": [
        "par_selec(model_gru, 40, 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hXiStNo7Tx8"
      },
      "source": [
        "### For GRU, With double the no of hidden units, the accuracy didn't change from 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvcJ1EEAj-5y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCKOM-Oq7Tx9"
      },
      "source": [
        "# Halving the no of hidden units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGVzQb0G7Tx-"
      },
      "source": [
        "par_selec(model_vanilla_rnn, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vUi_W427TyE"
      },
      "source": [
        "### With halve the no of hidden units, the accuracy increased from 60 to 73."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJS0-PRv7TyG"
      },
      "source": [
        "par_selec(model_lstm, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP9J5ZPS7TyK"
      },
      "source": [
        "### For LSTM, With halve the no of hidden units, the accuracy increased from 72 to 74 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ceg8MQq87TyM"
      },
      "source": [
        "par_selec(model_gru, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JibZiZXD7TyQ"
      },
      "source": [
        "### For GRU, With halve the no of hidden units, the accuracy remain unchanged."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eScyCusZ7TyR"
      },
      "source": [
        "# Changing the max_length \n",
        "# Keeping no of hidden units = 300\n",
        "# epochs = 15\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dAgLy7G27TyR"
      },
      "source": [
        "## Doubling the sequence length from 40 to 80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGLD_44Z7TyT"
      },
      "source": [
        "par_selec(model_vanilla_rnn, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nj_9UVMY7TyX"
      },
      "source": [
        "### For vanilla rnn, With double the sequence length, there was a decrease in accuracy from 60 to 53\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4mHJuFk7TyY"
      },
      "source": [
        "par_selec(model_lstm, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyOMu3Gj7Tyd"
      },
      "source": [
        "### For LSTM, With double the sequence length, there was a decrease in accuracy from 72 to 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8y17kI_X7Tyf"
      },
      "source": [
        "par_selec(model_gru, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtNmEflB7Tyk"
      },
      "source": [
        "### For GRU, With double the sequence length, there was no change in accuracy from 50\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt3_Uirp7Tyk"
      },
      "source": [
        "# When we increase the sequence length we might include noisy (unimportant) words that would change the sentiment of the sentence\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRRMJvPi7Tyl"
      },
      "source": [
        "## Halving the sequence length from 40 to 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSNoA1q47Tym"
      },
      "source": [
        "par_selec(model_vanilla_rnn, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cphAW4jM7Tyu"
      },
      "source": [
        "### For vanilla RNN, With half the sequence length, there was an increase in accuracy from 60 to 73\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuGQ9V1i7Tyu"
      },
      "source": [
        "par_selec(model_lstm, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUyjDAP87Tyy"
      },
      "source": [
        "### For LSTM, With half the sequence length, the accuracy increased from 72 to 74 percent.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9j5BS97Tyz"
      },
      "source": [
        "par_selec(model_gru, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqoXjGKQ7Ty4"
      },
      "source": [
        "### For GRU, With half the sequence length, the accuracy increased from 50 to 74 percent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wf62lx_C7Ty4"
      },
      "source": [
        "\n",
        "## It makes sense as when we decrease the sequence length we might concentrate more on the important words that change the sentiment of the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvPmoZjc7Ty4"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBhjO4nN7Ty5"
      },
      "source": [
        "## Fine tuning the word embeddings of 300 dimensions using mittens library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9heV52M07Ty6"
      },
      "source": [
        "def finetune(training): \n",
        "  training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
        "  #training_tokens\n",
        "\n",
        "  oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
        "  print(len(oov))\n",
        "\n",
        "  corp_vocab = list(set(oov))\n",
        "\n",
        "  cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "  trr =''\n",
        "  for i in training_tokens:\n",
        "    for j in i:\n",
        "      trr+= j\n",
        "      trr += ' '\n",
        "\n",
        "  # print(trr)\n",
        "  # print(z)\n",
        "  X = cv.fit_transform([trr])\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  coocc_ar = Xc.toarray()\n",
        "\n",
        "  mittens_model = Mittens(n=300, max_iter=1000)\n",
        "\n",
        "  new_embeddings = mittens_model.fit(\n",
        "      coocc_ar,\n",
        "      vocab=corp_vocab,\n",
        "      initial_embedding_dict= embeddings)\n",
        "\n",
        "  new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
        "  return training_tokens, new_embeddings\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwP2PO-T7Ty8"
      },
      "source": [
        "embeddings2= embeddings.copy()\n",
        "\n",
        "training_tokens, new_embeddings = finetune(training)\n",
        "embeddings2.update(new_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mt98x9ps7TzA"
      },
      "source": [
        "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
        "print(len(oov2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-Pn9IMU7TzD"
      },
      "source": [
        "### Using new embeddings for the vanilla rnn em"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HQ7f4bd7TzE"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round(accuracy,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_vanilla_rnn2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_vanilla_rnn2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsOrfMuhDgOV"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_lstm(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round(accuracy,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_lstm2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_lstm2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4PHN2V08O-j"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_gru(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round(accuracy,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_gru2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_gru2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AfA31RGemoGF"
      },
      "source": [
        "# For all the three RNNs,I got best accuracy when they were trained on the pre-trained and fine-tuned word embeddings.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7V5GU3fPxjZq"
      },
      "source": [
        "batch_size_temp = [32,64,128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPARrC-3n9W2"
      },
      "source": [
        "# I got best validation accuracy of 73 with Vanilla RNN model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
        "\n",
        "## 'batch_size': 128, 'epochs': 11, 'learning_rate': 0.0016, 'max_length': 8"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtRsC-V3sETk"
      },
      "source": [
        "# best_vanilla_rnn2['batch_size'] = batch_size_temp[best_vanilla_rnn2['batch_size']]\n",
        "# best_v_r_df = pd.DataFrame(data = best_vanilla_rnn2.items(),index=best_vanilla_rnn2.keys())\n",
        "# best_v_r_df.to_csv('best_v_r.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VItBueodmo--"
      },
      "source": [
        "# I got best validation accuracy of 75 with LSTM model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
        "\n",
        "## 'batch_size': 64, 'epochs': 12, 'learning_rate': 0.0018206437643498807, 'max_length': 15"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6BodZIKsVXy"
      },
      "source": [
        "# best_l_r['batch_size'] = batch_size_temp[best_l_r['batch_size']]\n",
        "# best_l_r_df = pd.DataFrame(best_l_r.items(),index=best_l_r.keys())\n",
        "# best_l_r_df.to_csv('best_l_r.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJxPMORcn9eK"
      },
      "source": [
        "# I got best validation accuracy of 74 with GRU model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
        "\n",
        "## 'batch_size': 64, 'epochs': 11, 'learning_rate': 0.001, 'max_length': 19"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYLQRhDHskQy"
      },
      "source": [
        "# best_g_r = {'batch_size': 64, 'epochs': 11, 'learning_rate': 0.001, 'max_length': 19}\n",
        "# best_g_r_df = pd.DataFrame(best_g_r.items(),index=best_g_r.keys())\n",
        "# best_g_r_df.to_csv('best_g_r.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nb9iWGCZod88"
      },
      "source": [
        "## Out of the three above, LSTM performed best on the validation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2uQbiiJooM8"
      },
      "source": [
        "# ###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYcv8O3yoraN"
      },
      "source": [
        "# Evaluating on the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxEQ4EWronP8"
      },
      "source": [
        "test = pd.read_csv('Test Data1.csv').iloc[:,1:]\n",
        "\n",
        "test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qt8fLB8EonmO"
      },
      "source": [
        "def test_evaluation(test, model, max_length, batch_size, epochs, learning_rate):\n",
        "  tokenise_tf = Tokenizer()\n",
        "  tokenise_tf.fit_on_texts(training['Doc Text']) \n",
        "  \n",
        "  encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "  training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "  embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "  encoded_test = tokenise_tf.texts_to_sequences(test['Doc Text'])\n",
        "  test_padded = pad_sequences(encoded_test, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "  \n",
        "  adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "  model2 = model(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "  history = model2.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
        "  temp = lambda x: 1 if x>= 0.5 else 0 \n",
        "  y_pred = [temp(i) for i in model2.predict(test_padded)]\n",
        "  # print(y_pred) \n",
        "  y_test = test['Sentiment']\n",
        "  # print(y_test)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  recall = recall_score(y_test, y_pred)\n",
        "  f1 = f1_score(y_test, y_pred)\n",
        "  return accuracy, precision, recall, f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oavVnBnpzPX"
      },
      "source": [
        "# Testing Vanilla RNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVgsJS0bqL9I"
      },
      "source": [
        "best_v_r_df2 = pd.read_csv('best_v_r.csv')\n",
        "Vanilla_acc, Vanilla_prec, Vanilla_rec, Vanilla_f1= test_evaluation(test, model_vanilla_rnn, best_v_r_df2['max_length'][0], best_v_r_df2['batch_size'][0], best_v_r_df2['epochs'][0], best_v_r_df2['learning_rate'][0])\n",
        "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_acc))\n",
        "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_prec))\n",
        "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_rec))\n",
        "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdwoTc3Fz_AG"
      },
      "source": [
        "# Testing LSTM "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMrLL4Uo1fSB"
      },
      "source": [
        "best_lstm_df2 = pd.read_csv('best_l_r.csv')\n",
        "lstm_acc, lstm_prec, lstm_rec, lstm_f1= test_evaluation(test, model_lstm, best_lstm_df2['max_length'][0], best_lstm_df2['batch_size'][0], best_lstm_df2['epochs'][0], best_lstm_df2['learning_rate'][0])\n",
        "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_acc))\n",
        "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_prec))\n",
        "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_rec))\n",
        "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9x_ONNn1kLH"
      },
      "source": [
        "# Testing GRU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKqIZAaf1kNY"
      },
      "source": [
        "best_gru_df2 = pd.read_csv('best_g_r.csv')\n",
        "gru_acc, gru_prec,gru_rec,gru_f1= test_evaluation(test, model_gru, best_gru_df2['max_length'][0], best_gru_df2['batch_size'][0], best_gru_df2['epochs'][0], best_gru_df2['learning_rate'][0])\n",
        "print('Accuracy of GRU RNN on the test data: ' + str(gru_acc))\n",
        "print('Accuracy of GRU RNN on the test data: ' + str(gru_prec))\n",
        "print('Accuracy of GRU RNN on the test data: ' + str(gru_rec))\n",
        "print('Accuracy of GRU RNN on the test data: ' + str(gru_f1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjqAkObFmQZv"
      },
      "source": [
        "## I got best test accuracy of 75 percent with the GRU RNN "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_FRcBkpoMTP"
      },
      "source": [
        "# Observed differences between the performances of the three rnns:\n",
        "## Got best validation accuracy with LSTM but got best test accuracy with GRU\n",
        "## Vanilla RNN is fastest and LSTM is slowest in terms of training because LSTM has more parameters and heavier computation\n",
        "## Vanilla RNN gave comparitively lowest accuracies after hyper parameter tuning maybe because of vanishing gradient problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZNX9oam3GYG"
      },
      "source": [
        "# ##########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HimtALnV3Gbo"
      },
      "source": [
        "# Ignore the code from now on. I just wanted to save the code for element-wise max.\n",
        "# With the element_wise max of all the hidden states as the output, I got less accuracy of 50. Whatever the parameters, the accuracy didn't cross 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G84h-TiMpZel"
      },
      "source": [
        "## Taking elementwise max of the hidden states"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uE2SSiwupZel"
      },
      "source": [
        "def model_vanilla_rnn_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(SimpleRNN(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.add(MaxPool1D(max_length, strides = None))\n",
        "    # model.add(Dropout(Dropout_rate))\n",
        "\n",
        "#     model.add(Dense(1, activation='softmax'))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "    \n",
        "#     print(z)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3cyFsTtpZep"
      },
      "source": [
        "def model_lstm_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(LSTM(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.add(MaxPool1D(max_length, strides = None))\n",
        "    # model.add(Dropout(Dropout_rate))\n",
        "\n",
        "#     model.add(Dense(1, activation='softmax'))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "    \n",
        "#     print(z)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyw6ayuspZes"
      },
      "source": [
        "def model_gru_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(GRU(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.add(MaxPool1D(max_length, strides = None))\n",
        "#     model.add(Dropout(Dropout_rate))\n",
        "\n",
        "#     model.add(Dense(1, activation='softmax'))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "    \n",
        "#     print(z)\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDR5cTqT_hob"
      },
      "source": [
        "## Changing the no of hidden units\n",
        "## Keeping max_length = 40\n",
        "## epochs = 15\n",
        "## batch_size = 64\n",
        "## learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TObOEV44_frn"
      },
      "source": [
        "par_selec(model_vanilla_rnn_em, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9hd1nbGBsyZ"
      },
      "source": [
        "par_selec(model_lstm_em, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Y4MX-zeBs1u"
      },
      "source": [
        "par_selec(model_gru_em, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvA_WhEg_iG5"
      },
      "source": [
        "## Doubling the no of hidden units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUYJKAtYBoef"
      },
      "source": [
        "par_selec(model_vanilla_rnn_em, 40, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDyq_uNe_wYX"
      },
      "source": [
        "### For Vanilla RNN,  With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WxwJGQApAQJj"
      },
      "source": [
        "par_selec(model_lstm_em, 40, 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTST8ZiAWwa"
      },
      "source": [
        "### For LSTM, With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTtifvwGAXNl"
      },
      "source": [
        "par_selec(model_gru_em, 40, 600)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkTMhMc1Ac8f"
      },
      "source": [
        "### For GRU, With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AY_wx8EM_3eq"
      },
      "source": [
        "# Halving the no of hidden units"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HN94EZfo_6L1"
      },
      "source": [
        "par_selec(model_vanilla_rnn_em, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPDlfM7tADBF"
      },
      "source": [
        "### With halve the no of hidden units, the accuracy increased slightly from 73.8 to 74.4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vttKOmoP_xC7"
      },
      "source": [
        "par_selec(model_lstm_em, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xy5cfYQAj_j"
      },
      "source": [
        "### For LSTM, With halve the no of hidden units, the accuracy reduced from 74 to 73 percent."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dkRv2e_Aka8"
      },
      "source": [
        "par_selec(model_gru_em, 40, 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w4_lAZ_rAkzb"
      },
      "source": [
        "### For GRU, With halve the no of hidden units, the accuracy reduced from 74 to 73 percent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNp_oho8BMtS"
      },
      "source": [
        "# Changing the max_length \n",
        "# Keeping no of hidden units = 300\n",
        "# epochs = 15\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.001\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZg_5ohNBQqZ"
      },
      "source": [
        "## Doubling the sequence length from 40 to 80"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JsCbKK77BT2Y"
      },
      "source": [
        "par_selec(model_vanilla_rnn_em, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrwMU7rXBXgR"
      },
      "source": [
        "### For vanilla rnn, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AgG6XClUBX0N"
      },
      "source": [
        "par_selec(model_lstm_em, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uX2xyVkCBYTx"
      },
      "source": [
        "### For LSTM, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agm_JZO4BY0K"
      },
      "source": [
        "par_selec(model_gru_em, 80, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn1ht9nkBZGE"
      },
      "source": [
        "### For GRU, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "darw7z1oCaoX"
      },
      "source": [
        "### When we increase the sequence length we might include important words that would change the sentiment of the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e42nuGgaB3c2"
      },
      "source": [
        "## Halving the sequence length from 40 to 20"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wa3jFH6bBZhT"
      },
      "source": [
        "par_selec(model_vanilla_rnn_em, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHoMYBMWkK_l"
      },
      "source": [
        "# max_length = 20\n",
        "# epochs = 15\n",
        "# batch_size = 64\n",
        "# learning_rate = 0.001\n",
        "\n",
        "# encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "\n",
        "# encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "# model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "# history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "# accuracy = history.history['val_accuracy'][-1]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLpxiHHjBZxm"
      },
      "source": [
        "### For vanilla RNN, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3D0Ngk_zBaFa"
      },
      "source": [
        "par_selec(model_lstm_em, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4R2ckx5MBaP5"
      },
      "source": [
        "### For LSTM, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lulWk6EwBaaV"
      },
      "source": [
        "par_selec(model_gru_em, 20, 300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQpFaM3VBaj1"
      },
      "source": [
        "### For GRU, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuj2ablkCyQZ"
      },
      "source": [
        "\n",
        "### It makes sense as when we decrease the sequence length we might lose important words that change the sentiment of the sentence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "goykoVf2Bate"
      },
      "source": [
        "###"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzFp427zDELR"
      },
      "source": [
        "## Fine tuning the word embeddings using mittens library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qclt65kfDJBD"
      },
      "source": [
        "def finetune(training): \n",
        "  training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
        "  #training_tokens\n",
        "\n",
        "  oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
        "  print(len(oov))\n",
        "\n",
        "  corp_vocab = list(set(oov))\n",
        "\n",
        "  cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
        "  trr =''\n",
        "  for i in training_tokens:\n",
        "    for j in i:\n",
        "      trr+= j\n",
        "      trr += ' '\n",
        "\n",
        "  # print(trr)\n",
        "  # print(z)\n",
        "  X = cv.fit_transform([trr])\n",
        "  Xc = (X.T * X)\n",
        "  Xc.setdiag(0)\n",
        "  coocc_ar = Xc.toarray()\n",
        "\n",
        "  mittens_model = Mittens(n=300, max_iter=1000)\n",
        "\n",
        "  new_embeddings = mittens_model.fit(\n",
        "      coocc_ar,\n",
        "      vocab=corp_vocab,\n",
        "      initial_embedding_dict= embeddings)\n",
        "\n",
        "  new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
        "  return training_tokens, new_embeddings\n",
        "  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTxXFJy6DMLp"
      },
      "source": [
        "embeddings2= embeddings.copy()\n",
        "\n",
        "training_tokens, new_embeddings = finetune(training)\n",
        "embeddings2.update(new_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0U7PZnWnDPE8"
      },
      "source": [
        "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
        "print(len(oov2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_1MSXXvDU9k"
      },
      "source": [
        "### Using new embeddings for the vanilla rnn em"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMp1_4cVDbWR"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_vanilla_rnn_em(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round(accuracy,2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_vanilla_rnn_em2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_vanilla_rnn_em2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lj-i74diDUPT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb0o5YDrpZew"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
        "\n",
        "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    \n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = round(learning_rate,5)\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round((accuracy),2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_vanilla_rnn_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_vanilla_rnn_em)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYFAm5c_pZey"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_lstm_em(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = learning_rate\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round((accuracy),2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,50)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_lstm_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_lstm_em)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sc9KmobHpZez"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "    epochs = args['epochs']\n",
        "\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_gru_em(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    u = {}\n",
        "    u['max_len'] = max_length\n",
        "    u['batch'] = batch_size\n",
        "    u['l_rate'] = learning_rate\n",
        "    u['epochs'] = epochs\n",
        "\n",
        "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
        "\n",
        "    ax[0].plot(history.history['loss'], label='Training')\n",
        "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
        "    ax[0].legend(loc='upper left')\n",
        "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
        "\n",
        "\n",
        "    ax[1].plot(history.history['accuracy'], label='Training')\n",
        "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
        "    ax[1].legend(loc='upper left')\n",
        "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
        "    plt.show()\n",
        "\n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round((accuracy),2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,50)),  \n",
        "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_gru_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_gru_em)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QsrKXRCpZe1"
      },
      "source": [
        "# training.to_csv('Training Data.csv')\n",
        "# test.to_csv('Test Data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnjHcUEgpZe3"
      },
      "source": [
        "# def generate_embedding_matrix(word_embeddings):\n",
        "#     vocabulary_size = len(tokenizer.word_index)+1\n",
        "#     embedding_matrix = np.zeros((vocabulary_size, 300))\n",
        "#     for word, index in tokenizer.word_index.items():\n",
        "#         embedding_vector = word_embeddings.get(word)\n",
        "#         if embedding_vector is not None:\n",
        "#             embedding_matrix[index] = embedding_vector\n",
        "#     return embedding_matrix,vocabulary_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1ELmHAipZe4"
      },
      "source": [
        "# input_len = 50\n",
        "# tokenizer = Tokenizer(oov_token='<OOV>')\n",
        "# tokenizer.fit_on_texts(training['Doc Text'])\n",
        "# X_train_processed = tokenizer.texts_to_sequences(training['Doc Text'])\n",
        "# X_train_processed = pad_sequences(X_train_processed, padding='post', maxlen=input_len,truncating='pre')\n",
        "# embedding_matrix,vocabulary_size = generate_embedding_matrix(embeddings)\n",
        "# tf.keras.backend.clear_session()\n",
        "# model = Sequential()\n",
        "# embedding_layer = Embedding(vocabulary_size, 300, weights=[embedding_matrix], input_length=input_len,trainable = False,mask_zero=True)\n",
        "# model.add(embedding_layer)\n",
        "# model.add(SimpleRNN(300,kernel_regularizer = tf.keras.regularizers.L1(0.01)))\n",
        "\n",
        "# model.add(Dropout(0.5))\n",
        "# model.add(Dense(1, activation='sigmoid'))\n",
        "# # adam_optimizer = optimizers.adam(learning_rate=learning_rate)\n",
        "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "# print(model.summary())\n",
        "# model.fit(X_train_processed, training['Sentiment'], batch_size=128, epochs=30, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uZK2i9tpZe6"
      },
      "source": [
        "def model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(Bidirectional(SimpleRNN(units = 300, activation = activation, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
        "    model.add(Dropout(Dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn7QKQzJpZe8"
      },
      "source": [
        "max_length = 40\n",
        "epochs = 15\n",
        "learning_rate = 0.001\n",
        "batch_size = 64\n",
        "encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "model = model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
        "accuracy = history.history['val_accuracy'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_DL0cJypZe_"
      },
      "source": [
        "def model_lstm_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
        "\n",
        "    model = Sequential()\n",
        "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
        "    model.add(e)\n",
        "    model.add(Bidirectional(LSTM(units = 300, activation = activation, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
        "    model.add(Dropout(Dropout_rate))\n",
        "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
        "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "#     print(model.summary())\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "13APqbFlpZfA"
      },
      "source": [
        "max_length = 40\n",
        "epochs = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 128\n",
        "encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "model = model_lstm_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "\n",
        "history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
        "accuracy = history.history['val_accuracy'][-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpnT_uzUpZfC"
      },
      "source": [
        "def objective_func(args):\n",
        "    max_length = args['max_length']\n",
        "#     af_in_simpleRNN = args['af_in_simpleRNN']\n",
        "    batch_size = args['batch_size']\n",
        "    learning_rate = args['learning_rate']\n",
        "#     padd = args['padd']\n",
        "#     trunc = args['trunc']\n",
        "    epochs = args['epochs']\n",
        "#     kernel_regularizer_coef = args['kernel_regularizer_coef']\n",
        "#     activity_regularizer_coef = args['activity_regularizer_coef']\n",
        "#     Dropout_rate = args['Dropout_rate']\n",
        "    \n",
        "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
        "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
        "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
        "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
        "    model = model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
        "    \n",
        "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
        "    accuracy = history.history['val_accuracy'][-1]\n",
        "    \n",
        "    # summarize history for accuracy\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('model accuracy')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend(['training', 'validation'], loc='upper left')\n",
        "    plt.show()    \n",
        "    \n",
        "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
        "\n",
        "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
        "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
        "    return -round((accuracy),2)\n",
        "\n",
        "space = {'max_length': hp.choice('max_length',range(4,100)), \n",
        "#          'af_in_simpleRNN': hp.choice('af_in_simpleRNN', ['tanh', 'sigmoid']), \n",
        "        'batch_size': hp.choice('batch_size', [64, 128, 256]),\n",
        "#          'padd': hp.choice('padd', ['pre', 'post']),\n",
        "#          'trunc': hp.choice('trunc', ['pre', 'post']),\n",
        "         'epochs': hp.choice('epochs',range(5,20)), \n",
        "         'learning_rate': hp.uniform('learning_rate', 0,1)\n",
        "#          'Dropout_rate': hp.uniform('Dropout_rate', 0, 1),\n",
        "#          'kernel_regularizer_coef': hp.uniform('kernel_regularizer_coef', 0, 10),\n",
        "#          'activity_regularizer_coef': hp.uniform('activity_regularizer_coef', 0, 10)\n",
        "        }\n",
        "                                \n",
        "                                \n",
        "tokenise_tf = Tokenizer()\n",
        "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
        "best_vanilla_rnn_bd = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
        "print(best_vanilla_rnn_bd)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dn_G84lpZfD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}