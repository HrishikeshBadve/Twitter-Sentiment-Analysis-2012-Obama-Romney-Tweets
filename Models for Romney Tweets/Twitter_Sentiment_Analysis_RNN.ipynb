{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Twitter_Sentiment_Analysis_RNN.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"code","metadata":{"id":"78PhOL0jpZdY","outputId":"c8c26556-eb34-4804-edae-1e3b5629bf99","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AEM1ZS6vpu6b","outputId":"c1445094-73dc-436c-e07c-a48c04464588","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd /content/drive/My Drive/Research Project"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Research Project\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QsHo75fupasE"},"source":["import pandas as pd\n","import numpy as np\n","import os\n","# import src\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ePRqI5M3qPi","outputId":"041fa584-9641-4638-cfe3-1c1b488c54ed","colab":{"base_uri":"https://localhost:8080/"}},"source":["pip install -U mittens"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting mittens\n","  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.18.5)\n","Installing collected packages: mittens\n","Successfully installed mittens-0.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hgktrAVlpZdb"},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.model_selection import train_test_split, cross_val_score, KFold\n","from statistics import mean, stdev, median, mode\n","# With PCA\n","from sklearn.decomposition import PCA\n","\n","from sklearn.feature_extraction.text import CountVectorizer\n","# SVM\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","# KNN\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","\n","from mittens import GloVe, Mittens"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO6oNPy7px_h","outputId":"e3560c1e-0ba7-4359-c6f6-03ac586f3722","colab":{"base_uri":"https://localhost:8080/"}},"source":["pip install autocorrect"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting autocorrect\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/71/eb8c1f83439dfe6cbe1edb03be1f1110b242503b61950e7c292dd557c23e/autocorrect-2.2.2.tar.gz (621kB)\n","\r\u001b[K     |▌                               | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 22.0MB/s eta 0:00:01\r\u001b[K     |█▋                              | 30kB 17.2MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 14.8MB/s eta 0:00:01\r\u001b[K     |██▋                             | 51kB 12.3MB/s eta 0:00:01\r\u001b[K     |███▏                            | 61kB 10.6MB/s eta 0:00:01\r\u001b[K     |███▊                            | 71kB 9.3MB/s eta 0:00:01\r\u001b[K     |████▏                           | 81kB 9.6MB/s eta 0:00:01\r\u001b[K     |████▊                           | 92kB 9.7MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 102kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 112kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 122kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 133kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 143kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 153kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 163kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 174kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 184kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 194kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 204kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 215kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 225kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 235kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 245kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 256kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 266kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 276kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 286kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 296kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 307kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 317kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 327kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 337kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 348kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 358kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 368kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 378kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 389kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 399kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 409kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 419kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 430kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 440kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 450kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 460kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 471kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 481kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 491kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 501kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 512kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 522kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 532kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 542kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 552kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 563kB 9.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 573kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 583kB 9.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 593kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 604kB 9.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 614kB 9.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 624kB 9.8MB/s \n","\u001b[?25hBuilding wheels for collected packages: autocorrect\n","  Building wheel for autocorrect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for autocorrect: filename=autocorrect-2.2.2-cp36-none-any.whl size=621491 sha256=cf7cf3da46e9683136b62633842be613dee634683073fc5ee284b850b20b7492\n","  Stored in directory: /root/.cache/pip/wheels/b4/0b/7d/98268d64c8697425f712c897265394486542141bbe4de319d6\n","Successfully built autocorrect\n","Installing collected packages: autocorrect\n","Successfully installed autocorrect-2.2.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qXIBsqXjpZdd"},"source":["\n","import nltk\n","# nltk.download()\n","import string\n","from nltk.tokenize import word_tokenize\n","import random\n","import pickle\n","from nltk.corpus import stopwords\n","\n","from autocorrect import Speller\n","# from pycontractions import Contractions\n","\n","# from spellchecker import SpellChecker\n","\n","import re\n","from nltk.corpus import wordnet\n","from nltk.stem.wordnet import WordNetLemmatizer \n","\n","from hyperopt import fmin, tpe, hp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BALelbZyNU02","outputId":"7fd26c72-6ed9-45a1-c9ad-0c6a6fb506a7","colab":{"base_uri":"https://localhost:8080/"}},"source":["nltk.download('punkt')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"0lim7c1rpZdf"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"w8gGSNnE_Ngv"},"source":["import tensorflow_addons as tfa\n","\n","from numpy import asarray\n","from numpy import zeros\n","from sklearn.utils import shuffle\n","from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n","from tensorflow.keras import optimizers\n","from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n","import tensorflow.keras.backend as K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IUnMZe9zpZdh"},"source":["# nltk.download('all')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cBTlpT5I_Nhp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dugwt7lTaCiA"},"source":["### Getting Data"]},{"cell_type":"code","metadata":{"id":"AD9U00CVddNC"},"source":["# data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"96eAfNsDaiVd"},"source":["data_path = r'/content/drive/My Drive/Research Project/Data'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kljiyU8TaCiF"},"source":["# obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training Data.csv'), usecols = [1,2])\n","romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training Data.csv'), usecols = [1,2])\n","\n","# obama_val_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Validation Data.csv'), usecols = [1,2])\n","romney_val_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Validation Data.csv'), usecols = [1,2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mcLnGHdPaCiI","outputId":"13ff1594-68b8-4ab2-f331-8237cb7c2310","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["romney_train_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>obama need b remove dangerous white house czar...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>sidenote obama profile sexy back debate</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>hillary take bullet obama n buck still fall</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>morgan freeman obama best commercial ever need...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>bitch like obama bitch want food stamp lmao</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  obama need b remove dangerous white house czar...  Negative\n","1            sidenote obama profile sexy back debate  Positive\n","2        hillary take bullet obama n buck still fall  Negative\n","3  morgan freeman obama best commercial ever need...  Positive\n","4        bitch like obama bitch want food stamp lmao   Neutral"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"SpmtPe1Ih_sO","outputId":"3a6a46fc-0d72-4daf-ab33-1c0f7a1aa286","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["romney_val_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>hell obama can not even take care one border p...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>obama save auto industry enough say team obama</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>great obama hope take today sport section cheat</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>appreciate obama capitalisizes fact black</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>ann romney michelle obama dress second debate ...</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  hell obama can not even take care one border p...  Negative\n","1     obama save auto industry enough say team obama  Positive\n","2    great obama hope take today sport section cheat  Negative\n","3          appreciate obama capitalisizes fact black  Negative\n","4  ann romney michelle obama dress second debate ...   Neutral"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"XORvLV2iaCiM"},"source":["# obama_train_pr_df = obama_train_pr_df.dropna()\n","romney_train_pr_df = romney_train_pr_df.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l4qdpS83daOj"},"source":["romney_train1_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QgMnAHp2h_sj","outputId":"5b487d62-4f2f-4305-b354-a7c43d7b923b","colab":{"base_uri":"https://localhost:8080/","height":204}},"source":["romney_train1_pr_df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Doc Text</th>\n","      <th>Sentiment</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>obama debate cracker as cracker tonight tune t...</td>\n","      <td>Positive</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>miss point afraid understand big picture dont ...</td>\n","      <td>Neutral</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>raise democrat leave party year ago lifetime n...</td>\n","      <td>Negative</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>obama camp can not afford low expectation toni...</td>\n","      <td>Neutral</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            Doc Text Sentiment\n","0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n","1  obama debate cracker as cracker tonight tune t...  Positive\n","2  miss point afraid understand big picture dont ...   Neutral\n","3  raise democrat leave party year ago lifetime n...  Negative\n","4  obama camp can not afford low expectation toni...   Neutral"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"xX1IKkGsh_sq"},"source":["romney_train1_pr_df = romney_train1_pr_df.dropna()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"omba0Abfh_sz"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Nt4Q-LLu_Nh6"},"source":["def one_hot(data):\n","    data = np.asarray(data)\n","    temp = np.zeros((len(data),3))\n","#     print(data[0])\n","    for i in range(len(temp)):\n","        if data[i] == 'Negative':\n","            temp[i][2] = 1 ## Negative sentiment third neuron\n","        elif data[i] == 'Neutral':\n","            temp[i][1] = 1 ## Neutral sentiment second neuron  \n","        else:\n","            temp[i][0] = 1 ## Positive sentiment first neuron \n","\n","    return temp\n","    \n","    \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ETvx1WoKfrfk"},"source":["def pred(x):\n","    temp = []\n","    for i in x:\n","        m = np.argmax(i)\n","        if m == 0:\n","            temp.append('Positive')\n","        elif m == 1:\n","            temp.append('Neutral')\n","        else:\n","            temp.append('Negative')\n","    return temp"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmGshHslpZeH"},"source":["## Building Glove Dictionary"]},{"cell_type":"code","metadata":{"id":"YU-SxaOZ_Nh9"},"source":["embeddings = {}\n","with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n","    for line in file:\n","        values = line.split()\n","        word = values[0]\n","        vector = asarray(values[1:], dtype='float32')\n","        embeddings[word] = vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"or3hmQvdpZeJ"},"source":["## Embedding Matrix Function"]},{"cell_type":"code","metadata":{"id":"wBG3Kq7EpZeJ"},"source":["def emb_matrix(t,embeddings, we_dim):\n","    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n","    vocab_size = len(t.word_index) + 1    \n","    embedding_matrix = zeros((vocab_size, we_dim))\n","    for word, i in t.word_index.items():\n","        embedding_vector = embeddings.get(word)\n","        if embedding_vector is not None:\n","            embedding_matrix[i] = embedding_vector\n","    return embedding_matrix, vocab_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9kT9ZAXi_NiC"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MgK6d3Qy_NiF"},"source":["## Fine tuning the word embeddings of 300 dimensions using mittens library\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VqwJmPT-_NiF"},"source":["## Used the code for finetuning from the following link:\n","### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"]},{"cell_type":"code","metadata":{"id":"HRwhIKRT_NiF"},"source":["def finetune(training): \n","    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n","    #training_tokens\n","\n","    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n","    print(len(oov))\n","\n","    corp_vocab = list(set(oov))\n","\n","    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n","    trr =''\n","    for i in training_tokens:\n","        for j in i:\n","            trr+= j\n","            trr += ' '\n","\n","    # print(trr)\n","    # print(z)\n","    X = cv.fit_transform([trr])\n","    Xc = (X.T * X)\n","    Xc.setdiag(0)\n","    coocc_ar = Xc.toarray()\n","\n","    mittens_model = Mittens(n=200, max_iter=len(oov)+200)\n","\n","    new_embeddings = mittens_model.fit(\n","      coocc_ar,\n","      vocab=corp_vocab,\n","      initial_embedding_dict= embeddings)\n","\n","    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n","    return training_tokens, new_embeddings\n","  \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MB2Nd-Kr_NiH","outputId":"12b78681-b78f-4765-d181-ad5595254ec8","colab":{"base_uri":"https://localhost:8080/"}},"source":["embeddings2= embeddings.copy()\n","\n","training_tokens, new_embeddings = finetune(obama_train_pr_df)\n","embeddings2.update(new_embeddings)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["567\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n"],"name":"stdout"},{"output_type":"stream","text":["Iteration 760: loss: 0.010824520140886307"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"spGvX_Cg_NiI","outputId":"e060c009-b1bc-4c7b-c923-1034552cdbb3","colab":{"base_uri":"https://localhost:8080/"}},"source":["oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n","print(len(oov2))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ir2jPLOS_NiK"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ldH6dPIM_NiL"},"source":["## Custom F1 value"]},{"cell_type":"code","metadata":{"id":"9Q5Q1HVs_NiM"},"source":["def f1_value(y_true, y_pred): #taken from old keras source code\n","    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n","    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n","    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n","    precision = true_positives / (predicted_positives + K.epsilon())\n","    recall = true_positives / (possible_positives + K.epsilon())\n","    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n","    return f1_val"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3VHQfx6SpZeN"},"source":["## Building Vanilla RNN, LSTM, and GRU models"]},{"cell_type":"code","metadata":{"id":"sNbzJtMWpZeN"},"source":["def model_vanilla_rnn(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n","\n","    model = Sequential()\n","    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n","    model.add(e)\n","    model.add(Bidirectional(SimpleRNN(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n","    model.add(Dropout(Dropout_rate))\n","    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n","#     print(model.summary())\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D69lMKSLpZeP"},"source":["def model_lstm(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n","\n","    model = Sequential()\n","    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n","    model.add(e)\n","    model.add(Bidirectional(LSTM(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n","    model.add(Dropout(Dropout_rate))\n","    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n","#     print(model.summary())\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDL6HFQYpZeQ"},"source":["def model_gru(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n","\n","    model = Sequential()\n","    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n","    model.add(e)\n","    model.add(Bidirectional(GRU(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n","    model.add(Dropout(Dropout_rate))\n","    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n","    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n","#     print(model.summary())\n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"42FDB3zmpZeS"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ZHXpH9_Tg9Q"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YR6ArxHZUA0H"},"source":["# max_length = 20\n","# epochs = 2\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","\n","# tokenise_tf = Tokenizer()\n","# tokenise_tf.fit_on_texts(obama_train2_pr_df['Doc Text'])   \n","# encoded_train = tokenise_tf.texts_to_sequences(obama_train2_pr_df['Doc Text'])\n","# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n","\n","# encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n","# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","# model = model_vanilla_rnn(embedding_matrix, 300, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n","# history = model.fit(training_padded, one_hot(obama_train2_pr_df['Sentiment']), epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True)\n","# y_pred_temp = model.predict(validation_padded)\n","# y_pred = pred(y_pred_temp)\n","# f1_list= np.round(f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None),3)\n","# accuracy = accuracy_score(obama_val_pr_df['Sentiment'], y_pred)\n","# f1_dict = {'f1_pos': f1_list[2], 'f1_neu': f1_list[1], 'f1_neg': f1_list[0]}\n","# # print([f1_list[2],f1_list[1],f1_list[0]])\n","# print(f1_dict)\n","# f1 = mean([accuracy, f1_list[2], f1_list[0]])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dzh3dE1gU0Yf","outputId":"2c74dbd0-5b3a-4710-c442-1c2142a30018","colab":{"base_uri":"https://localhost:8080/"}},"source":["def objective_func_GRU(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","    em_dim = args['em_dim']\n","\n","    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'em_dim': em_dim}\n","    print(par_dict)\n","    encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n","    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n","\n","    encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n","    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","    model = model_gru(embedding_matrix, em_dim, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n","    history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","    y_pred_temp = model.predict(validation_padded)\n","    y_pred = pred(y_pred_temp)\n","    f1_list= np.round(f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None),3)\n","    accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3)\n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_list[2], 'f1_neu': f1_list[1], 'f1_neg': f1_list[0]}\n","    # print([f1_list[2],f1_list[1],f1_list[0]])\n","    print('\\n')\n","    print(eval_dict)\n","    f1 = round(mean([accuracy, f1_list[2], f1_list[0]]),3)\n","\n","\n","    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n","#     f1 = history.history['f1_value'][-1]\n","   \n","    return - (f1)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n","         'em_dim': hp.choice('em_dim', [50, 100, 200])\n","        }\n","                                \n","we_dim = 200                               \n","tokenise_tf = Tokenizer()\n","tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text'])                                \n","best_gru = fmin(objective_func_GRU, space, algo=tpe.suggest, max_evals=10)\n","print(best_gru)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["{'max_length': 17, 'batch_size': 64, 'learning_rate': 0.07774962422377119, 'epochs': 6, 'em_dim': 100}\n","  0%|          | 0/10 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n","WARNING:tensorflow:Layer gru_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"TfM5yBjxl-5P"},"source":["def objective_func_LSTM(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","    em_dim = args['em_dim']\n","\n","    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs, 'em_dim': em_dim}\n","    print(par_dict)\n","    encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n","    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n","\n","    encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n","    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","    model = model_gru(embedding_matrix, em_dim, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n","    history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","    y_pred_temp = model.predict(validation_padded)\n","    y_pred = pred(y_pred_temp)\n","    f1_list= np.round(f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None),3)\n","    accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3)\n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_list[2], 'f1_neu': f1_list[1], 'f1_neg': f1_list[0]}\n","    # print([f1_list[2],f1_list[1],f1_list[0]])\n","    print('\\n')\n","    print(eval_dict)\n","    f1 = round(mean([accuracy, f1_list[2], f1_list[0]]),3)\n","\n","\n","    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n","#     f1 = history.history['f1_value'][-1]\n","   \n","    return - (f1)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n","         'em_dim': hp.choice('em_dim', [50, 100, 200])\n","        }\n","                                \n","we_dim = 200                               \n","tokenise_tf = Tokenizer()\n","tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text'])                                \n","best_LSTM = fmin(objective_func_LSTM, space, algo=tpe.suggest, max_evals=10)\n","print(best_LSTM)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FFlG0HflnwG6"},"source":["print(dfaasd)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hnDrYVDC_Nib"},"source":["## Building models for obama tweets"]},{"cell_type":"markdown","metadata":{"id":"HLDi0elvjHkC"},"source":["## Training the Model"]},{"cell_type":"markdown","metadata":{"id":"SBp3ntnMpZeX"},"source":["## Using Hyperopt library to tune the Hyperparameters. "]},{"cell_type":"markdown","metadata":{"id":"iD2A19Iy42R1"},"source":["### Cross Valid Function for LSTM"]},{"cell_type":"code","metadata":{"id":"GTx5aKtXaCkP"},"source":["def cross_valid_lstm(X,y,epochs,batch_size,max_length, we_dim, learning_rate):\n","    f1_Positive  =[]\n","    f1_Neutral =[]\n","    f1_Negative =[]\n","    acc =[]\n","    cv = KFold(n_splits=5,shuffle=True)\n","    for train_index, val_index in cv.split(X):\n","    #     print(\"Train Index: \", train_index, \"\\n\")\n","    #     print(\"Test Index: \", test_index)\n","    #     print(X.iloc[train_index])\n","    #     print(f)\n","\n","        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n","        tokenise_tf = Tokenizer()\n","        tokenise_tf.fit_on_texts(X_train1) \n","    \n","        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n","        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n","\n","        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n","        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","        model = model_lstm(embedding_matrix, 300, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)        \n","        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        y_pred_temp = model.predict(validation_padded)\n","        y_pred = pred(y_pred_temp)\n","\n","        f1_temp = f1_score(y_val, y_pred, average = None)\n","\n","        f1_Positive.append(f1_temp[2])\n","        f1_Neutral.append(f1_temp[1])\n","        f1_Negative.append(f1_temp[0])\n","\n","        acc.append(round(accuracy_score(y_val, y_pred),3))\n","\n","    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JECfQFe446Ke"},"source":["### Cross Valid Function for GRU"]},{"cell_type":"code","metadata":{"id":"MHbmJeGS4Uan"},"source":["def cross_valid_gru(X,y,epochs,batch_size,max_length, em_dim, we_dim, learning_rate):\n","    f1_Positive  =[]\n","    f1_Neutral =[]\n","    f1_Negative =[]\n","    acc =[]\n","    cv = KFold(n_splits=5,shuffle=True)\n","    for train_index, val_index in cv.split(X):\n","    #     print(\"Train Index: \", train_index, \"\\n\")\n","    #     print(\"Test Index: \", test_index)\n","    #     print(X.iloc[train_index])\n","    #     print(f)\n","\n","        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n","        tokenise_tf = Tokenizer()\n","        tokenise_tf.fit_on_texts(X_train1) \n","    \n","        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n","        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n","\n","        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n","        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","        model = model_gru(embedding_matrix, em_dim, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)        \n","        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","        y_pred_temp = model.predict(validation_padded)\n","        y_pred = pred(y_pred_temp)\n","\n","        f1_temp = f1_score(y_val, y_pred, average = None)\n","\n","        f1_Positive.append(f1_temp[2])\n","        f1_Neutral.append(f1_temp[1])\n","        f1_Negative.append(f1_temp[0])\n","\n","        acc.append(round(accuracy_score(y_val, y_pred),3))\n","\n","    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6qsr1PKaCkS"},"source":["# max_length = 8\n","# epochs = 1\n","# batch_size = 64\n","# learning_rate = 0.001\n","# we_dim = 200\n","# em_dim = 200\n","\n","# # accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n","# temp = cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n","# temp\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TitWsucZ6C18"},"source":["# import warnings\n","# warnings.filterwarnings('ignore');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wa5Tzpa85DTd"},"source":["### Tuning Hyperparameters of GRU with Cross Valid GRU"]},{"cell_type":"code","metadata":{"id":"6PBOzWHwaCkU"},"source":["def objective_func_GRU(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","    em_dim = args['em_dim']\n","\n","    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_gru(obama_train1_pr_df['Doc Text'],obama_train1_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n","    f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","    print('\\n')\n","    print(eval_dict)\n","\n","    return -(f1)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n","         'em_dim': hp.choice('em_dim', [50, 100, 200])\n","        }                     \n","                                \n","we_dim = 200                               \n","best_GRU_cv = fmin(objective_func_GRU, space, algo=tpe.suggest, max_evals=20)\n","print(best_GRU_cv)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bxq3SGbN5bST"},"source":["### Tuning Hyperparameters of LSTM with Cross Valid LSTM"]},{"cell_type":"code","metadata":{"id":"Jx7bgJXn5eu_"},"source":["def objective_func_LSTM(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","    em_dim = args['em_dim']\n","\n","    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_lstm(obama_train1_pr_df['Doc Text'],obama_train1_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n","    f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n","    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n","    print('\\n')\n","    print(eval_dict)\n","\n","    return -(f1)\n","\n","space = {'max_length': hp.choice('max_length',range(4,60)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(5,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n","         'em_dim': hp.choice('em_dim', [50, 100, 200])\n","        }                       \n","                                \n","we_dim = 200                                \n","best_LSTM_cv = fmin(objective_func_LSTM, space, algo=tpe.suggest, max_evals=20)\n","print(best_LSTM_cv)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GhZTgQGmaCkW"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Am8LqqY96ghg"},"source":["def objective_func(args):\n","    max_length = args['max_length']\n","    batch_size = args['batch_size']\n","    learning_rate = args['learning_rate']\n","    epochs = args['epochs']\n","\n","    \n","    encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n","    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n","\n","    encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n","    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n","\n","    history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n","    y_pred_temp = model.predict(validation_padded)\n","    y_pred = pred(y_pred_temp)\n","    f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n","    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n","#     f1 = history.history['f1_value'][-1]\n","   \n","    return -round(f1,2)\n","\n","space = {'max_length': hp.choice('max_length',range(4,20)),  \n","        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n","         'epochs': hp.choice('epochs',range(10,30)), \n","         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n","        }\n","                                \n","                                \n","tokenise_tf = Tokenizer()\n","tokenise_tf.fit_on_texts(obama_train['Doc Text'])                                \n","best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n","print(best_vanilla_rnn)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jfD4GJ3tNQbo"},"source":["max_length = 5\n","epochs = 5\n","batch_size = 64\n","learning_rate = 0.0007711038235329859\n","\n","tokenise_tf = Tokenizer()\n","tokenise_tf.fit_on_texts(obama_train['Doc Text'])   \n","encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n","training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n","embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n","\n","encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n","validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n","\n","adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n","model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n","\n","history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True)\n","y_pred_temp = model.predict(validation_padded)\n","y_pred = pred(y_pred_temp)\n","acc= accuracy_score(obama_val['Sentiment'], y_pred)\n","print(acc)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6vO3cE8h-Np"},"source":["# For LSTM rnn, the best hyper parameters are:\n","# 'batch_size': 128, 'epochs': 10, 'learning_rate': 0.002, 'max_length': 50"]},{"cell_type":"code","metadata":{"id":"Cf5oOkX5h_15"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"illGwOwGiO_y"},"source":["# For GRU rnn, the best hyper parameters are:\n","# 'batch_size': 64, 'epochs': 7, 'learning_rate': 0.0006, 'max_length': 11"]},{"cell_type":"code","metadata":{"id":"hTooPqJ3pZej"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SmX0YNPUQDjL"},"source":[""],"execution_count":null,"outputs":[]}]}