{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ce569851-00f4-4d5e-885f-09017ba06d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "33c0b826-09cd-46a9-d064-b3d2e3f29dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "3daebdaa-b3bf-4e0e-db83-54f15377e157"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "e0d02290-c4fe-42de-a584-b148f9e7ea94"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "BALelbZyNU02"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "CCe7BteZpZdj"
   },
   "outputs": [],
   "source": [
    "# # Load your favorite word2vec model\n",
    "# cont = Contractions('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "# text = \"we're\"\n",
    "# text = list(cont.expand_texts([text], precise=True))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "\n",
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "96eAfNsDaiVd"
   },
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kljiyU8TaCiF"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2])\n",
    "romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mcLnGHdPaCiI",
    "outputId": "a08bc803-0a20-4a28-f538-77929db4d5df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...  Negative\n",
       "1                     mean like romney cheat primary  Negative\n",
       "2          mitt romney still believe black president  Negative\n",
       "3  romney tax plan deserve nd look secret one dif...  Negative\n",
       "4        hope romney debate prepped people last time  Positive"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XORvLV2iaCiM"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr_df = obama_train_pr_df.dropna()\n",
    "romney_train_pr_df = romney_train_pr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...  Negative\n",
       "1                     mean like romney cheat primary  Negative\n",
       "2          mitt romney still believe black president  Negative\n",
       "3  romney tax plan deserve nd look secret one dif...  Negative\n",
       "4        hope romney debate prepped people last time  Positive"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "mXJwrtOtaCiQ"
   },
   "outputs": [],
   "source": [
    "def tf_idf(x):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(x)\n",
    "    #print(tfidf_matrix)\n",
    "    x1 = tfidf_matrix.toarray()\n",
    "#     print(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "hM-Q2KTBaCiU"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr_tfidf = tf_idf(obama_train_pr_df['Doc Text'])\n",
    "romney_train_pr_tfidf = tf_idf(romney_train_pr_df['Doc Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NimCgs87eUzF",
    "outputId": "c6ecaa8e-c556-4bb8-d907-fe29294689b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5647, 6544)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWPQASu9eUzG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm66hwR6eUzJ"
   },
   "source": [
    "### Cross Valid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "a6F0wDNPeUzJ"
   },
   "outputs": [],
   "source": [
    "def cross_valid(X,y,model):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(X_train1, temp)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Valid func with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_PCA(X,y,model, n):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        x_pca1 = pca.fit_transform(X_train1)\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(x_pca1, temp)\n",
    "        \n",
    "        x_pca_val = pca.transform(X_val)\n",
    "        y_pred = model.predict(x_pca_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhRyNUshSYd"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "-PbiFcztf_NY"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP6ste5OD_9a",
    "outputId": "2a7a71e4-4f73-4130-ec7d-c6c80e0f44e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # clf = SVC(kernel=\"rbf\", gamma=1, class_weight='balanced')\n",
    "# #     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
    "# #     f1 = mean(error) + stdev(error) \n",
    "# #print(f1)\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# # print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "\n",
    "# print(eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "JoDEYnJDhX6a",
    "outputId": "4721cafd-2b1b-47fd-a178-a47f10f631cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.557, 'f1_pos': 0.479, 'f1_neu': 0.44, 'f1_neg': 0.66, 'eval_score': 0.565}\n",
      "{'accuracy': 0.548, 'f1_pos': 0.463, 'f1_neu': 0.435, 'f1_neg': 0.651, 'eval_score': 0.554}\n",
      "{'accuracy': 0.535, 'f1_pos': 0.439, 'f1_neu': 0.422, 'f1_neg': 0.642, 'eval_score': 0.539}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.494, 'f1_neu': 0.451, 'f1_neg': 0.655, 'eval_score': 0.569}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.486, 'f1_neu': 0.44, 'f1_neg': 0.658, 'eval_score': 0.567}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.461, 'f1_neu': 0.449, 'f1_neg': 0.656, 'eval_score': 0.557}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.498, 'f1_neu': 0.44, 'f1_neg': 0.649, 'eval_score': 0.567}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.453, 'f1_neu': 0.438, 'f1_neg': 0.656, 'eval_score': 0.553}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.447, 'f1_neu': 0.435, 'f1_neg': 0.661, 'eval_score': 0.553}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.477, 'f1_neu': 0.441, 'f1_neg': 0.654, 'eval_score': 0.562}\n",
      "100%|██████████| 10/10 [23:31<00:00, 141.16s/trial, best loss: -0.569]\n",
      "{'C': 0.5053794421087532}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR(args):\n",
    "\n",
    "    C = args['C']\n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,10),\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.8571377955328698, 'n': 200}                   \n",
      "{'accuracy': 0.524, 'f1_pos': 0.479, 'f1_neu': 0.439, 'f1_neg': 0.605, 'eval_score': 0.536}\n",
      "{'C': 0.40387625537931315, 'n': 100}                                \n",
      "{'accuracy': 0.504, 'f1_pos': 0.448, 'f1_neu': 0.433, 'f1_neg': 0.581, 'eval_score': 0.511}\n",
      "{'C': 1.794597882868036, 'n': 300}                                  \n",
      "{'accuracy': 0.533, 'f1_pos': 0.48, 'f1_neu': 0.442, 'f1_neg': 0.62, 'eval_score': 0.544}\n",
      "{'C': 0.37922168127727574, 'n': 450}                                \n",
      "{'accuracy': 0.538, 'f1_pos': 0.49, 'f1_neu': 0.436, 'f1_neg': 0.626, 'eval_score': 0.551}\n",
      "{'C': 0.39877146758101367, 'n': 300}                                \n",
      "{'accuracy': 0.528, 'f1_pos': 0.469, 'f1_neu': 0.435, 'f1_neg': 0.62, 'eval_score': 0.539}\n",
      "{'C': 1.2532023308159683, 'n': 100}                                 \n",
      "{'accuracy': 0.503, 'f1_pos': 0.45, 'f1_neu': 0.432, 'f1_neg': 0.581, 'eval_score': 0.511}\n",
      "{'C': 0.23684789267426987, 'n': 400}                                \n",
      "{'accuracy': 0.531, 'f1_pos': 0.492, 'f1_neu': 0.421, 'f1_neg': 0.622, 'eval_score': 0.548}\n",
      "{'C': 0.15719629578966532, 'n': 400}                                \n",
      "{'accuracy': 0.53, 'f1_pos': 0.472, 'f1_neu': 0.429, 'f1_neg': 0.624, 'eval_score': 0.542}\n",
      "{'C': 0.09961200993019559, 'n': 450}                                \n",
      "{'accuracy': 0.535, 'f1_pos': 0.478, 'f1_neu': 0.432, 'f1_neg': 0.629, 'eval_score': 0.547}\n",
      "{'C': 1.9140464855003492, 'n': 450}                                 \n",
      "{'accuracy': 0.531, 'f1_pos': 0.48, 'f1_neu': 0.431, 'f1_neg': 0.622, 'eval_score': 0.544}\n",
      "100%|██████████| 10/10 [05:23<00:00, 32.30s/trial, best loss: -0.551]\n",
      "{'C': 0.37922168127727574, 'n': 7}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR_PCA(args):\n",
    "\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    \n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    "\n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    par_dict = {'C': C, 'n': n}\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(par_dict)\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,2),\n",
    "         'n': hp.choice('n', np.arange(100,500, step =50))\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR_PCA = fmin(objective_func_LR_PCA, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With Bayesian Optimization, different class weights and LR\n",
    "\n",
    "# def objective_func_LR(args):\n",
    "\n",
    "#     C = args['C']\n",
    "#     balance = args['balance']\n",
    "#     # multi_class = args['multi_class']\n",
    "#     # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "#     clf = LogisticRegression(C= C,class_weight=balance,n_jobs =-1)\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "#     eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "#     print(eval_dict)\n",
    "#     return -(eval_score)\n",
    "\n",
    "\n",
    "# space = {'C': hp.uniform('C', 0,10),\n",
    "#          'balance': hp.choice('balance', [{1:3, 2:1, 3:2}, {1:15, 2:1, 3:10}, {1:20, 2:5, 3:10}, {1:100, 2:10, 3:100}])\n",
    "#         # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "#         # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "#         #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "#         }                        \n",
    "                                \n",
    "                                \n",
    "# best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1\n",
    "n = 3\n",
    "kernel = 'rbf'\n",
    "\n",
    "par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "print(par_dict)\n",
    "\n",
    "clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "print(eval_dict)\n",
    "print('\\n')    \n",
    "return -(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "58UghlAQeUzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.6152775265666071, 'n': 603, 'kernel': 'rbf'}  \n",
      "{'accuracy': 0.548, 'f1_pos': 0.508, 'f1_neu': 0.451, 'f1_neg': 0.632, 'eval_score': 0.563}\n",
      "{'C': 0.45868449743882456, 'n': 903, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.543, 'f1_pos': 0.507, 'f1_neu': 0.462, 'f1_neg': 0.619, 'eval_score': 0.556}\n",
      "{'C': 0.3693150939132843, 'n': 903, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.538, 'f1_pos': 0.489, 'f1_neu': 0.464, 'f1_neg': 0.615, 'eval_score': 0.547}\n",
      "{'C': 0.23872776219369496, 'n': 903, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.534, 'f1_pos': 0.469, 'f1_neu': 0.473, 'f1_neg': 0.605, 'eval_score': 0.536}\n",
      "{'C': 0.5200857592502984, 'n': 503, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.546, 'f1_pos': 0.494, 'f1_neu': 0.456, 'f1_neg': 0.633, 'eval_score': 0.558}\n",
      "{'C': 0.7674861655083285, 'n': 3, 'kernel': 'rbf'}                   \n",
      "{'accuracy': 0.407, 'f1_pos': 0.26, 'f1_neu': 0.409, 'f1_neg': 0.446, 'eval_score': 0.371}\n",
      "{'C': 0.22748891813962124, 'n': 503, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.533, 'f1_pos': 0.476, 'f1_neu': 0.454, 'f1_neg': 0.614, 'eval_score': 0.541}\n",
      "{'C': 0.7021057145299278, 'n': 403, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.545, 'f1_pos': 0.497, 'f1_neu': 0.45, 'f1_neg': 0.632, 'eval_score': 0.558}\n",
      "{'C': 0.006925990841927909, 'n': 403, 'kernel': 'rbf'}               \n",
      "{'accuracy': 0.396, 'f1_pos': 0.06, 'f1_neu': 0.092, 'f1_neg': 0.4, 'eval_score': 0.285}\n",
      "{'C': 0.10191097668549853, 'n': 703, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.541, 'f1_pos': 0.409, 'f1_neu': 0.442, 'f1_neg': 0.639, 'eval_score': 0.53}\n",
      "{'C': 0.0807848483634882, 'n': 203, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.509, 'f1_pos': 0.429, 'f1_neu': 0.442, 'f1_neg': 0.588, 'eval_score': 0.509}\n",
      "{'C': 0.364495184372708, 'n': 3, 'kernel': 'rbf'}                     \n",
      "{'accuracy': 0.429, 'f1_pos': 0.261, 'f1_neu': 0.372, 'f1_neg': 0.511, 'eval_score': 0.4}\n",
      "{'C': 0.09603150206545541, 'n': 603, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.533, 'f1_pos': 0.398, 'f1_neu': 0.431, 'f1_neg': 0.633, 'eval_score': 0.521}\n",
      "{'C': 0.6705199170182374, 'n': 703, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.545, 'f1_pos': 0.492, 'f1_neu': 0.448, 'f1_neg': 0.636, 'eval_score': 0.558}\n",
      "{'C': 0.6408608355658896, 'n': 803, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.546, 'f1_pos': 0.49, 'f1_neu': 0.453, 'f1_neg': 0.634, 'eval_score': 0.557}\n",
      "{'C': 0.03566177089067457, 'n': 103, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.486, 'f1_pos': 0.323, 'f1_neu': 0.43, 'f1_neg': 0.578, 'eval_score': 0.462}\n",
      "{'C': 0.2645282288703802, 'n': 403, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.537, 'f1_pos': 0.475, 'f1_neu': 0.464, 'f1_neg': 0.619, 'eval_score': 0.544}\n",
      "{'C': 0.8604572580260872, 'n': 303, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.543, 'f1_pos': 0.488, 'f1_neu': 0.45, 'f1_neg': 0.63, 'eval_score': 0.554}\n",
      "{'C': 0.9277483608741345, 'n': 903, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.549, 'f1_pos': 0.488, 'f1_neu': 0.45, 'f1_neg': 0.642, 'eval_score': 0.56}\n",
      "{'C': 0.5637704686917612, 'n': 303, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.536, 'f1_pos': 0.485, 'f1_neu': 0.446, 'f1_neg': 0.622, 'eval_score': 0.548}\n",
      "100%|██████████| 20/20 [43:07<00:00, 129.38s/trial, best loss: -0.563]\n",
      "{'C': 0.6152775265666071, 'n': 6}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and SVM\n",
    "\n",
    "def objective_func_SVM_PCA(args):\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "#     kernel = args['kernel']\n",
    "#     gamma = args['gamma']\n",
    "#     degree = args['degree']\n",
    "\n",
    "    par_dict = {'C': C, 'n': n, 'kernel': 'rbf'}\n",
    "    print(par_dict)\n",
    " \n",
    "    clf = SVC(C= C, kernel = 'rbf', class_weight='balanced')\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,1),\n",
    "         'n': hp.choice('n', np.arange(3, 1003, step =100))\n",
    "#         'kernel': hp.choice('kernel', ['poly', 'rbf']) \n",
    "#         'gamma': hp.choice('gamma',range(1,4)),\n",
    "#          'degree' : hp.choice('degree',range(1,4))\n",
    "        }\n",
    "                                \n",
    "                       \n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_SVM_PCA = fmin(objective_func_SVM_PCA, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_classifier_SVM_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 1, 'metric': 'manhattan'}             \n",
      "{'accuracy': 0.454, 'f1_pos': 0.368, 'f1_neu': 0.412, 'f1_neg': 0.53, 'eval_score': 0.451}\n",
      "{'n_neighbors': 28, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.545, 'f1_pos': 0.407, 'f1_neu': 0.329, 'f1_neg': 0.675, 'eval_score': 0.542}\n",
      "{'n_neighbors': 25, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.424, 'f1_pos': 0.385, 'f1_neu': 0.357, 'f1_neg': 0.49, 'eval_score': 0.433}\n",
      "{'n_neighbors': 28, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.44, 'f1_pos': 0.392, 'f1_neu': 0.324, 'f1_neg': 0.519, 'eval_score': 0.45}\n",
      "{'n_neighbors': 10, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.509, 'f1_pos': 0.414, 'f1_neu': 0.381, 'f1_neg': 0.62, 'eval_score': 0.514}\n",
      "{'n_neighbors': 28, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.542, 'f1_pos': 0.41, 'f1_neu': 0.315, 'f1_neg': 0.674, 'eval_score': 0.542}\n",
      "{'n_neighbors': 22, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.42, 'f1_pos': 0.393, 'f1_neu': 0.356, 'f1_neg': 0.474, 'eval_score': 0.429}\n",
      "{'n_neighbors': 22, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.539, 'f1_pos': 0.407, 'f1_neu': 0.34, 'f1_neg': 0.669, 'eval_score': 0.538}\n",
      "{'n_neighbors': 13, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.381, 'f1_pos': 0.378, 'f1_neu': 0.376, 'f1_neg': 0.385, 'eval_score': 0.381}\n",
      "{'n_neighbors': 4, 'metric': 'manhattan'}                            \n",
      "{'accuracy': 0.412, 'f1_pos': 0.369, 'f1_neu': 0.408, 'f1_neg': 0.448, 'eval_score': 0.41}\n",
      "100%|██████████| 10/10 [20:37<00:00, 123.77s/trial, best loss: -0.542]\n",
      "{'metric': 0, 'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "# WIth Bayesian Optimization and KNN\n",
    "def objective_func_KNN(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']  \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,31, step =3)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"])\n",
    "        }\n",
    "\n",
    "best_classifier_KNN = fmin(objective_func_KNN, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 7, 'n': 13, 'metric': 'euclidean'}    \n",
      "{'accuracy': 0.488, 'f1_pos': 0.297, 'f1_neu': 0.359, 'f1_neg': 0.614, 'eval_score': 0.466}\n",
      "{'n_neighbors': 5, 'n': 33, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.488, 'f1_pos': 0.354, 'f1_neu': 0.379, 'f1_neg': 0.602, 'eval_score': 0.481}\n",
      "{'n_neighbors': 6, 'n': 38, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.499, 'f1_pos': 0.341, 'f1_neu': 0.39, 'f1_neg': 0.616, 'eval_score': 0.485}\n",
      "{'n_neighbors': 1, 'n': 23, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.482, 'f1_pos': 0.345, 'f1_neu': 0.369, 'f1_neg': 0.598, 'eval_score': 0.475}\n",
      "{'n_neighbors': 4, 'n': 48, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.49, 'f1_pos': 0.371, 'f1_neu': 0.41, 'f1_neg': 0.589, 'eval_score': 0.483}\n",
      "{'n_neighbors': 2, 'n': 8, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.384, 'f1_pos': 0.332, 'f1_neu': 0.379, 'f1_neg': 0.422, 'eval_score': 0.379}\n",
      "{'n_neighbors': 3, 'n': 3, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.428, 'f1_pos': 0.272, 'f1_neu': 0.309, 'f1_neg': 0.558, 'eval_score': 0.419}\n",
      "{'n_neighbors': 7, 'n': 23, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.509, 'f1_pos': 0.338, 'f1_neu': 0.377, 'f1_neg': 0.634, 'eval_score': 0.494}\n",
      "{'n_neighbors': 3, 'n': 13, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.472, 'f1_pos': 0.342, 'f1_neu': 0.331, 'f1_neg': 0.601, 'eval_score': 0.472}\n",
      "{'n_neighbors': 5, 'n': 28, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.484, 'f1_pos': 0.353, 'f1_neu': 0.364, 'f1_neg': 0.603, 'eval_score': 0.48}\n",
      "{'n_neighbors': 7, 'n': 13, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.489, 'f1_pos': 0.304, 'f1_neu': 0.355, 'f1_neg': 0.615, 'eval_score': 0.469}\n",
      "{'n_neighbors': 3, 'n': 28, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.476, 'f1_pos': 0.339, 'f1_neu': 0.34, 'f1_neg': 0.605, 'eval_score': 0.473}\n",
      "{'n_neighbors': 1, 'n': 38, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.479, 'f1_pos': 0.34, 'f1_neu': 0.37, 'f1_neg': 0.596, 'eval_score': 0.472}\n",
      "{'n_neighbors': 8, 'n': 53, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.51, 'f1_pos': 0.36, 'f1_neu': 0.404, 'f1_neg': 0.621, 'eval_score': 0.497}\n",
      "{'n_neighbors': 8, 'n': 38, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.503, 'f1_pos': 0.356, 'f1_neu': 0.376, 'f1_neg': 0.621, 'eval_score': 0.493}\n",
      "100%|██████████| 15/15 [01:40<00:00,  6.72s/trial, best loss: -0.497]\n",
      "{'metric': 0, 'n': 10, 'n_neighbors': 7}\n"
     ]
    }
   ],
   "source": [
    "# WIth PCA , Bayesian Optimization and KNN\n",
    "def objective_func_KNN_PCA(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']\n",
    "    n = args['n']    \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'n': n, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,10, step =1)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"]),\n",
    "        'n': hp.choice('n', np.arange(3,63, step =5))}\n",
    "\n",
    "best_classifier_KNN_PCA = fmin(objective_func_KNN_PCA, space, algo=tpe.suggest, max_evals=15)\n",
    "print(best_classifier_KNN_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-z9cSEeUzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.395, 'f1_pos': 0.364, 'f1_neu': 0.361, 'f1_neg': 0.444, 'eval_score': 0.401}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 35}                                             \n",
      "{'accuracy': 0.44, 'f1_pos': 0.311, 'f1_neu': 0.422, 'f1_neg': 0.503, 'eval_score': 0.418}\n",
      "{'n': 56}                                                           \n",
      "{'accuracy': 0.432, 'f1_pos': 0.328, 'f1_neu': 0.45, 'f1_neg': 0.455, 'eval_score': 0.405}\n",
      "{'n': 49}                                                           \n",
      "{'accuracy': 0.433, 'f1_pos': 0.339, 'f1_neu': 0.447, 'f1_neg': 0.459, 'eval_score': 0.41}\n",
      "{'n': 28}                                                           \n",
      "{'accuracy': 0.457, 'f1_pos': 0.306, 'f1_neu': 0.392, 'f1_neg': 0.547, 'eval_score': 0.437}\n",
      "{'n': 62}                                                           \n",
      "{'accuracy': 0.43, 'f1_pos': 0.322, 'f1_neu': 0.448, 'f1_neg': 0.454, 'eval_score': 0.402}\n",
      "{'n': 3}                                                            \n",
      "{'accuracy': 0.498, 'f1_pos': 0.097, 'f1_neu': 0.156, 'f1_neg': 0.656, 'eval_score': 0.417}\n",
      "{'n': 46}                                                           \n",
      "{'accuracy': 0.442, 'f1_pos': 0.367, 'f1_neu': 0.442, 'f1_neg': 0.47, 'eval_score': 0.426}\n",
      "{'n': 38}                                                           \n",
      "{'accuracy': 0.432, 'f1_pos': 0.3, 'f1_neu': 0.422, 'f1_neg': 0.492, 'eval_score': 0.408}\n",
      "{'n': 42}                                                           \n",
      "{'accuracy': 0.442, 'f1_pos': 0.338, 'f1_neu': 0.451, 'f1_neg': 0.476, 'eval_score': 0.419}\n",
      "{'n': 3}                                                            \n",
      "{'accuracy': 0.495, 'f1_pos': 0.119, 'f1_neu': 0.151, 'f1_neg': 0.652, 'eval_score': 0.422}\n",
      "100%|██████████| 10/10 [01:06<00:00,  6.64s/trial, best loss: -0.437]\n"
     ]
    }
   ],
   "source": [
    "# With PCA and Naive bayes\n",
    "def objective_func_NB_PCA(args):\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = {'n': n}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n': hp.choice('n', np.arange(3,63, step =1))}\n",
    "\n",
    "best_classifier_NB_PCA = fmin(objective_func_NB_PCA, space, algo=tpe.suggest, max_evals=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uqUrTlhaCiX",
    "outputId": "46220e5f-4754-4177-f6df-fc32cf0fa883"
   },
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnu2mBLwaCie"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "r-6Q8dRwaCig",
    "outputId": "b42504a9-9d98-49fd-e286-297bb9fcd020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 36, 'max_depth': 78, 'min_samples_split': 8, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.194, 'f1_neu': 0.158, 'f1_neg': 0.692, 'eval_score': 0.476}\n",
      "{'n_estimators': 53, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.075, 'f1_neu': 0.051, 'f1_neg': 0.683, 'eval_score': 0.428}\n",
      "{'n_estimators': 54, 'max_depth': 71, 'min_samples_split': 4, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.247, 'f1_neu': 0.195, 'f1_neg': 0.697, 'eval_score': 0.499}\n",
      "{'n_estimators': 81, 'max_depth': 61, 'min_samples_split': 5, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.539, 'f1_pos': 0.173, 'f1_neu': 0.133, 'f1_neg': 0.689, 'eval_score': 0.467}\n",
      "{'n_estimators': 91, 'max_depth': 38, 'min_samples_split': 11, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.528, 'f1_pos': 0.118, 'f1_neu': 0.043, 'f1_neg': 0.685, 'eval_score': 0.444}\n",
      "{'n_estimators': 55, 'max_depth': 30, 'min_samples_split': 2, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.129, 'f1_neu': 0.065, 'f1_neg': 0.687, 'eval_score': 0.45}\n",
      "{'n_estimators': 75, 'max_depth': 51, 'min_samples_split': 5, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.172, 'f1_neu': 0.119, 'f1_neg': 0.691, 'eval_score': 0.468}\n",
      "{'n_estimators': 40, 'max_depth': 44, 'min_samples_split': 7, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.102, 'f1_neu': 0.049, 'f1_neg': 0.684, 'eval_score': 0.438}\n",
      "{'n_estimators': 72, 'max_depth': 91, 'min_samples_split': 9, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.242, 'f1_neu': 0.206, 'f1_neg': 0.697, 'eval_score': 0.498}\n",
      "{'n_estimators': 50, 'max_depth': 37, 'min_samples_split': 6, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.103, 'f1_neu': 0.059, 'f1_neg': 0.685, 'eval_score': 0.439}\n",
      "{'n_estimators': 82, 'max_depth': 21, 'min_samples_split': 6, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.519, 'f1_pos': 0.05, 'f1_neu': 0.022, 'f1_neg': 0.681, 'eval_score': 0.417}\n",
      "{'n_estimators': 38, 'max_depth': 49, 'min_samples_split': 5, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.531, 'f1_pos': 0.133, 'f1_neu': 0.063, 'f1_neg': 0.686, 'eval_score': 0.45}\n",
      "{'n_estimators': 91, 'max_depth': 78, 'min_samples_split': 5, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.278, 'f1_neu': 0.253, 'f1_neg': 0.704, 'eval_score': 0.515}\n",
      "{'n_estimators': 30, 'max_depth': 94, 'min_samples_split': 8, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.227, 'f1_neu': 0.222, 'f1_neg': 0.695, 'eval_score': 0.492}\n",
      "{'n_estimators': 85, 'max_depth': 58, 'min_samples_split': 8, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.231, 'f1_neu': 0.205, 'f1_neg': 0.697, 'eval_score': 0.494}\n",
      "{'n_estimators': 39, 'max_depth': 90, 'min_samples_split': 9, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.256, 'f1_neu': 0.252, 'f1_neg': 0.7, 'eval_score': 0.506}\n",
      "{'n_estimators': 97, 'max_depth': 90, 'min_samples_split': 10, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.09, 'f1_neu': 0.046, 'f1_neg': 0.684, 'eval_score': 0.433}\n",
      "{'n_estimators': 83, 'max_depth': 88, 'min_samples_split': 7, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.097, 'f1_neu': 0.048, 'f1_neg': 0.684, 'eval_score': 0.436}\n",
      "{'n_estimators': 45, 'max_depth': 83, 'min_samples_split': 8, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.173, 'f1_neu': 0.138, 'f1_neg': 0.693, 'eval_score': 0.47}\n",
      "{'n_estimators': 51, 'max_depth': 99, 'min_samples_split': 11, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.232, 'f1_neu': 0.214, 'f1_neg': 0.698, 'eval_score': 0.495}\n",
      "{'n_estimators': 61, 'max_depth': 78, 'min_samples_split': 9, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.248, 'f1_neu': 0.217, 'f1_neg': 0.7, 'eval_score': 0.502}\n",
      "{'n_estimators': 91, 'max_depth': 23, 'min_samples_split': 9, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.072, 'f1_neu': 0.041, 'f1_neg': 0.684, 'eval_score': 0.427}\n",
      "{'n_estimators': 39, 'max_depth': 90, 'min_samples_split': 10, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.565, 'f1_pos': 0.261, 'f1_neu': 0.257, 'f1_neg': 0.703, 'eval_score': 0.51}\n",
      "{'n_estimators': 56, 'max_depth': 50, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.209, 'f1_neu': 0.178, 'f1_neg': 0.695, 'eval_score': 0.485}\n",
      "{'n_estimators': 96, 'max_depth': 66, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.556, 'f1_pos': 0.239, 'f1_neu': 0.227, 'f1_neg': 0.698, 'eval_score': 0.498}\n",
      "100%|██████████| 25/25 [07:25<00:00, 17.82s/trial, best loss: -0.515]\n",
      "{'max_depth': 58, 'min_samples_leaf': 0, 'min_samples_split': 3, 'n_estimators': 71}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "\n",
    "    par_dict = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF = fmin(objective_func_RF, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "AI0VHzOleUzi",
    "outputId": "de0a927c-95c9-439d-d3ff-06cd57c0aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 123, 'n_estimators': 54, 'max_depth': 91, 'min_samples_split': 9, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.292, 'f1_neu': 0.308, 'f1_neg': 0.698, 'eval_score': 0.518}\n",
      "{'n': 123, 'n_estimators': 30, 'max_depth': 87, 'min_samples_split': 7, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.25, 'f1_neu': 0.294, 'f1_neg': 0.695, 'eval_score': 0.5}\n",
      "{'n': 183, 'n_estimators': 36, 'max_depth': 93, 'min_samples_split': 11, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.21, 'f1_neu': 0.252, 'f1_neg': 0.699, 'eval_score': 0.489}\n",
      "{'n': 143, 'n_estimators': 42, 'max_depth': 33, 'min_samples_split': 8, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.239, 'f1_neu': 0.247, 'f1_neg': 0.694, 'eval_score': 0.495}\n",
      "{'n': 63, 'n_estimators': 85, 'max_depth': 99, 'min_samples_split': 4, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.262, 'f1_neu': 0.301, 'f1_neg': 0.699, 'eval_score': 0.508}\n",
      "{'n': 123, 'n_estimators': 74, 'max_depth': 58, 'min_samples_split': 3, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.274, 'f1_neu': 0.28, 'f1_neg': 0.697, 'eval_score': 0.511}\n",
      "{'n': 123, 'n_estimators': 54, 'max_depth': 39, 'min_samples_split': 10, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.56, 'f1_pos': 0.28, 'f1_neu': 0.294, 'f1_neg': 0.698, 'eval_score': 0.513}\n",
      "{'n': 123, 'n_estimators': 45, 'max_depth': 94, 'min_samples_split': 11, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.235, 'f1_neu': 0.256, 'f1_neg': 0.694, 'eval_score': 0.494}\n",
      "{'n': 23, 'n_estimators': 92, 'max_depth': 35, 'min_samples_split': 10, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.291, 'f1_neu': 0.33, 'f1_neg': 0.696, 'eval_score': 0.516}\n",
      "{'n': 183, 'n_estimators': 34, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.228, 'f1_neu': 0.239, 'f1_neg': 0.696, 'eval_score': 0.492}\n",
      "{'n': 123, 'n_estimators': 61, 'max_depth': 44, 'min_samples_split': 9, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.566, 'f1_pos': 0.288, 'f1_neu': 0.321, 'f1_neg': 0.699, 'eval_score': 0.518}\n",
      "{'n': 43, 'n_estimators': 87, 'max_depth': 31, 'min_samples_split': 10, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.565, 'f1_pos': 0.301, 'f1_neu': 0.323, 'f1_neg': 0.698, 'eval_score': 0.521}\n",
      "{'n': 103, 'n_estimators': 23, 'max_depth': 73, 'min_samples_split': 2, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.261, 'f1_neu': 0.302, 'f1_neg': 0.689, 'eval_score': 0.501}\n",
      "{'n': 43, 'n_estimators': 95, 'max_depth': 65, 'min_samples_split': 6, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.292, 'f1_neu': 0.313, 'f1_neg': 0.7, 'eval_score': 0.519}\n",
      "{'n': 43, 'n_estimators': 28, 'max_depth': 68, 'min_samples_split': 8, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.269, 'f1_neu': 0.297, 'f1_neg': 0.69, 'eval_score': 0.504}\n",
      "{'n': 23, 'n_estimators': 37, 'max_depth': 88, 'min_samples_split': 10, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.542, 'f1_pos': 0.252, 'f1_neu': 0.298, 'f1_neg': 0.685, 'eval_score': 0.493}\n",
      "{'n': 163, 'n_estimators': 75, 'max_depth': 95, 'min_samples_split': 8, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.197, 'f1_neu': 0.21, 'f1_neg': 0.695, 'eval_score': 0.481}\n",
      "{'n': 163, 'n_estimators': 25, 'max_depth': 37, 'min_samples_split': 2, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.238, 'f1_neu': 0.285, 'f1_neg': 0.69, 'eval_score': 0.493}\n",
      "{'n': 143, 'n_estimators': 50, 'max_depth': 80, 'min_samples_split': 5, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.266, 'f1_neu': 0.27, 'f1_neg': 0.696, 'eval_score': 0.506}\n",
      "{'n': 163, 'n_estimators': 63, 'max_depth': 33, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.269, 'f1_neu': 0.29, 'f1_neg': 0.7, 'eval_score': 0.51}\n",
      "{'n': 43, 'n_estimators': 87, 'max_depth': 65, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.307, 'f1_neu': 0.314, 'f1_neg': 0.692, 'eval_score': 0.519}\n",
      "{'n': 83, 'n_estimators': 87, 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.288, 'f1_neu': 0.312, 'f1_neg': 0.697, 'eval_score': 0.516}\n",
      "{'n': 3, 'n_estimators': 95, 'max_depth': 31, 'min_samples_split': 6, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.511, 'f1_pos': 0.161, 'f1_neu': 0.323, 'f1_neg': 0.653, 'eval_score': 0.442}\n",
      "{'n': 43, 'n_estimators': 87, 'max_depth': 65, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.333, 'f1_neu': 0.32, 'f1_neg': 0.697, 'eval_score': 0.531}\n",
      "{'n': 43, 'n_estimators': 87, 'max_depth': 83, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.319, 'f1_neu': 0.33, 'f1_neg': 0.692, 'eval_score': 0.525}\n",
      "{'n': 43, 'n_estimators': 59, 'max_depth': 96, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.285, 'f1_neu': 0.329, 'f1_neg': 0.698, 'eval_score': 0.515}\n",
      "{'n': 43, 'n_estimators': 88, 'max_depth': 83, 'min_samples_split': 7, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.271, 'f1_neu': 0.267, 'f1_neg': 0.697, 'eval_score': 0.509}\n",
      "{'n': 83, 'n_estimators': 67, 'max_depth': 34, 'min_samples_split': 7, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.278, 'f1_neu': 0.289, 'f1_neg': 0.696, 'eval_score': 0.511}\n",
      "{'n': 103, 'n_estimators': 20, 'max_depth': 92, 'min_samples_split': 7, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.295, 'f1_neu': 0.33, 'f1_neg': 0.685, 'eval_score': 0.51}\n",
      "{'n': 63, 'n_estimators': 100, 'max_depth': 75, 'min_samples_split': 4, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.256, 'f1_neu': 0.276, 'f1_neg': 0.697, 'eval_score': 0.504}\n",
      "100%|██████████| 30/30 [05:48<00:00, 11.63s/trial, best loss: -0.531]\n",
      "{'max_depth': 45, 'min_samples_leaf': 1, 'min_samples_split': 5, 'n': 2, 'n_estimators': 67}\n"
     ]
    }
   ],
   "source": [
    "# With PCA,  Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF_PCA(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = { 'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF_PCA = fmin(objective_func_RF_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_RF_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKP0NEj6eUzk"
   },
   "outputs": [],
   "source": [
    "# # WITH LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from math import sqrt\n",
    "# lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "# x_lda = lda.fit_transform(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQsPlUENeUzl",
    "outputId": "7a8a84e7-88c0-4bab-bf29-611d0cc98d98"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "#     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda, temp, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzCfxZ1GeUzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Y5ahpseUzp"
   },
   "outputs": [],
   "source": [
    "# obama_train2_pr_tfidf, obama_val_pr_tfidf, obama_train2_pr_class, obama_val_pr_class = train_test_split(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SUBRUJ8eUzq"
   },
   "outputs": [],
   "source": [
    "# x_lda2 = lda.fit_transform(obama_train2_pr_tfidf, obama_train2_pr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cz5e2KzeUzs",
    "outputId": "9d0f0738-440f-4c25-dcf2-79fc3f0dce22"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "# #     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda2, obama_train2_pr_class, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm0j2MPOeUzt"
   },
   "outputs": [],
   "source": [
    "# x_val_lda = lda.transform(obama_val_pr_tfidf)\n",
    "# x_val_lda_df = pd.DataFrame(data = x_val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkTc0libeUzy",
    "outputId": "e52d6004-e6f8-46bd-863b-90bb91bba6be"
   },
   "outputs": [],
   "source": [
    "# c = lambda b : 1 if b == 0 else b\n",
    "# d = lambda b : 2 if b <= 1 else b\n",
    "# bc = RandomForestClassifier(max_depth = c(best_classifier['max_depth']), min_samples_leaf = c(best_classifier['min_samples_leaf']), min_samples_split = d(best_classifier['min_samples_split'])\n",
    "#                             ,n_estimators = best_classifier['n_estimators'])\n",
    "\n",
    "# bc.fit(x_lda2, obama_train2_pr_class)\n",
    "\n",
    "# y_pred = bc.predict(x_val_lda)\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDPpRJ91eUz0",
    "outputId": "7338ec62-8947-4d33-a1ef-f2e2eee8843d"
   },
   "outputs": [],
   "source": [
    "# lda_clf = lda.fit(obama_train2_pr_tfidf, obama_train2_pr_class)\n",
    "# y_pred_lda_clf = lda_clf.predict(obama_val_pr_tfidf)\n",
    "# # y_pred_lda_clf\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred_lda_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBsD7WYFeUz4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "0UB056v0eUz6"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'gbdt', 'n_estimators': 345, 'max_depth': 6, 'learning_rate': 0.010871082478688177, 'subsample': 0.8666666666666667, 'min_split_gain': 0}\n",
      "{'accuracy': 0.491, 'f1_pos': 0.414, 'f1_neu': 0.466, 'f1_neg': 0.549, 'eval_score': 0.485}\n",
      "{'boosting_type': 'dart', 'n_estimators': 116, 'max_depth': 5, 'learning_rate': 0.117138490340471, 'subsample': 0.8666666666666667, 'min_split_gain': 5}\n",
      "{'accuracy': 0.478, 'f1_pos': 0.411, 'f1_neu': 0.457, 'f1_neg': 0.53, 'eval_score': 0.473}\n",
      "{'boosting_type': 'goss', 'n_estimators': 315, 'max_depth': 9, 'learning_rate': 0.07149395952029858, 'subsample': 0.9333333333333333, 'min_split_gain': 4}\n",
      "{'accuracy': 0.498, 'f1_pos': 0.426, 'f1_neu': 0.41, 'f1_neg': 0.591, 'eval_score': 0.505}\n",
      "{'boosting_type': 'dart', 'n_estimators': 256, 'max_depth': 9, 'learning_rate': 0.04951899427471998, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.503, 'f1_pos': 0.427, 'f1_neu': 0.462, 'f1_neg': 0.572, 'eval_score': 0.501}\n",
      "{'boosting_type': 'goss', 'n_estimators': 366, 'max_depth': 5, 'learning_rate': 0.14561175775246685, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.484, 'f1_pos': 0.411, 'f1_neu': 0.406, 'f1_neg': 0.574, 'eval_score': 0.49}\n",
      "{'boosting_type': 'dart', 'n_estimators': 483, 'max_depth': 3, 'learning_rate': 0.18353193261178585, 'subsample': 0.8, 'min_split_gain': 3}\n",
      "{'accuracy': 0.504, 'f1_pos': 0.448, 'f1_neu': 0.464, 'f1_neg': 0.563, 'eval_score': 0.505}\n",
      "{'boosting_type': 'dart', 'n_estimators': 259, 'max_depth': 9, 'learning_rate': 0.04279713362682372, 'subsample': 0.8666666666666667, 'min_split_gain': 4}\n",
      "{'accuracy': 0.489, 'f1_pos': 0.425, 'f1_neu': 0.465, 'f1_neg': 0.544, 'eval_score': 0.486}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 213, 'max_depth': 4, 'learning_rate': 0.024679310536330948, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.49, 'f1_pos': 0.418, 'f1_neu': 0.466, 'f1_neg': 0.544, 'eval_score': 0.484}\n",
      "{'boosting_type': 'dart', 'n_estimators': 284, 'max_depth': 8, 'learning_rate': 0.08187123629604952, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.44, 'f1_neu': 0.458, 'f1_neg': 0.596, 'eval_score': 0.518}\n",
      "{'boosting_type': 'dart', 'n_estimators': 145, 'max_depth': 6, 'learning_rate': 0.17024392835369084, 'subsample': 0.8, 'min_split_gain': 3}\n",
      "{'accuracy': 0.5, 'f1_pos': 0.447, 'f1_neu': 0.459, 'f1_neg': 0.561, 'eval_score': 0.503}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 340, 'max_depth': 8, 'learning_rate': 0.05771729270521822, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.512, 'f1_pos': 0.441, 'f1_neu': 0.465, 'f1_neg': 0.583, 'eval_score': 0.512}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 145, 'max_depth': 8, 'learning_rate': 0.18032894280329664, 'subsample': 1.0, 'min_split_gain': 2}\n",
      "{'accuracy': 0.511, 'f1_pos': 0.44, 'f1_neu': 0.46, 'f1_neg': 0.584, 'eval_score': 0.512}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 224, 'max_depth': 9, 'learning_rate': 0.1773210433999423, 'subsample': 0.8666666666666667, 'min_split_gain': 4}\n",
      "{'accuracy': 0.5, 'f1_pos': 0.446, 'f1_neu': 0.46, 'f1_neg': 0.562, 'eval_score': 0.503}\n",
      "{'boosting_type': 'dart', 'n_estimators': 177, 'max_depth': 6, 'learning_rate': 0.07296333446856129, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.495, 'f1_pos': 0.422, 'f1_neu': 0.469, 'f1_neg': 0.552, 'eval_score': 0.49}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.10308028348062018, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.431, 'f1_neu': 0.435, 'f1_neg': 0.612, 'eval_score': 0.52}\n",
      "{'boosting_type': 'goss', 'n_estimators': 334, 'max_depth': 9, 'learning_rate': 0.1950213986229712, 'subsample': 0.8666666666666667, 'min_split_gain': 5}\n",
      "{'accuracy': 0.494, 'f1_pos': 0.399, 'f1_neu': 0.412, 'f1_neg': 0.592, 'eval_score': 0.495}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 491, 'max_depth': 7, 'learning_rate': 0.006844179527253736, 'subsample': 1.0, 'min_split_gain': 5}\n",
      "{'accuracy': 0.482, 'f1_pos': 0.414, 'f1_neu': 0.464, 'f1_neg': 0.531, 'eval_score': 0.476}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.18420202327834415, 'subsample': 0.8, 'min_split_gain': 4}\n",
      "{'accuracy': 0.494, 'f1_pos': 0.423, 'f1_neu': 0.464, 'f1_neg': 0.552, 'eval_score': 0.49}\n",
      "{'boosting_type': 'goss', 'n_estimators': 350, 'max_depth': 4, 'learning_rate': 0.18449501967516307, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.479, 'f1_pos': 0.398, 'f1_neu': 0.41, 'f1_neg': 0.565, 'eval_score': 0.481}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 149, 'max_depth': 9, 'learning_rate': 0.14865031089608374, 'subsample': 0.9333333333333333, 'min_split_gain': 3}\n",
      "{'accuracy': 0.497, 'f1_pos': 0.423, 'f1_neu': 0.458, 'f1_neg': 0.565, 'eval_score': 0.495}\n",
      "{'boosting_type': 'dart', 'n_estimators': 150, 'max_depth': 8, 'learning_rate': 0.10984173933839171, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.52, 'f1_pos': 0.445, 'f1_neu': 0.469, 'f1_neg': 0.595, 'eval_score': 0.52}\n",
      "{'boosting_type': 'dart', 'n_estimators': 500, 'max_depth': 6, 'learning_rate': 0.109176087262601, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.518, 'f1_pos': 0.442, 'f1_neu': 0.456, 'f1_neg': 0.599, 'eval_score': 0.52}\n",
      "{'boosting_type': 'dart', 'n_estimators': 426, 'max_depth': 6, 'learning_rate': 0.12793471449338353, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.514, 'f1_pos': 0.439, 'f1_neu': 0.445, 'f1_neg': 0.6, 'eval_score': 0.518}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 150, 'max_depth': 8, 'learning_rate': 0.09997180587380002, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.521, 'f1_pos': 0.452, 'f1_neu': 0.458, 'f1_neg': 0.601, 'eval_score': 0.525}\n",
      "{'boosting_type': 'goss', 'n_estimators': 150, 'max_depth': 7, 'learning_rate': 0.08838927400622217, 'subsample': 1.0, 'min_split_gain': 1}\n",
      "{'accuracy': 0.489, 'f1_pos': 0.41, 'f1_neu': 0.396, 'f1_neg': 0.587, 'eval_score': 0.495}\n",
      "100%|██████████| 25/25 [02:26<00:00,  5.86s/trial, best loss: -0.525]\n",
      "{'boosting_type': 0, 'learning_rate': 0.09997180587380002, 'max_depth': 5, 'min_split_gain': 1, 'n_estimators': 100, 'subsample': 3}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "    subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "\n",
    "    par_dict = {'boosting_type': boosting_type, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': subsample, 'min_split_gain': min_split_gain}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=subsample, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(romney_train_pr_tfidf,romney_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,501, step =1)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "     'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB = fmin(objective_func_LGB, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 43, 'n_estimators': 212, 'max_depth': 6, 'learning_rate': 0.01800028418964581, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.497, 'f1_pos': 0.421, 'f1_neu': 0.429, 'f1_neg': 0.58, 'eval_score': 0.499}\n",
      "{'n': 163, 'n_estimators': 341, 'max_depth': 6, 'learning_rate': 0.05778800188029725, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.463, 'f1_neu': 0.446, 'f1_neg': 0.615, 'eval_score': 0.536}\n",
      "{'n': 83, 'n_estimators': 438, 'max_depth': 5, 'learning_rate': 0.09849562123511453, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.392, 'f1_neu': 0.404, 'f1_neg': 0.652, 'eval_score': 0.526}\n",
      "{'n': 163, 'n_estimators': 203, 'max_depth': 9, 'learning_rate': 0.09458342802423055, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.423, 'f1_neu': 0.406, 'f1_neg': 0.65, 'eval_score': 0.537}\n",
      "{'n': 183, 'n_estimators': 321, 'max_depth': 6, 'learning_rate': 0.05716678656047405, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.554, 'f1_pos': 0.433, 'f1_neu': 0.449, 'f1_neg': 0.658, 'eval_score': 0.548}\n",
      "{'n': 143, 'n_estimators': 480, 'max_depth': 9, 'learning_rate': 0.1921659752851809, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.4, 'f1_neu': 0.399, 'f1_neg': 0.638, 'eval_score': 0.521}\n",
      "{'n': 163, 'n_estimators': 248, 'max_depth': 3, 'learning_rate': 0.13176225573531217, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.435, 'f1_neu': 0.435, 'f1_neg': 0.588, 'eval_score': 0.51}\n",
      "{'n': 23, 'n_estimators': 292, 'max_depth': 9, 'learning_rate': 0.09209977020820681, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.484, 'f1_pos': 0.396, 'f1_neu': 0.414, 'f1_neg': 0.572, 'eval_score': 0.484}\n",
      "{'n': 63, 'n_estimators': 327, 'max_depth': 3, 'learning_rate': 0.1284432718262811, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.51, 'f1_pos': 0.408, 'f1_neu': 0.423, 'f1_neg': 0.608, 'eval_score': 0.509}\n",
      "{'n': 83, 'n_estimators': 134, 'max_depth': 4, 'learning_rate': 0.02030050770057872, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.495, 'f1_pos': 0.433, 'f1_neu': 0.435, 'f1_neg': 0.57, 'eval_score': 0.499}\n",
      "{'n': 103, 'n_estimators': 61, 'max_depth': 8, 'learning_rate': 0.15002573194405713, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.408, 'f1_neu': 0.417, 'f1_neg': 0.621, 'eval_score': 0.515}\n",
      "{'n': 43, 'n_estimators': 174, 'max_depth': 3, 'learning_rate': 0.16443039630681489, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.502, 'f1_pos': 0.415, 'f1_neu': 0.414, 'f1_neg': 0.598, 'eval_score': 0.505}\n",
      "{'n': 183, 'n_estimators': 409, 'max_depth': 3, 'learning_rate': 0.015805737501114293, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.505, 'f1_pos': 0.425, 'f1_neu': 0.439, 'f1_neg': 0.587, 'eval_score': 0.506}\n",
      "{'n': 63, 'n_estimators': 258, 'max_depth': 3, 'learning_rate': 0.06963166383355589, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.504, 'f1_pos': 0.411, 'f1_neu': 0.424, 'f1_neg': 0.6, 'eval_score': 0.505}\n",
      "{'n': 23, 'n_estimators': 198, 'max_depth': 9, 'learning_rate': 0.03857252021506366, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.472, 'f1_pos': 0.388, 'f1_neu': 0.399, 'f1_neg': 0.561, 'eval_score': 0.474}\n",
      "{'n': 23, 'n_estimators': 309, 'max_depth': 3, 'learning_rate': 0.15670942344955513, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.475, 'f1_pos': 0.396, 'f1_neu': 0.399, 'f1_neg': 0.561, 'eval_score': 0.477}\n",
      "{'n': 43, 'n_estimators': 304, 'max_depth': 7, 'learning_rate': 0.09558190768917085, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.396, 'f1_neu': 0.398, 'f1_neg': 0.641, 'eval_score': 0.521}\n",
      "{'n': 143, 'n_estimators': 202, 'max_depth': 9, 'learning_rate': 0.12180319701528436, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.565, 'f1_pos': 0.394, 'f1_neu': 0.399, 'f1_neg': 0.689, 'eval_score': 0.549}\n",
      "{'n': 103, 'n_estimators': 361, 'max_depth': 5, 'learning_rate': 0.12928959063839543, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.44, 'f1_neu': 0.448, 'f1_neg': 0.598, 'eval_score': 0.518}\n",
      "{'n': 43, 'n_estimators': 120, 'max_depth': 8, 'learning_rate': 0.17167065676840693, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.479, 'f1_pos': 0.42, 'f1_neu': 0.407, 'f1_neg': 0.558, 'eval_score': 0.486}\n",
      "{'n': 183, 'n_estimators': 295, 'max_depth': 6, 'learning_rate': 0.06705291996696008, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.532, 'f1_pos': 0.431, 'f1_neu': 0.436, 'f1_neg': 0.633, 'eval_score': 0.532}\n",
      "{'n': 143, 'n_estimators': 331, 'max_depth': 6, 'learning_rate': 0.0408996140771562, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.441, 'f1_neu': 0.433, 'f1_neg': 0.648, 'eval_score': 0.545}\n",
      "{'n': 123, 'n_estimators': 250, 'max_depth': 7, 'learning_rate': 0.11247544851540964, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.397, 'f1_neu': 0.437, 'f1_neg': 0.669, 'eval_score': 0.54}\n",
      "{'n': 3, 'n_estimators': 321, 'max_depth': 4, 'learning_rate': 0.07274590466392873, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.391, 'f1_pos': 0.297, 'f1_neu': 0.363, 'f1_neg': 0.452, 'eval_score': 0.38}\n",
      "{'n': 143, 'n_estimators': 223, 'max_depth': 6, 'learning_rate': 0.0024138873629280036, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.496, 'f1_pos': 0.389, 'f1_neu': 0.445, 'f1_neg': 0.578, 'eval_score': 0.488}\n",
      "{'n': 183, 'n_estimators': 351, 'max_depth': 9, 'learning_rate': 0.19912707395707735, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.537, 'f1_pos': 0.44, 'f1_neu': 0.439, 'f1_neg': 0.635, 'eval_score': 0.537}\n",
      "{'n': 123, 'n_estimators': 403, 'max_depth': 8, 'learning_rate': 0.0500124686771667, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.423, 'f1_neu': 0.437, 'f1_neg': 0.653, 'eval_score': 0.541}\n",
      "{'n': 143, 'n_estimators': 202, 'max_depth': 9, 'learning_rate': 0.08362969452086075, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.414, 'f1_neu': 0.418, 'f1_neg': 0.678, 'eval_score': 0.551}\n",
      "{'n': 143, 'n_estimators': 443, 'max_depth': 9, 'learning_rate': 0.11114706798363785, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.573, 'f1_pos': 0.393, 'f1_neu': 0.4, 'f1_neg': 0.698, 'eval_score': 0.555}\n",
      "{'n': 143, 'n_estimators': 263, 'max_depth': 9, 'learning_rate': 0.08038951974453604, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.57, 'f1_pos': 0.423, 'f1_neu': 0.417, 'f1_neg': 0.687, 'eval_score': 0.56}\n",
      "{'n': 3, 'n_estimators': 452, 'max_depth': 9, 'learning_rate': 0.1141250299090063, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.427, 'f1_pos': 0.305, 'f1_neu': 0.375, 'f1_neg': 0.519, 'eval_score': 0.417}\n",
      "{'n': 143, 'n_estimators': 455, 'max_depth': 5, 'learning_rate': 0.13820492733756704, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.557, 'f1_pos': 0.417, 'f1_neu': 0.433, 'f1_neg': 0.669, 'eval_score': 0.548}\n",
      "{'n': 143, 'n_estimators': 141, 'max_depth': 9, 'learning_rate': 0.10793705702096097, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.453, 'f1_neu': 0.445, 'f1_neg': 0.622, 'eval_score': 0.535}\n",
      "{'n': 83, 'n_estimators': 171, 'max_depth': 4, 'learning_rate': 0.18138351239682493, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.43, 'f1_neu': 0.442, 'f1_neg': 0.62, 'eval_score': 0.525}\n",
      "{'n': 143, 'n_estimators': 443, 'max_depth': 9, 'learning_rate': 0.0861684987291287, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.368, 'f1_neu': 0.385, 'f1_neg': 0.687, 'eval_score': 0.538}\n",
      "100%|██████████| 35/35 [16:10<00:00, 27.73s/trial, best loss: -0.56]\n",
      "{'boosting_type': 1, 'learning_rate': 0.08038951974453604, 'max_depth': 6, 'min_split_gain': 0, 'n': 7, 'n_estimators': 213}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB_PCA(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "#     subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "    n = args['n']\n",
    "    \n",
    "    par_dict = {'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': 1, 'min_split_gain': min_split_gain} \n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=1, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(romney_train_pr_tfidf, romney_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,501, step =1)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "#      'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB_PCA = fmin(objective_func_LGB_PCA, space, algo=tpe.suggest, max_evals=35)\n",
    "print(best_classifier_LGB_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "tt3WGISKa9_U"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-8066eedb3829>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0msm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSMOTE\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msampling_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'not majority'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mromney_train_pr_tfidf_smote\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mromney_train_pr_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_sample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mromney_train_pr_tfidf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mromney_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\base.py\u001b[0m in \u001b[0;36mfit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m     81\u001b[0m         )\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 83\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_resample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m         y_ = (label_binarize(output[1], np.unique(y))\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\imblearn\\over_sampling\\_smote.py\u001b[0m in \u001b[0;36m_fit_resample\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    731\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 732\u001b[1;33m             \u001b[0mnns\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn_k_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_distance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    733\u001b[0m             X_new, y_new = self._make_samples(\n\u001b[0;32m    734\u001b[0m                 \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_sample\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_class\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    660\u001b[0m                 \u001b[0mdelayed_query\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdelayed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_tree_query_parallel_helper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    661\u001b[0m                 \u001b[0mparallel_kwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"prefer\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"threads\"\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 662\u001b[1;33m             chunked_results = Parallel(n_jobs, **parallel_kwargs)(\n\u001b[0m\u001b[0;32m    663\u001b[0m                 delayed_query(\n\u001b[0;32m    664\u001b[0m                     self._tree, X[s], n_neighbors, return_distance)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1027\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1029\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    846\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 847\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    848\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    250\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    254\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36m_tree_query_parallel_helper\u001b[1;34m(tree, *args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m     \u001b[0munder\u001b[0m \u001b[0mPyPy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m     \"\"\"\n\u001b[1;32m--> 492\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "romney_train_pr_tfidf_smote, romney_train_pr_class = sm.fit_sample(romney_train_pr_tfidf, romney_train_pr_df['Sentiment'] )\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "obama_train_pr_tfidf_smote, obama_train_pr_class = sm.fit_sample(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
