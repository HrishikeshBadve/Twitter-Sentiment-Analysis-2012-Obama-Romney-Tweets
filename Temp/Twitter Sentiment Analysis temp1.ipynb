{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "78PhOL0jpZdY",
    "outputId": "06723e26-5714-413f-a8e9-c508d8b48959"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "47441879-b8f7-4c71-e210-267875896f3d"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/CS594 - DNLP/Assignments/Assignment 1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "c3a0bd4c-f647-4398-d367-926c6ad10446"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 201
    },
    "id": "NO6oNPy7px_h",
    "outputId": "89a76740-7b33-4fb9-82ee-aa2ac6d2feba"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "# nltk.download()\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "BALelbZyNU02",
    "outputId": "3d26d58b-f15d-422f-d649-3960129d606e"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lim7c1rpZdf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CCe7BteZpZdj"
   },
   "outputs": [],
   "source": [
    "# # Load your favorite word2vec model\n",
    "# cont = Contractions('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "# text = \"we're\"\n",
    "# text = list(cont.expand_texts([text], precise=True))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "jBMsohUhpZdl"
   },
   "outputs": [],
   "source": [
    "def conv_dataframes(pos_path, neg_path):\n",
    "    with open(pos_path,'r',encoding='latin1') as f:\n",
    "        data_p = f.readlines()\n",
    "#     print(data_p[11])\n",
    "    with open(neg_path,'r',encoding='latin1') as f:\n",
    "        data_n = f.readlines()\n",
    "    pos_data = shuffle(pd.DataFrame(data_p, columns = [\"Doc Text\"]))\n",
    "#     pos_data['Sentiment'] = 1\n",
    "#     pos_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    neg_data = shuffle(pd.DataFrame(data_n, columns = [\"Doc Text\"]))\n",
    "#     neg_data['Sentiment'] = -1\n",
    "#     neg_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    return pos_data, neg_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hr-TspWpZdn"
   },
   "source": [
    "## The code for pos tagging and lemmatize sentence is fron the following link:\n",
    " ### https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "CmfT_d2dpZdn"
   },
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UJso8xCYpZdq"
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     print(wordnet_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#         if tag is not None:\n",
    "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "eioAEw_bpZdr"
   },
   "outputs": [],
   "source": [
    "# print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "# print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "# print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "# print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "0yxaZ3DHpZdt"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    # This method replaces two or more consecutive letters with the same character to something shorter. For example, gooooooood becomes good.\n",
    "    def replaceTwoOrMore(s):\n",
    "        #look for 2 or more repetitions of character\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "        return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "    # This method converts camel cased words into space delimited words.\n",
    "    # For example: ThisIsASentence will be changed to This Is A Sentence\n",
    "    def convertCamelCase(word):\n",
    "        return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",word)\n",
    "\n",
    "    # Read a flat file containing some abbreviations and their expansions in pipe separated format\n",
    "    # Use these abbreviations to replace text in the tweets as part of Preprocessing\n",
    "    \n",
    "    def readAbbrFile(abb_path):\n",
    "        global abbr_dict\n",
    "        abbr_dict ={}\n",
    "        f = open(abb_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            tmp = i.split('|')\n",
    "            abbr_dict[tmp[0]] = tmp[1]\n",
    "\n",
    "        return abbr_dict\n",
    "  \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replaceAbbr(s):\n",
    "#         temp =[]\n",
    "#         for word in s.split():\n",
    "#             if word.lower() in abbr_dict.keys():\n",
    "# #                 print('t')                \n",
    "#                 temp.append(abbr_dict[word.lower()])\n",
    "#             else:\n",
    "#                 temp.append(word)\n",
    "        temp = \" \".join([abbr_dict[word.lower()] if word.lower() in abbr_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end\n",
    "\n",
    "    def readcontractions(contra_path):\n",
    "        global contra_dict\n",
    "        contra_dict ={}\n",
    "        f = open(contra_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            try: \n",
    "                tmp = i.replace('\"', '').replace(',', '').replace('\\n', ' ').split(':')\n",
    "                contra_dict[tmp[0]] = tmp[1]\n",
    "            except:\n",
    "                print(tmp)\n",
    "                print(z)\n",
    "\n",
    "        return contra_dict\n",
    "    \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replacecontra(s):\n",
    "        temp = \" \".join([contra_dict[word.lower()] if word.lower() in contra_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end    \n",
    "    \n",
    "\n",
    "    abb_path = os.path.join(data_path,\"abbrevations.txt\")\n",
    "    abbr_dict = readAbbrFile(abb_path)\n",
    "    \n",
    "    contra_path = os.path.join(data_path,\"contractions.txt\")\n",
    "    contra_dict = readcontractions(contra_path)\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: x.lower())\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceAbbr(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replacecontra(x))\n",
    "\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[<>!#@$:.,%\\?-]+', ' ') #remove punctuation and special characters\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n\\n', ' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n', ' ') # removing new line characters\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[^\\w\\s]',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+[a-zA-Z]\\s+',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\^[a-zA-Z]\\s+',' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+',' ')  \n",
    "\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceTwoOrMore(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: convertCamelCase(x))\n",
    "\n",
    "    # Remove stop words from text\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_list]))\n",
    "    \n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(y) for y in x.split()]))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k k  k'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = 'k k  k'\n",
    "st.replace('\\s+','')\n",
    "st.replace('\\n\\n', '')\n",
    "st.replace('[^\\w\\s]','')\n",
    "st.replace('\\s+[a-zA-Z]\\s+','')\n",
    "st.replace('\\^[a-zA-Z]\\s+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "DgyAcMZRpZdv"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data1, data2):\n",
    "    temp1 = text_preprocessing(data1)\n",
    "    temp2 = text_preprocessing(data2)\n",
    "#     temp_pos2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_pos['Doc Text']])\n",
    "#     temp_neg2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_neg['Doc Text']]) \n",
    "#     temp_pos[\"Doc Text Tokens\"] = temp_pos2 \n",
    "#     temp_neg[\"Doc Text Tokens\"] = temp_neg2\n",
    "    return temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "YcIgGnBHpZd1",
    "outputId": "8e57d4d7-94c2-49f4-b28a-46e149ba5faa"
   },
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'\n",
    "trainFile = os.path.join(data_path,\"training-Obama-Romney-tweets.xlsx\")\n",
    "obama_train_temp = pd.read_excel(trainFile, sheet_name = 'Obama', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "romney_train_temp = pd.read_excel(trainFile, sheet_name = 'Romney', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "Aoo2NCbGpZd3",
    "outputId": "ff9514b3-a449-4c9e-a214-d3e144fe9ed8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
       "1  Question: If <e>Romney</e> and <e>Obama</e> ha...         2\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
       "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...         2\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...         0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
       "1  Senior <e>Romney</e> Advisor Claims <e>Obama</...         2\n",
       "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
       "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
       "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing datapoints with mixed sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...         0\n",
       "6  I was raised as a Democrat  left the party yea...        -1\n",
       "7  The <e>Obama camp</e> can't afford to lower ex...         0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train = obama_train_temp[obama_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "obama_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
       "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
       "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
       "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1\n",
       "5  Hope <e>Romney</e> debate prepped w/ the same ...         1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train = romney_train_temp[romney_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "romney_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train = obama_train.dropna()\n",
    "romney_train = romney_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1922\n",
       " 0    1895\n",
       " 1    1653\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_DA = obama_train['Sentiment'].value_counts()\n",
    "obama_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD6CAYAAABNu5eFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAARBElEQVR4nO3dcayddX3H8fdnRck2JZRxYbWtK5rigmYrcgMkRtOFDUqzCC5xgz+kOpKqgU3jlgjuD5iGxGyiG5ljqaMBEi1jQ0Kz4LASlSwB5Ra7AiLjgiiXNu11NYDBsIHf/XGfq8f23Ntz77m9t+X3fiUn9znf5/c8z+/kJJ/z3N/zO+dJVSFJasOvLHUHJEmLx9CXpIYY+pLUEENfkhpi6EtSQwx9SWrIYUM/yeokX0/yWJJHk3ykq5+UZEeSJ7q/y7t6ktyQZDzJ7iRv79nXpq79E0k2HbmXJUnqJ4ebp59kBbCiqh5K8npgJ3Ax8H7gQFV9OslVwPKq+niSjcCfARuBc4C/r6pzkpwEjAGjQHX7Oauqfjzb8U8++eRas2bNMK9Rkpqyc+fOH1XVSL91xx1u46raC+ztll9I8hiwErgIWN81uwX4BvDxrn5rTX2aPJDkxO6DYz2wo6oOACTZAWwAts12/DVr1jA2Nna4bkqSOkl+MNO6OY3pJ1kDnAl8Czi1+0CY/mA4pWu2EnimZ7OJrjZTvd9xNicZSzI2OTk5ly5KkmYxcOgneR1wB/DRqnp+tqZ9ajVL/dBi1ZaqGq2q0ZGRvv+hSJLmYaDQT/IapgL/i1X15a68rxu2mR7339/VJ4DVPZuvAvbMUpckLZJBZu8EuAl4rKo+27NqOzA9A2cTcFdP/bJuFs+5wHPd8M89wPlJlnczfc7vapKkRXLYC7nAO4D3AQ8n2dXVPgF8Grg9yeXAD4H3duvuZmrmzjjwIvABgKo6kORTwINdu09OX9SVJC2Ow07ZXGqjo6Pl7B1JGlySnVU12m+d38iVpIYY+pLUEENfkhoyyIXcY1f6fTVAC+IovxYkqT/P9CWpIYa+JDXk1T28o2NKvvGNpe7Cq1atX7/UXdBRwjN9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYPcGH1rkv1JHump/UuSXd3j6el75yZZk+SnPev+qWebs5I8nGQ8yQ3dDdclSYtokB9cuxn4B+DW6UJV/cn0cpLrged62j9ZVev67OdGYDPwAFM3T98AfGXuXZYkzddhz/Sr6j7gQL913dn6HwPbZttHkhXACVV1f03dif1W4OK5d1eSNIxhx/TfCeyrqid6aqcl+U6SbyZ5Z1dbCUz0tJnoan0l2ZxkLMnY5OTkkF2UJE0bNvQv5ZfP8vcCb6yqM4GPAV9KcgLQb/x+xvvtVdWWqhqtqtGRkZEhuyhJmjbvm6gkOQ74I+Cs6VpVvQS81C3vTPIkcDpTZ/arejZfBeyZ77ElSfMzzJn+7wPfq6qfD9skGUmyrFt+E7AWeKqq9gIvJDm3uw5wGXDXEMeWJM3DIFM2twH3A29JMpHk8m7VJRx6AfddwO4k/wX8G/Chqpq+CPxh4J+BceBJnLkjSYvusMM7VXXpDPX396ndAdwxQ/sx4G1z7J8kaQH5jVxJaoihL0kNMfQlqSGGviQ1xNCXpIbM+8tZkpS/9sdyj5S6ZsYfLRiKZ/qS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jasgg98jdmmR/kkd6atcmeTbJru6xsWfd1UnGkzye5IKe+oauNp7kqoV/KZKkwxnkTP9mYEOf+ueqal33uBsgyRlM3TD9rd02/5hkWZJlwOeBC4EzgEu7tpKkRTTIjdHvS7JmwP1dBNxWVS8B308yDpzdrRuvqqcAktzWtf3unHssSZq3Ycb0r0yyuxv+Wd7VVgLP9LSZ6Goz1ftKsjnJWJKxycnJIbooSeo139C/EXgzsA7YC1zf1fvdUaFmqfdVVVuqarSqRkdGRubZRUnSweZ156yq2je9nOQLwL93TyeA1T1NVwF7uuWZ6pKkRTKvM/0kK3qevgeYntmzHbgkyfFJTgPWAt8GHgTWJjktyWuZuti7ff7dliTNx2HP9JNsA9YDJyeZAK4B1idZx9QQzdPABwGq6tEktzN1gfZl4IqqeqXbz5XAPcAyYGtVPbrgr0aSNKtBZu9c2qd80yztrwOu61O/G7h7Tr2TJC0ov5ErSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhw39JFuT7E/ySE/tb5N8L8nuJHcmObGrr0ny0yS7usc/9WxzVpKHk4wnuSFJjsxLkiTNZJAz/ZuBDQfVdgBvq6rfAf4buLpn3ZNVta57fKinfiOwGVjbPQ7epyTpCDts6FfVfcCBg2pfraqXu6cPAKtm20eSFcAJVXV/VRVwK3Dx/LosSZqvhRjT/1PgKz3PT0vynSTfTPLOrrYSmOhpM9HV+kqyOclYkrHJyckF6KIkCYYM/SR/BbwMfLEr7QXeWFVnAh8DvpTkBKDf+H3NtN+q2lJVo1U1OjIyMkwXJUk9jpvvhkk2AX8InNcN2VBVLwEvdcs7kzwJnM7UmX3vENAqYM98jy1Jmp95nekn2QB8HHh3Vb3YUx9JsqxbfhNTF2yfqqq9wAtJzu1m7VwG3DV07yVJc3LYM/0k24D1wMlJJoBrmJqtczywo5t5+UA3U+ddwCeTvAy8AnyoqqYvAn+YqZlAv8rUNYDe6wCSpEVw2NCvqkv7lG+aoe0dwB0zrBsD3jan3kmSFpTfyJWkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JCBQj/J1iT7kzzSUzspyY4kT3R/l3f1JLkhyXiS3Une3rPNpq79E0k2LfzLkSTNZtAz/ZuBDQfVrgLuraq1wL3dc4ALgbXdYzNwI0x9SDB1U/VzgLOBa6Y/KCRJi2Og0K+q+4ADB5UvAm7plm8BLu6p31pTHgBOTLICuADYUVUHqurHwA4O/SCRJB1Bw4zpn1pVewG6v6d09ZXAMz3tJrraTPVDJNmcZCzJ2OTk5BBdlCT1OhIXctOnVrPUDy1Wbamq0aoaHRkZWdDOSVLLhgn9fd2wDd3f/V19Aljd024VsGeWuiRpkQwT+tuB6Rk4m4C7euqXdbN4zgWe64Z/7gHOT7K8u4B7fleTJC2S4wZplGQbsB44OckEU7NwPg3cnuRy4IfAe7vmdwMbgXHgReADAFV1IMmngAe7dp+sqoMvDkuSjqCBQr+qLp1h1Xl92hZwxQz72QpsHbh3kqQF5TdyJakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1ZN6hn+QtSXb1PJ5P8tEk1yZ5tqe+sWebq5OMJ3k8yQUL8xIkSYMa6B65/VTV48A6gCTLgGeBO5m6Efrnquozve2TnAFcArwVeAPwtSSnV9Ur8+2DJGluFmp45zzgyar6wSxtLgJuq6qXqur7wDhw9gIdX5I0gIUK/UuAbT3Pr0yyO8nWJMu72krgmZ42E13tEEk2JxlLMjY5OblAXZQkDR36SV4LvBv41650I/BmpoZ+9gLXTzfts3n122dVbamq0aoaHRkZGbaLkqTOQpzpXwg8VFX7AKpqX1W9UlU/A77AL4ZwJoDVPdutAvYswPElSQNaiNC/lJ6hnSQreta9B3ikW94OXJLk+CSnAWuBby/A8SVJA5r37B2AJL8G/AHwwZ7y3yRZx9TQzdPT66rq0SS3A98FXgaucOaOJC2uoUK/ql4EfuOg2vtmaX8dcN0wx5QkzZ/fyJWkhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1JChQz/J00keTrIryVhXOynJjiRPdH+Xd/UkuSHJeJLdSd4+7PElSYNbqDP936uqdVU12j2/Cri3qtYC93bPAS4E1naPzcCNC3R8SdIAjtTwzkXALd3yLcDFPfVba8oDwIlJVhyhPkiSDrIQoV/AV5PsTLK5q51aVXsBur+ndPWVwDM92050tV+SZHOSsSRjk5OTC9BFSRLAcQuwj3dU1Z4kpwA7knxvlrbpU6tDClVbgC0Ao6Ojh6yXJM3P0Gf6VbWn+7sfuBM4G9g3PWzT/d3fNZ8AVvdsvgrYM2wfJEmDGSr0k/x6ktdPLwPnA48A24FNXbNNwF3d8nbgsm4Wz7nAc9PDQJKkI2/Y4Z1TgTuTTO/rS1X1H0keBG5PcjnwQ+C9Xfu7gY3AOPAi8IEhjy9JmoOhQr+qngJ+t0/9f4Dz+tQLuGKYY0qS5s9v5EpSQwx9SWqIoS9JDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkMMfUlqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1Jasi8Qz/J6iRfT/JYkkeTfKSrX5vk2SS7usfGnm2uTjKe5PEkFyzEC5AkDW6Ye+S+DPxFVT2U5PXAziQ7unWfq6rP9DZOcgZwCfBW4A3A15KcXlWvDNEHSdIczPtMv6r2VtVD3fILwGPAylk2uQi4rapeqqrvA+PA2fM9viRp7hZkTD/JGuBM4Ftd6coku5NsTbK8q60EnunZbIIZPiSSbE4ylmRscnJyIbooSWIBQj/J64A7gI9W1fPAjcCbgXXAXuD66aZ9Nq9++6yqLVU1WlWjIyMjw3ZRktQZKvSTvIapwP9iVX0ZoKr2VdUrVfUz4Av8YghnAljds/kqYM8wx5ckzc0ws3cC3AQ8VlWf7amv6Gn2HuCRbnk7cEmS45OcBqwFvj3f40uS5m6Y2TvvAN4HPJxkV1f7BHBpknVMDd08DXwQoKoeTXI78F2mZv5c4cwdSVpc8w79qvpP+o/T3z3LNtcB1833mJKk4fiNXElqiKEvSQ0x9CWpIYa+JDXE0Jekhhj6ktQQQ1+SGmLoS1JDDH1JaoihL0kNMfQlqSGGviQ1xNCXpIYY+pLUEENfkhpi6EtSQwx9SWqIoS9JDVn00E+yIcnjScaTXLXYx5ekli1q6CdZBnweuBA4g6mbqJ+xmH2QpJYt9pn+2cB4VT1VVf8L3AZctMh9kKRmHbfIx1sJPNPzfAI45+BGSTYDm7unP0ny+CL0bamdDPxoqTsxsGSpe3A0OGbeM9+tnzt23rNrh3rXfmumFYsd+v1eRR1SqNoCbDny3Tl6JBmrqtGl7ocG53t27PE9W/zhnQlgdc/zVcCeRe6DJDVrsUP/QWBtktOSvBa4BNi+yH2QpGYt6vBOVb2c5ErgHmAZsLWqHl3MPhzFmhrOepXwPTv2NP+epeqQIXVJ0quU38iVpIYY+pLUEEP/KJDkt5Pcn+SlJH+51P3R7PwpkWNPkq1J9id5ZKn7stQM/aPDAeDPgc8sdUc0O39K5Jh1M7BhqTtxNDD0jwJVtb+qHgT+b6n7osPyp0SOQVV1H1MnV80z9KW56fdTIiuXqC/SnBn60twM9FMi0tHK0F8iSa5Isqt7vGGp+6OB+VMiOqYZ+kukqj5fVeu6h6Fx7PCnRHRM8xu5R4EkvwmMAScAPwN+ApxRVc8vacfUV5KNwN/xi58SuW6Ju6TDSLINWM/UTyvvA66pqpuWtFNLxNCXpIY4vCNJDTH0Jakhhr4kNcTQl6SGGPqS1BBDX5IaYuhLUkP+Hytol2Xy36lOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = obama_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2893\n",
       " 0    1680\n",
       " 1    1075\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_DA = romney_train['Sentiment'].value_counts()\n",
    "romney_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = romney_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Romney data is very imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hm_lines = 5331\n",
    "\n",
    "# tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# spell = Speller(lang='en')\n",
    "# # spell = SpellChecker()\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list.extend(['rt', 'retweet', 'e'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train_pr, romney_train_pr = create_vocab(obama_train, romney_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...         0\n",
       "2  obama debate cracker as cracker tonight tune t...         1\n",
       "4  miss point afraid understand big picture dont ...         0\n",
       "6  raise democrat leave party year ago lifetime n...        -1\n",
       "7  obama camp can not afford low expectation toni...         0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train_pr['Sentiment'] = obama_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "2  obama debate cracker as cracker tonight tune t...  Positive\n",
       "4  miss point afraid understand big picture dont ...   Neutral\n",
       "6  raise democrat leave party year ago lifetime n...  Negative\n",
       "7  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...        -1\n",
       "2                     mean like romney cheat primary        -1\n",
       "3          mitt romney still believe black president        -1\n",
       "4  romney tax plan deserve nd look secret one dif...        -1\n",
       "5        hope romney debate prepped people last time         1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "romney_train_pr['Sentiment'] = romney_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...  Negative\n",
       "2                     mean like romney cheat primary  Negative\n",
       "3          mitt romney still believe black president  Negative\n",
       "4  romney tax plan deserve nd look secret one dif...  Negative\n",
       "5        hope romney debate prepped people last time  Positive"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWwsQNTfpZd-"
   },
   "source": [
    "## Splitting the data into training, and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "POMPwmnZpZd_",
    "outputId": "872ee0f6-384c-45ea-95c4-96ba426a1539"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1987</th>\n",
       "      <td>obama force nature debate</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2384</th>\n",
       "      <td>wow group think obama perform well attack idea...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1293</th>\n",
       "      <td>honey boo boo endorse obama obama president ji...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1603</th>\n",
       "      <td>exit sir barack time</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>feel obama still sucker punch leave</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Doc Text Sentiment\n",
       "1987                          obama force nature debate  Positive\n",
       "2384  wow group think obama perform well attack idea...  Negative\n",
       "1293  honey boo boo endorse obama obama president ji...  Positive\n",
       "1603                               exit sir barack time  Negative\n",
       "413                 feel obama still sucker punch leave   Neutral"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_train, obama_train_pr_val = train_test_split(obama_train_pr, test_size = 0.2)\n",
    "obama_train_pr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>billy graham prays romney victory</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>woman push romney r lead</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3992</th>\n",
       "      <td>gt gop hop soar romney roll ohio tcot</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4239</th>\n",
       "      <td>eve second debate romney rise cbs news cbc cao...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3521</th>\n",
       "      <td>romney bury trample female moderator say care ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Doc Text Sentiment\n",
       "706                   billy graham prays romney victory  Positive\n",
       "2537                           woman push romney r lead   Neutral\n",
       "3992              gt gop hop soar romney roll ohio tcot  Positive\n",
       "4239  eve second debate romney rise cbs news cbc cao...  Positive\n",
       "3521  romney bury trample female moderator say care ...  Negative"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr_train, romney_train_pr_val = train_test_split(romney_train_pr, test_size = 0.2)\n",
    "romney_train_pr_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lafbBlAkpZeE"
   },
   "outputs": [],
   "source": [
    "# training = shuffle(pd.concat([vocab_p_train,vocab_n_train])).reset_index(drop = True)\n",
    "\n",
    "# validation = pd.concat([vocab_p_val,vocab_n_val]).reset_index(drop = True)\n",
    "\n",
    "# test = pd.concat([vocab_p_test,vocab_n_test]).reset_index(drop = True)\n",
    "# # len(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "_m-rtqE9qPwD"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr_train.to_csv(os.path.join(data_path, 'Obama Training Data.csv'))\n",
    "# obama_train_pr_val.to_csv(os.path.join(data_path, 'Obama Validation Data.csv'))\n",
    "# # test.to_csv('Test Data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# romney_train_pr_train.to_csv(os.path.join(data_path, 'Romney Training Data.csv'))\n",
    "# romney_train_pr_val.to_csv(os.path.join(data_path, 'Romney Validation Data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lRe51K-ssOYp",
    "outputId": "c7c78223-0005-4e84-a369-6bd424d94eb2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obama force nature debate</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wow group think obama perform well attack idea...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>honey boo boo endorse obama obama president ji...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exit sir barack time</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>feel obama still sucker punch leave</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0                          obama force nature debate  Positive\n",
       "1  wow group think obama perform well attack idea...  Negative\n",
       "2  honey boo boo endorse obama obama president ji...  Positive\n",
       "3                               exit sir barack time  Negative\n",
       "4                feel obama still sucker punch leave   Neutral"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train = pd.read_csv(os.path.join(data_path, 'Obama Training Data.csv'), usecols=[1,2])\n",
    "obama_val = pd.read_csv(os.path.join(data_path, 'Obama Validation Data.csv'), usecols=[1,2])\n",
    "\n",
    "obama_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    temp = np.zeros((len(data),3))\n",
    "    for i in range(len(temp)):\n",
    "        if data[i] == 'Negative':\n",
    "            temp[i][2] = 1 ## Negative sentiment third neuron\n",
    "        elif data[i] == 'Neutral':\n",
    "            temp[i][1] = 1 ## Neutral sentiment second neuron  \n",
    "        else:\n",
    "            temp[i][0] = 1 ## Positive sentiment first neuron             \n",
    "    return temp\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>romney romney newnikeslogan</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>need republican friend maybe shed light romney...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt people die lack health insurance mitt tal...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney say borrow money parent college bitch c...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  romney tax plan deserve nd look secret one dif...  Negative\n",
       "1                        romney romney newnikeslogan  Negative\n",
       "2  need republican friend maybe shed light romney...  Negative\n",
       "3  mitt people die lack health insurance mitt tal...  Negative\n",
       "4  romney say borrow money parent college bitch c...  Negative"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train = pd.read_csv(os.path.join(data_path,'Romney Training Data.csv'), usecols=[1,2])\n",
    "romney_val = pd.read_csv(os.path.join(data_path,'Romney Validation Data.csv'), usecols=[1,2])\n",
    "\n",
    "romney_val.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmGshHslpZeH"
   },
   "source": [
    "## Building Glove Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(os.path.join(data_path,\"glove.6B.300d.txt\"), 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or3hmQvdpZeJ"
   },
   "source": [
    "## Embedding Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "wBG3Kq7EpZeJ"
   },
   "outputs": [],
   "source": [
    "def emb_matrix(t,embeddings):\n",
    "    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n",
    "    vocab_size = len(t.word_index) + 1    \n",
    "    embedding_matrix = zeros((vocab_size, 300))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning the word embeddings of 300 dimensions using mittens library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Used the code for finetuning from the following link:\n",
    "### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "    #training_tokens\n",
    "\n",
    "    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "    print(len(oov))\n",
    "\n",
    "    corp_vocab = list(set(oov))\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "    trr =''\n",
    "    for i in training_tokens:\n",
    "        for j in i:\n",
    "            trr+= j\n",
    "            trr += ' '\n",
    "\n",
    "    # print(trr)\n",
    "    # print(z)\n",
    "    X = cv.fit_transform([trr])\n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    coocc_ar = Xc.toarray()\n",
    "\n",
    "    mittens_model = Mittens(n=300, max_iter=2000)\n",
    "\n",
    "    new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "    return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\scipy\\sparse\\_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:77: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 2000: loss: 0.19232714176177979"
     ]
    }
   ],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(obama_train)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom F1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_value(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VHQfx6SpZeN"
   },
   "source": [
    "## Building Vanilla RNN, LSTM, and GRU models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "sNbzJtMWpZeN"
   },
   "outputs": [],
   "source": [
    "def model_vanilla_rnn(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(SimpleRNN(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "D69lMKSLpZeP"
   },
   "outputs": [],
   "source": [
    "def model_lstm(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "tDL6HFQYpZeQ"
   },
   "outputs": [],
   "source": [
    "def model_gru(embedding_matrix, noh,  vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(GRU(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "42FDB3zmpZeS"
   },
   "outputs": [],
   "source": [
    "# max_length = 40\n",
    "# epochs = 2\n",
    "# batch_size = 200\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# tokenise_tf = Tokenizer()\n",
    "# tokenise_tf.fit_on_texts(obama_train['Doc Text'])   \n",
    "# encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
    "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "# encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
    "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "# model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "# history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True)\n",
    "# y_pred_temp = model.predict(validation_padded)\n",
    "# y_pred = pred(y_pred_temp)\n",
    "# f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n",
    "# print(f1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u5oc67yzQ2-Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dw6cg12SpZeU"
   },
   "outputs": [],
   "source": [
    "# u = {}\n",
    "# u['max_len'] = max_length\n",
    "# u['batch'] = batch_size\n",
    "# u['l_rate'] = learning_rate\n",
    "# u['epochs'] = epochs\n",
    "\n",
    "# fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "# ax[0].plot(history.history['loss'], label='Training')\n",
    "# ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "# ax[0].legend(loc='upper left')\n",
    "# ax[0].title.set_text('Loss plot for the combination ' + str(u)) \n",
    "\n",
    "# ax[1].plot(history.history['accuracy'], label='Training')\n",
    "# ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "# ax[1].legend(loc='upper left')\n",
    "# ax[1].title.set_text('Accuracy plot for the combination ' + str(u))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(Callback):\n",
    "    def __init__(self, validation):   \n",
    "        super(Metrics, self).__init__()\n",
    "        self.validation = validation    \n",
    "            \n",
    "        print('validation shape', len(self.validation[0]))\n",
    "        \n",
    "    def on_train_begin(self, logs={}):        \n",
    "        self.val_f1s = []\n",
    "        self.val_recalls = []\n",
    "        self.val_precisions = []\n",
    "     \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        val_targ = self.validation[1]   \n",
    "        val_predict = (np.asarray(self.model.predict(self.validation[0]))).round()        \n",
    "    \n",
    "        val_f1 = f1_score(val_targ, val_predict)\n",
    "        val_recall = recall_score(val_targ, val_predict)         \n",
    "        val_precision = precision_score(val_targ, val_predict)\n",
    "        \n",
    "        self.val_f1s.append(round(val_f1, 6))\n",
    "        self.val_recalls.append(round(val_recall, 6))\n",
    "        self.val_precisions.append(round(val_precision, 6))\n",
    " \n",
    "        print(f' — val_f1: {val_f1} — val_precision: {val_precision}, — val_recall: {val_recall}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    temp = []\n",
    "    for i in x:\n",
    "        m = np.argmax(i)\n",
    "        if m == 0:\n",
    "            temp.append('Positive')\n",
    "        elif m == 1:\n",
    "            temp.append('Neutral')\n",
    "        else:\n",
    "            temp.append('Negative')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models for obama tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLDi0elvjHkC"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBp3ntnMpZeX"
   },
   "source": [
    "## Using Hyperopt library to tune the Hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Am8LqqY96ghg",
    "outputId": "9371e1aa-69bb-4fe6-ea75-cf27b3725656"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [1:31:10<28:52:10, 5470.04s/trial, best loss: -0.56]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-73-958ab2917cad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[0mtokenise_tf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mtokenise_tf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobama_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Doc Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mbest_vanilla_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_vanilla_rnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 509\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    511\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    284\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    163\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 165\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    166\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    167\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    892\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             )\n\u001b[1;32m--> 894\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    895\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    896\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-73-958ab2917cad>\u001b[0m in \u001b[0;36mobjective_func\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_vanilla_rnn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madam_optimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'tanh'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_padded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobama_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[0my_pred_temp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_padded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred_temp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    808\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    810\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    664\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    665\u001b[0m         \u001b[0mvalidation_freq\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_freq\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 666\u001b[1;33m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[0;32m    667\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    668\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq, mode, validation_in_fit, prepared_feed_values_from_dataset, steps_name, **kwargs)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 386\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    387\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3824\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3825\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3826\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3827\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[0;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                                                run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1473\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "    y_pred_temp = model.predict(validation_padded)\n",
    "    y_pred = pred(y_pred_temp)\n",
    "    f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n",
    "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
    "#     f1 = history.history['f1_value'][-1]\n",
    "   \n",
    "    return -round(f1,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,50)),  \n",
    "        'batch_size': hp.choice('batch_size', [64, 128, 256]),\n",
    "         'epochs': hp.choice('epochs',range(10,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(obama_train['Doc Text'])                                \n",
    "best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_vanilla_rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpDDkemAhldQ"
   },
   "source": [
    "# For vanilla rnn, the best hyper parameters are:\n",
    "# 'batch_size': 128, 'epochs': 6, 'learning_rate': 0.0003, 'max_length': 68"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ougNDoQpZea"
   },
   "outputs": [],
   "source": [
    "# {'batch_size': 0, 'epochs': 7, 'max_length': 59, 'padd': 0, 'trunc': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5JhmM1-BpZed"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "rN74XEQLpZef",
    "outputId": "d4b19de2-4abe-43c8-e12b-5617f1acfbe3"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_lstm(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = round(history.history['val_accuracy'][-1],2)\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "    return -round(accuracy,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }             \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_lstm = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_lstm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6vO3cE8h-Np"
   },
   "source": [
    "# For LSTM rnn, the best hyper parameters are:\n",
    "# 'batch_size': 128, 'epochs': 10, 'learning_rate': 0.002, 'max_length': 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf5oOkX5h_15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Lgt4uCrYpZeh",
    "outputId": "34c29222-7a66-4984-96d2-c19b9ceea492"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_gru(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round((accuracy),2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_gru = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "illGwOwGiO_y"
   },
   "source": [
    "# For GRU rnn, the best hyper parameters are:\n",
    "# 'batch_size': 64, 'epochs': 7, 'learning_rate': 0.0006, 'max_length': 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTooPqJ3pZej"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmX0YNPUQDjL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FagU-LZrpZeu"
   },
   "outputs": [],
   "source": [
    "# max_length = 40\n",
    "# epochs = 15\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "\n",
    "# encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "# model = model_vanilla_rnn(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "# history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "# accuracy = history.history['val_accuracy'][-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vbdqiyy_ibn4"
   },
   "source": [
    "# Experimenting with the Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dmf1gvnwjj0E"
   },
   "source": [
    "## Using the Final \f",
    "hidden state as the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffW7HsAY_SD_"
   },
   "source": [
    "## max_length = 40\n",
    "## epochs = 15\n",
    "## batch_size = 64\n",
    "## learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whLYqoGw_K4q"
   },
   "outputs": [],
   "source": [
    "\n",
    "def par_selec(model_r, max_length, nnh):\n",
    "  # max_length = 40\n",
    "  epochs = 15\n",
    "  batch_size = 64\n",
    "  learning_rate = 0.001\n",
    "\n",
    "  tokenise_tf = Tokenizer()\n",
    "  tokenise_tf.fit_on_texts(training['Doc Text']) \n",
    "\n",
    "  encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "  training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "  embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "\n",
    "  encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "  validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "\n",
    "  adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "  model = model_r(embedding_matrix, nnh, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "  history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "  accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "  u = {}\n",
    "  u['max_len'] = max_length\n",
    "  u['batch'] = batch_size\n",
    "  u['l_rate'] = learning_rate\n",
    "  u['epochs'] = epochs\n",
    "\n",
    "  fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "  ax[0].plot(history.history['loss'], label='Training')\n",
    "  ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "  ax[0].legend(loc='upper left')\n",
    "  ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "  ax[1].plot(history.history['accuracy'], label='Training')\n",
    "  ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "  ax[1].legend(loc='upper left')\n",
    "  ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "  plt.show()\n",
    "\n",
    "  print('Accuracy for ' + str(nnh) + ' hidden units ' + str(max_length) + ' sequence length: ' + str(accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qoSZZjwb7TRx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDJxWgnn7Tw8"
   },
   "source": [
    "## Changing the no of hidden units\n",
    "## Keeping max_length = 40\n",
    "## epochs = 15\n",
    "## batch_size = 64\n",
    "## learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "2Yn9-A557TxB",
    "outputId": "0586062c-07c2-4ae9-8797-a607d99e6d19"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn, 40, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "J5ATsHkx7TxS",
    "outputId": "384f2698-53cf-4029-fbea-36a216569f79"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm, 40, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "Wa9_OGRP7TxY",
    "outputId": "021c8375-359c-4261-e3a0-f285034d94b9"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru, 40, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fs3tQI087Txe"
   },
   "source": [
    "## Doubling the no of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "44Tox3Py7Txe",
    "outputId": "a9d312cb-0e9c-4f56-9739-935b0b0826a5"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn, 40, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cClcEyhU7Txl"
   },
   "source": [
    "### For Vanilla RNN,  With double the no of hidden units, the accuracy reduced from 60 to 56 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "tZrghPzq7Txm",
    "outputId": "49894fb8-a3c4-4be2-81f1-5b96828f95af"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm, 40, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q9v5oT1L7Txr"
   },
   "source": [
    "### For LSTM, With double the no of hidden units, the accuracy slightly increased from 72 to 73 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "_GUX6NVE7Tx2",
    "outputId": "212bf133-e313-47a5-a5f6-a69a3e4fe52d"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru, 40, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7hXiStNo7Tx8"
   },
   "source": [
    "### For GRU, With double the no of hidden units, the accuracy didn't change from 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvcJ1EEAj-5y"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCKOM-Oq7Tx9"
   },
   "source": [
    "# Halving the no of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "DGVzQb0G7Tx-",
    "outputId": "1c708af5-10d3-4013-85b5-3b80dafee399"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vUi_W427TyE"
   },
   "source": [
    "### With halve the no of hidden units, the accuracy increased from 60 to 73."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "tJS0-PRv7TyG",
    "outputId": "d06b33b5-378b-4df7-9fde-a1ce191f9bea"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jP9J5ZPS7TyK"
   },
   "source": [
    "### For LSTM, With halve the no of hidden units, the accuracy increased from 72 to 74 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "Ceg8MQq87TyM",
    "outputId": "44632338-fc6b-4ad4-a11d-67cb201ce63c"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JibZiZXD7TyQ"
   },
   "source": [
    "### For GRU, With halve the no of hidden units, the accuracy remain unchanged."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eScyCusZ7TyR"
   },
   "source": [
    "# Changing the max_length \n",
    "# Keeping no of hidden units = 300\n",
    "# epochs = 15\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dAgLy7G27TyR"
   },
   "source": [
    "## Doubling the sequence length from 40 to 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "MGLD_44Z7TyT",
    "outputId": "d20286b4-ac1e-4385-eb1e-436ffd3d9126"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nj_9UVMY7TyX"
   },
   "source": [
    "### For vanilla rnn, With double the sequence length, there was a decrease in accuracy from 60 to 53\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "J4mHJuFk7TyY",
    "outputId": "59587bb0-0674-40e0-e2b4-68b9ee5fbc84"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HyOMu3Gj7Tyd"
   },
   "source": [
    "### For LSTM, With double the sequence length, there was a decrease in accuracy from 72 to 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "8y17kI_X7Tyf",
    "outputId": "97c5f13f-9097-4b07-82a0-cd2752e6a9af"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtNmEflB7Tyk"
   },
   "source": [
    "### For GRU, With double the sequence length, there was no change in accuracy from 50\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wt3_Uirp7Tyk"
   },
   "source": [
    "# When we increase the sequence length we might include noisy (unimportant) words that would change the sentiment of the sentence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRRMJvPi7Tyl"
   },
   "source": [
    "## Halving the sequence length from 40 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "xSNoA1q47Tym",
    "outputId": "d6790b51-54ea-4934-828d-b153409815ab"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cphAW4jM7Tyu"
   },
   "source": [
    "### For vanilla RNN, With half the sequence length, there was an increase in accuracy from 60 to 73\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "VuGQ9V1i7Tyu",
    "outputId": "42306b8a-10b0-4106-e617-653e41a4e7bb"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUyjDAP87Tyy"
   },
   "source": [
    "### For LSTM, With half the sequence length, the accuracy increased from 72 to 74 percent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "df9j5BS97Tyz",
    "outputId": "74ce5f16-ab3b-453a-deb0-8093af47577e"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hqoXjGKQ7Ty4"
   },
   "source": [
    "### For GRU, With half the sequence length, the accuracy increased from 50 to 74 percent.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wf62lx_C7Ty4"
   },
   "source": [
    "\n",
    "## It makes sense as when we decrease the sequence length we might concentrate more on the important words that change the sentiment of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qvPmoZjc7Ty4"
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBhjO4nN7Ty5"
   },
   "source": [
    "## Fine tuning the word embeddings of 300 dimensions using mittens library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9heV52M07Ty6"
   },
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "  training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "  #training_tokens\n",
    "\n",
    "  oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "  print(len(oov))\n",
    "\n",
    "  corp_vocab = list(set(oov))\n",
    "\n",
    "  cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "  trr =''\n",
    "  for i in training_tokens:\n",
    "    for j in i:\n",
    "      trr+= j\n",
    "      trr += ' '\n",
    "\n",
    "  # print(trr)\n",
    "  # print(z)\n",
    "  X = cv.fit_transform([trr])\n",
    "  Xc = (X.T * X)\n",
    "  Xc.setdiag(0)\n",
    "  coocc_ar = Xc.toarray()\n",
    "\n",
    "  mittens_model = Mittens(n=300, max_iter=1000)\n",
    "\n",
    "  new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "  new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "  return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 166
    },
    "id": "vwP2PO-T7Ty8",
    "outputId": "4bae2159-219f-4c2c-e9ec-68a19f4ca2df"
   },
   "outputs": [],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(training)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Mt98x9ps7TzA",
    "outputId": "5736a498-d0c3-487b-d63a-e2e5dd2d95db"
   },
   "outputs": [],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-Pn9IMU7TzD"
   },
   "source": [
    "### Using new embeddings for the vanilla rnn em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7HQ7f4bd7TzE",
    "outputId": "ff577747-707c-44fa-98a1-de44def1682d"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round(accuracy,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_vanilla_rnn2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_vanilla_rnn2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "IsOrfMuhDgOV",
    "outputId": "008d8719-432e-4ce2-f0a8-4092b0895659"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_lstm(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round(accuracy,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_lstm2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_lstm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "g4PHN2V08O-j",
    "outputId": "7982e451-015d-4180-8145-5b8075256da3"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_gru(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round(accuracy,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_gru2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_gru2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfA31RGemoGF"
   },
   "source": [
    "# For all the three RNNs,I got best accuracy when they were trained on the pre-trained and fine-tuned word embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7V5GU3fPxjZq"
   },
   "outputs": [],
   "source": [
    "batch_size_temp = [32,64,128]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xPARrC-3n9W2"
   },
   "source": [
    "# I got best validation accuracy of 73 with Vanilla RNN model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
    "\n",
    "## 'batch_size': 128, 'epochs': 11, 'learning_rate': 0.0016, 'max_length': 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AtRsC-V3sETk"
   },
   "outputs": [],
   "source": [
    "# best_vanilla_rnn2['batch_size'] = batch_size_temp[best_vanilla_rnn2['batch_size']]\n",
    "# best_v_r_df = pd.DataFrame(data = best_vanilla_rnn2.items(),index=best_vanilla_rnn2.keys())\n",
    "# best_v_r_df.to_csv('best_v_r.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VItBueodmo--"
   },
   "source": [
    "# I got best validation accuracy of 75 with LSTM model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
    "\n",
    "## 'batch_size': 64, 'epochs': 12, 'learning_rate': 0.0018206437643498807, 'max_length': 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c6BodZIKsVXy"
   },
   "outputs": [],
   "source": [
    "# best_l_r['batch_size'] = batch_size_temp[best_l_r['batch_size']]\n",
    "# best_l_r_df = pd.DataFrame(best_l_r.items(),index=best_l_r.keys())\n",
    "# best_l_r_df.to_csv('best_l_r.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJxPMORcn9eK"
   },
   "source": [
    "# I got best validation accuracy of 74 with GRU model trained on the pre trained and fine tuned word embeddings with hyperparameters: \n",
    "\n",
    "## 'batch_size': 64, 'epochs': 11, 'learning_rate': 0.001, 'max_length': 19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYLQRhDHskQy"
   },
   "outputs": [],
   "source": [
    "# best_g_r = {'batch_size': 64, 'epochs': 11, 'learning_rate': 0.001, 'max_length': 19}\n",
    "# best_g_r_df = pd.DataFrame(best_g_r.items(),index=best_g_r.keys())\n",
    "# best_g_r_df.to_csv('best_g_r.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nb9iWGCZod88"
   },
   "source": [
    "## Out of the three above, LSTM performed best on the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N2uQbiiJooM8"
   },
   "source": [
    "# ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYcv8O3yoraN"
   },
   "source": [
    "# Evaluating on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "vxEQ4EWronP8",
    "outputId": "d6c4ee85-5b30-47b4-d292-09a500d143a9"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('Test Data1.csv').iloc[:,1:]\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt8fLB8EonmO"
   },
   "outputs": [],
   "source": [
    "def test_evaluation(test, model, max_length, batch_size, epochs, learning_rate):\n",
    "  tokenise_tf = Tokenizer()\n",
    "  tokenise_tf.fit_on_texts(training['Doc Text']) \n",
    "  \n",
    "  encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "  training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "  embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "  encoded_test = tokenise_tf.texts_to_sequences(test['Doc Text'])\n",
    "  test_padded = pad_sequences(encoded_test, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "  \n",
    "  adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "  model2 = model(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "  history = model2.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "  temp = lambda x: 1 if x>= 0.5 else 0 \n",
    "  y_pred = [temp(i) for i in model2.predict(test_padded)]\n",
    "  # print(y_pred) \n",
    "  y_test = test['Sentiment']\n",
    "  # print(y_test)\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  precision = precision_score(y_test, y_pred)\n",
    "  recall = recall_score(y_test, y_pred)\n",
    "  f1 = f1_score(y_test, y_pred)\n",
    "  return accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oavVnBnpzPX"
   },
   "outputs": [],
   "source": [
    "# Testing Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "uVgsJS0bqL9I",
    "outputId": "116fba05-bdba-4a49-af2e-177e35ba810f"
   },
   "outputs": [],
   "source": [
    "best_v_r_df2 = pd.read_csv('best_v_r.csv')\n",
    "Vanilla_acc, Vanilla_prec, Vanilla_rec, Vanilla_f1= test_evaluation(test, model_vanilla_rnn, best_v_r_df2['max_length'][0], best_v_r_df2['batch_size'][0], best_v_r_df2['epochs'][0], best_v_r_df2['learning_rate'][0])\n",
    "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_acc))\n",
    "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_prec))\n",
    "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_rec))\n",
    "print('Accuracy of Vanilla RNN on the test data: ' + str(Vanilla_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BdwoTc3Fz_AG"
   },
   "outputs": [],
   "source": [
    "# Testing LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "TMrLL4Uo1fSB",
    "outputId": "0f535ba2-db79-487c-aa46-27c082936542"
   },
   "outputs": [],
   "source": [
    "best_lstm_df2 = pd.read_csv('best_l_r.csv')\n",
    "lstm_acc, lstm_prec, lstm_rec, lstm_f1= test_evaluation(test, model_lstm, best_lstm_df2['max_length'][0], best_lstm_df2['batch_size'][0], best_lstm_df2['epochs'][0], best_lstm_df2['learning_rate'][0])\n",
    "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_acc))\n",
    "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_prec))\n",
    "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_rec))\n",
    "print('Accuracy of LSTM RNN on the test data: ' + str(lstm_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9x_ONNn1kLH"
   },
   "outputs": [],
   "source": [
    "# Testing GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 109
    },
    "id": "EKqIZAaf1kNY",
    "outputId": "da665b2c-d582-4fc0-9698-3c97d23af48c"
   },
   "outputs": [],
   "source": [
    "best_gru_df2 = pd.read_csv('best_g_r.csv')\n",
    "gru_acc, gru_prec,gru_rec,gru_f1= test_evaluation(test, model_gru, best_gru_df2['max_length'][0], best_gru_df2['batch_size'][0], best_gru_df2['epochs'][0], best_gru_df2['learning_rate'][0])\n",
    "print('Accuracy of GRU RNN on the test data: ' + str(gru_acc))\n",
    "print('Accuracy of GRU RNN on the test data: ' + str(gru_prec))\n",
    "print('Accuracy of GRU RNN on the test data: ' + str(gru_rec))\n",
    "print('Accuracy of GRU RNN on the test data: ' + str(gru_f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjqAkObFmQZv"
   },
   "source": [
    "## I got best test accuracy of 75 percent with the GRU RNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F_FRcBkpoMTP"
   },
   "source": [
    "# Observed differences between the performances of the three rnns:\n",
    "## Got best validation accuracy with LSTM but got best test accuracy with GRU\n",
    "## Vanilla RNN is fastest and LSTM is slowest in terms of training because LSTM has more parameters and heavier computation\n",
    "## Vanilla RNN gave comparitively lowest accuracies after hyper parameter tuning maybe because of vanishing gradient problem.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZNX9oam3GYG"
   },
   "source": [
    "# ##########"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HimtALnV3Gbo"
   },
   "source": [
    "# Ignore the code from now on. I just wanted to save the code for element-wise max.\n",
    "# With the element_wise max of all the hidden states as the output, I got less accuracy of 50. Whatever the parameters, the accuracy didn't cross 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G84h-TiMpZel"
   },
   "source": [
    "## Taking elementwise max of the hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uE2SSiwupZel"
   },
   "outputs": [],
   "source": [
    "def model_vanilla_rnn_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(SimpleRNN(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.add(MaxPool1D(max_length, strides = None))\n",
    "    # model.add(Dropout(Dropout_rate))\n",
    "\n",
    "#     model.add(Dense(1, activation='softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    \n",
    "#     print(z)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a3cyFsTtpZep"
   },
   "outputs": [],
   "source": [
    "def model_lstm_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(LSTM(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.add(MaxPool1D(max_length, strides = None))\n",
    "    # model.add(Dropout(Dropout_rate))\n",
    "\n",
    "#     model.add(Dense(1, activation='softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    \n",
    "#     print(z)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qyw6ayuspZes"
   },
   "outputs": [],
   "source": [
    "def model_gru_em(embedding_matrix, noh, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(GRU(units = noh, activation = activation, dropout=0.2, return_sequences=True, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.add(MaxPool1D(max_length, strides = None))\n",
    "#     model.add(Dropout(Dropout_rate))\n",
    "\n",
    "#     model.add(Dense(1, activation='softmax'))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    \n",
    "#     print(z)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WDR5cTqT_hob"
   },
   "source": [
    "## Changing the no of hidden units\n",
    "## Keeping max_length = 40\n",
    "## epochs = 15\n",
    "## batch_size = 64\n",
    "## learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "id": "TObOEV44_frn",
    "outputId": "3d16605b-62c7-4120-fcf3-dbb3308d0369"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn_em, 40, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "l9hd1nbGBsyZ",
    "outputId": "7a55cac9-9f69-4f96-afae-ad28dcd17511"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm_em, 40, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "9Y4MX-zeBs1u",
    "outputId": "8f51bfd4-a83b-4c31-9ec7-fc872ee7c813"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru_em, 40, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NvA_WhEg_iG5"
   },
   "source": [
    "## Doubling the no of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "dUYJKAtYBoef",
    "outputId": "273192e8-4fe8-4cf9-efff-17bd3f87e790"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn_em, 40, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IDyq_uNe_wYX"
   },
   "source": [
    "### For Vanilla RNN,  With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "WxwJGQApAQJj",
    "outputId": "87f2f9fa-746b-4926-c9a3-0cb3abf36e96"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm_em, 40, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdTST8ZiAWwa"
   },
   "source": [
    "### For LSTM, With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "nTtifvwGAXNl",
    "outputId": "30a07930-3fe6-474b-ebbd-46687a493f33"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru_em, 40, 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkTMhMc1Ac8f"
   },
   "source": [
    "### For GRU, With double the no of hidden units, the accuracy reduced from 74 to 73 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AY_wx8EM_3eq"
   },
   "source": [
    "# Halving the no of hidden units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "HN94EZfo_6L1",
    "outputId": "11c61176-c18f-4bd8-a738-9ad6c2a0ca64"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn_em, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OPDlfM7tADBF"
   },
   "source": [
    "### With halve the no of hidden units, the accuracy increased slightly from 73.8 to 74.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "vttKOmoP_xC7",
    "outputId": "cd2ccd9c-bcb5-4004-fdc0-2fb01e52a20f"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm_em, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-xy5cfYQAj_j"
   },
   "source": [
    "### For LSTM, With halve the no of hidden units, the accuracy reduced from 74 to 73 percent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "7dkRv2e_Aka8",
    "outputId": "d2305b52-7926-4669-bfdf-a5e384c358c9"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru_em, 40, 150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w4_lAZ_rAkzb"
   },
   "source": [
    "### For GRU, With halve the no of hidden units, the accuracy reduced from 74 to 73 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LNp_oho8BMtS"
   },
   "source": [
    "# Changing the max_length \n",
    "# Keeping no of hidden units = 300\n",
    "# epochs = 15\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZg_5ohNBQqZ"
   },
   "source": [
    "## Doubling the sequence length from 40 to 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "JsCbKK77BT2Y",
    "outputId": "adc5ad5d-c5c8-4eaf-fb11-94991650816e"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn_em, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrwMU7rXBXgR"
   },
   "source": [
    "### For vanilla rnn, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "AgG6XClUBX0N",
    "outputId": "8b2cec18-3f7d-4eb6-f855-841af8e5a93a"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm_em, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uX2xyVkCBYTx"
   },
   "source": [
    "### For LSTM, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "Agm_JZO4BY0K",
    "outputId": "8d10c26c-6063-49bd-ccbc-035e84628280"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru_em, 80, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tn1ht9nkBZGE"
   },
   "source": [
    "### For GRU, With double the sequence length, there was an insignificant increase in accuracy from 73.83 to 73.87\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "darw7z1oCaoX"
   },
   "source": [
    "### When we increase the sequence length we might include important words that would change the sentiment of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e42nuGgaB3c2"
   },
   "source": [
    "## Halving the sequence length from 40 to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "id": "wa3jFH6bBZhT",
    "outputId": "2589caca-9952-46cc-d0ed-b6a4b543c9df"
   },
   "outputs": [],
   "source": [
    "par_selec(model_vanilla_rnn_em, 20, 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EHoMYBMWkK_l"
   },
   "outputs": [],
   "source": [
    "# max_length = 20\n",
    "# epochs = 15\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "\n",
    "# encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "\n",
    "# encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "# adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "# model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "# history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "# accuracy = history.history['val_accuracy'][-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DLpxiHHjBZxm"
   },
   "source": [
    "### For vanilla RNN, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "3D0Ngk_zBaFa",
    "outputId": "fa7f3822-7f5b-4a4f-9595-067bb999cff0"
   },
   "outputs": [],
   "source": [
    "par_selec(model_lstm_em, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4R2ckx5MBaP5"
   },
   "source": [
    "### For LSTM, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "lulWk6EwBaaV",
    "outputId": "b361929a-0045-4fbf-fa51-702037b31302"
   },
   "outputs": [],
   "source": [
    "par_selec(model_gru_em, 20, 300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sQpFaM3VBaj1"
   },
   "source": [
    "### For GRU, With half the sequence length, there was a slight decrease in accuracy from 73.8 to 73.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuj2ablkCyQZ"
   },
   "source": [
    "\n",
    "### It makes sense as when we decrease the sequence length we might lose important words that change the sentiment of the sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goykoVf2Bate"
   },
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzFp427zDELR"
   },
   "source": [
    "## Fine tuning the word embeddings using mittens library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qclt65kfDJBD"
   },
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "  training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "  #training_tokens\n",
    "\n",
    "  oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "  print(len(oov))\n",
    "\n",
    "  corp_vocab = list(set(oov))\n",
    "\n",
    "  cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "  trr =''\n",
    "  for i in training_tokens:\n",
    "    for j in i:\n",
    "      trr+= j\n",
    "      trr += ' '\n",
    "\n",
    "  # print(trr)\n",
    "  # print(z)\n",
    "  X = cv.fit_transform([trr])\n",
    "  Xc = (X.T * X)\n",
    "  Xc.setdiag(0)\n",
    "  coocc_ar = Xc.toarray()\n",
    "\n",
    "  mittens_model = Mittens(n=300, max_iter=1000)\n",
    "\n",
    "  new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "  new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "  return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 147
    },
    "id": "rTxXFJy6DMLp",
    "outputId": "0f47e3a1-3efd-454b-a251-abd98855054f"
   },
   "outputs": [],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(training)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "0U7PZnWnDPE8",
    "outputId": "ccf30201-21ba-44a4-ac41-b1ef977e79ef"
   },
   "outputs": [],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_1MSXXvDU9k"
   },
   "source": [
    "### Using new embeddings for the vanilla rnn em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TMp1_4cVDbWR",
    "outputId": "2dc104ba-8c9b-4e91-ef98-b816e1ee0080"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre') \n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn_em(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round(accuracy,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_vanilla_rnn_em2 = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_vanilla_rnn_em2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj-i74diDUPT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Eb0o5YDrpZew",
    "outputId": "740f6524-74ff-47ce-891e-7794afe3ecaf"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    \n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_data = (validation_padded, validation['Sentiment']))\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = round(learning_rate,5)\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round((accuracy),2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,75)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_vanilla_rnn_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_vanilla_rnn_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DYFAm5c_pZey",
    "outputId": "6e49529d-392c-48eb-d548-74b47eca748a"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_lstm_em(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = learning_rate\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round((accuracy),2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,50)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_lstm_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_lstm_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Sc9KmobHpZez",
    "outputId": "9f757057-507c-4a3c-d7f3-5a5c4a2a33b7"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_gru_em(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    u = {}\n",
    "    u['max_len'] = max_length\n",
    "    u['batch'] = batch_size\n",
    "    u['l_rate'] = learning_rate\n",
    "    u['epochs'] = epochs\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize = (20,5))\n",
    "\n",
    "    ax[0].plot(history.history['loss'], label='Training')\n",
    "    ax[0].plot(history.history['val_loss'], c='r',label='Validation')\n",
    "    ax[0].legend(loc='upper left')\n",
    "    ax[0].title.set_text('Loss plot for the combination ' + str(u))\n",
    "\n",
    "\n",
    "    ax[1].plot(history.history['accuracy'], label='Training')\n",
    "    ax[1].plot(history.history['val_accuracy'], c='r',label='Validation')\n",
    "    ax[1].legend(loc='upper left')\n",
    "    ax[1].title.set_text('Accuracy plot for the combination ' + str(u))  \n",
    "    plt.show()\n",
    "\n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round((accuracy),2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,50)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_gru_em = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_gru_em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QsrKXRCpZe1"
   },
   "outputs": [],
   "source": [
    "# training.to_csv('Training Data.csv')\n",
    "# test.to_csv('Test Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BnjHcUEgpZe3"
   },
   "outputs": [],
   "source": [
    "# def generate_embedding_matrix(word_embeddings):\n",
    "#     vocabulary_size = len(tokenizer.word_index)+1\n",
    "#     embedding_matrix = np.zeros((vocabulary_size, 300))\n",
    "#     for word, index in tokenizer.word_index.items():\n",
    "#         embedding_vector = word_embeddings.get(word)\n",
    "#         if embedding_vector is not None:\n",
    "#             embedding_matrix[index] = embedding_vector\n",
    "#     return embedding_matrix,vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P1ELmHAipZe4"
   },
   "outputs": [],
   "source": [
    "# input_len = 50\n",
    "# tokenizer = Tokenizer(oov_token='<OOV>')\n",
    "# tokenizer.fit_on_texts(training['Doc Text'])\n",
    "# X_train_processed = tokenizer.texts_to_sequences(training['Doc Text'])\n",
    "# X_train_processed = pad_sequences(X_train_processed, padding='post', maxlen=input_len,truncating='pre')\n",
    "# embedding_matrix,vocabulary_size = generate_embedding_matrix(embeddings)\n",
    "# tf.keras.backend.clear_session()\n",
    "# model = Sequential()\n",
    "# embedding_layer = Embedding(vocabulary_size, 300, weights=[embedding_matrix], input_length=input_len,trainable = False,mask_zero=True)\n",
    "# model.add(embedding_layer)\n",
    "# model.add(SimpleRNN(300,kernel_regularizer = tf.keras.regularizers.L1(0.01)))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# # adam_optimizer = optimizers.adam(learning_rate=learning_rate)\n",
    "# model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# print(model.summary())\n",
    "# model.fit(X_train_processed, training['Sentiment'], batch_size=128, epochs=30, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_uZK2i9tpZe6"
   },
   "outputs": [],
   "source": [
    "def model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(SimpleRNN(units = 300, activation = activation, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sn7QKQzJpZe8",
    "outputId": "b1db28d5-17b5-463d-ae07-3851d1959ac9"
   },
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "epochs = 15\n",
    "learning_rate = 0.001\n",
    "batch_size = 64\n",
    "encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model = model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
    "accuracy = history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m_DL0cJypZe_"
   },
   "outputs": [],
   "source": [
    "def model_lstm_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 300, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(units = 300, activation = activation, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(1, activation='sigmoid', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "13APqbFlpZfA",
    "outputId": "eec2d02f-943b-40a1-d6f1-d09d5184f122"
   },
   "outputs": [],
   "source": [
    "max_length = 40\n",
    "epochs = 20\n",
    "learning_rate = 0.001\n",
    "batch_size = 128\n",
    "encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model = model_lstm_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
    "accuracy = history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YpnT_uzUpZfC",
    "outputId": "3b08c5e5-9bab-4db7-e9c0-331b8bc2e6bc"
   },
   "outputs": [],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "#     af_in_simpleRNN = args['af_in_simpleRNN']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "#     padd = args['padd']\n",
    "#     trunc = args['trunc']\n",
    "    epochs = args['epochs']\n",
    "#     kernel_regularizer_coef = args['kernel_regularizer_coef']\n",
    "#     activity_regularizer_coef = args['activity_regularizer_coef']\n",
    "#     Dropout_rate = args['Dropout_rate']\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(training['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings)\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn_bd(embedding_matrix, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "    \n",
    "    history = model.fit(training_padded, training['Sentiment'], epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True, validation_split = 0.25)\n",
    "    accuracy = history.history['val_accuracy'][-1]\n",
    "    \n",
    "    # summarize history for accuracy\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='upper left')\n",
    "    plt.show()    \n",
    "    \n",
    "#     encoded_validation = tokenise_tf.texts_to_sequences(validation['Doc Text'])\n",
    "\n",
    "#     validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding=padd, truncating = trunc)\n",
    "#     loss, accuracy = model.evaluate(validation_padded, validation['Sentiment'], verbose=0)\n",
    "    return -round((accuracy),2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,100)), \n",
    "#          'af_in_simpleRNN': hp.choice('af_in_simpleRNN', ['tanh', 'sigmoid']), \n",
    "        'batch_size': hp.choice('batch_size', [64, 128, 256]),\n",
    "#          'padd': hp.choice('padd', ['pre', 'post']),\n",
    "#          'trunc': hp.choice('trunc', ['pre', 'post']),\n",
    "         'epochs': hp.choice('epochs',range(5,20)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,1)\n",
    "#          'Dropout_rate': hp.uniform('Dropout_rate', 0, 1),\n",
    "#          'kernel_regularizer_coef': hp.uniform('kernel_regularizer_coef', 0, 10),\n",
    "#          'activity_regularizer_coef': hp.uniform('activity_regularizer_coef', 0, 10)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(training['Doc Text'])                                \n",
    "best_vanilla_rnn_bd = fmin(objective_func, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_vanilla_rnn_bd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6dn_G84lpZfD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_assignment1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
