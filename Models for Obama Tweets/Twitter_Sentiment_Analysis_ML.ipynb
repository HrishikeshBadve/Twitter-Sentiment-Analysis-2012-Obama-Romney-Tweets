{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ce569851-00f4-4d5e-885f-09017ba06d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "33c0b826-09cd-46a9-d064-b3d2e3f29dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "3daebdaa-b3bf-4e0e-db83-54f15377e157"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "e0d02290-c4fe-42de-a584-b148f9e7ea94"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "BALelbZyNU02"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "96eAfNsDaiVd"
   },
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "kljiyU8TaCiF"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2])\n",
    "# romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mcLnGHdPaCiI",
    "outputId": "a08bc803-0a20-4a28-f538-77929db4d5df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "1  obama debate cracker as cracker tonight tune t...  Positive\n",
       "2  miss point afraid understand big picture dont ...   Neutral\n",
       "3  raise democrat leave party year ago lifetime n...  Negative\n",
       "4  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "XORvLV2iaCiM"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = obama_train_pr_df.dropna()\n",
    "# romney_train_pr_df = romney_train_pr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting into TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "mXJwrtOtaCiQ"
   },
   "outputs": [],
   "source": [
    "def tf_idf(x):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(x)\n",
    "    #print(tfidf_matrix)\n",
    "    x1 = tfidf_matrix.toarray()\n",
    "#     print(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "hM-Q2KTBaCiU"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_tfidf = tf_idf(obama_train_pr_df['Doc Text'])\n",
    "# romney_train_pr_tfidf = tf_idf(romney_train_pr_df['Doc Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NimCgs87eUzF",
    "outputId": "c6ecaa8e-c556-4bb8-d907-fe29294689b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5470, 6938)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWPQASu9eUzG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm66hwR6eUzJ"
   },
   "source": [
    "### Cross Valid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "a6F0wDNPeUzJ"
   },
   "outputs": [],
   "source": [
    "def cross_valid(X,y,model):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(X_train1, temp)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Valid func with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_PCA(X,y,model, n):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        x_pca1 = pca.fit_transform(X_train1)\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(x_pca1, temp)\n",
    "        \n",
    "        x_pca_val = pca.transform(X_val)\n",
    "        y_pred = model.predict(x_pca_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhRyNUshSYd"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "-PbiFcztf_NY"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP6ste5OD_9a",
    "outputId": "2a7a71e4-4f73-4130-ec7d-c6c80e0f44e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # clf = SVC(kernel=\"rbf\", gamma=1, class_weight='balanced')\n",
    "# #     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
    "# #     f1 = mean(error) + stdev(error) \n",
    "# #print(f1)\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# # print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "\n",
    "# print(eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "JoDEYnJDhX6a",
    "outputId": "4721cafd-2b1b-47fd-a178-a47f10f631cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [01:03<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-b59078db638c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m \u001b[0mbest_classifier_LR\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_func_LR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_classifier_LR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             )\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-b59078db638c>\u001b[0m in \u001b[0;36mobjective_func_LR\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Neutral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobama_train_pr_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0meval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Negative\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0meval_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_pos'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_neu'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Neutral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_neg'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Negative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval_score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_score\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-52-51e315430fd9>\u001b[0m in \u001b[0;36mcross_valid\u001b[1;34m(X, y, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Positive'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Negative'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1405\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1406\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'processes'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1407\u001b[1;33m         fold_coefs_ = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[0;32m   1408\u001b[0m                                \u001b[1;33m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1409\u001b[0m             path_func(X, y, pos_class=class_, Cs=[C_],\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR(args):\n",
    "\n",
    "    C = args['C']\n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,10),\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.882777178013636, 'n': 200}                    \n",
      "{'accuracy': 0.561, 'f1_pos': 0.572, 'f1_neu': 0.517, 'f1_neg': 0.592, 'eval_score': 0.575}\n",
      "{'C': 1.2318460206237267, 'n': 400}                                 \n",
      "{'accuracy': 0.578, 'f1_pos': 0.597, 'f1_neu': 0.531, 'f1_neg': 0.606, 'eval_score': 0.594}\n",
      "{'C': 0.032559308210585725, 'n': 250}                               \n",
      "{'accuracy': 0.556, 'f1_pos': 0.562, 'f1_neu': 0.498, 'f1_neg': 0.599, 'eval_score': 0.572}\n",
      "{'C': 1.584830077996315, 'n': 100}                                  \n",
      "{'accuracy': 0.547, 'f1_pos': 0.547, 'f1_neu': 0.513, 'f1_neg': 0.578, 'eval_score': 0.557}\n",
      "{'C': 0.4926397918121812, 'n': 150}                                 \n",
      "{'accuracy': 0.559, 'f1_pos': 0.557, 'f1_neu': 0.526, 'f1_neg': 0.593, 'eval_score': 0.57}\n",
      "{'C': 0.5762716880453709, 'n': 450}                                 \n",
      "{'accuracy': 0.576, 'f1_pos': 0.586, 'f1_neu': 0.533, 'f1_neg': 0.61, 'eval_score': 0.591}\n",
      "{'C': 1.4182001918278502, 'n': 300}                                 \n",
      "{'accuracy': 0.562, 'f1_pos': 0.579, 'f1_neu': 0.522, 'f1_neg': 0.586, 'eval_score': 0.576}\n",
      "{'C': 0.05708461107547502, 'n': 450}                                \n",
      "{'accuracy': 0.563, 'f1_pos': 0.576, 'f1_neu': 0.508, 'f1_neg': 0.601, 'eval_score': 0.58}\n",
      "{'C': 1.8830569093967386, 'n': 450}                                 \n",
      "{'accuracy': 0.576, 'f1_pos': 0.588, 'f1_neu': 0.535, 'f1_neg': 0.604, 'eval_score': 0.589}\n",
      "{'C': 0.05789767175251437, 'n': 150}                                \n",
      "{'accuracy': 0.551, 'f1_pos': 0.551, 'f1_neu': 0.506, 'f1_neg': 0.59, 'eval_score': 0.564}\n",
      "{'C': 0.8796188858062899, 'n': 100}                                  \n",
      "{'accuracy': 0.551, 'f1_pos': 0.557, 'f1_neu': 0.514, 'f1_neg': 0.583, 'eval_score': 0.564}\n",
      "{'C': 0.5126366384812129, 'n': 350}                                  \n",
      "{'accuracy': 0.569, 'f1_pos': 0.584, 'f1_neu': 0.528, 'f1_neg': 0.596, 'eval_score': 0.583}\n",
      "{'C': 0.7471988583974043, 'n': 200}                                  \n",
      "{'accuracy': 0.565, 'f1_pos': 0.568, 'f1_neu': 0.53, 'f1_neg': 0.595, 'eval_score': 0.576}\n",
      "{'C': 1.2240324738526702, 'n': 100}                                  \n",
      "{'accuracy': 0.557, 'f1_pos': 0.563, 'f1_neu': 0.523, 'f1_neg': 0.584, 'eval_score': 0.568}\n",
      "{'C': 1.8048760530363073, 'n': 400}                                  \n",
      "{'accuracy': 0.572, 'f1_pos': 0.583, 'f1_neu': 0.531, 'f1_neg': 0.6, 'eval_score': 0.585}\n",
      "{'C': 0.7983454758561241, 'n': 300}                                  \n",
      "{'accuracy': 0.573, 'f1_pos': 0.586, 'f1_neu': 0.534, 'f1_neg': 0.597, 'eval_score': 0.585}\n",
      "{'C': 1.082408942309585, 'n': 100}                                   \n",
      "{'accuracy': 0.549, 'f1_pos': 0.557, 'f1_neu': 0.514, 'f1_neg': 0.576, 'eval_score': 0.561}\n",
      "{'C': 1.6767738353481527, 'n': 250}                                  \n",
      "{'accuracy': 0.564, 'f1_pos': 0.576, 'f1_neu': 0.524, 'f1_neg': 0.593, 'eval_score': 0.578}\n",
      "{'C': 0.007707062910848439, 'n': 300}                                \n",
      "{'accuracy': 0.555, 'f1_pos': 0.558, 'f1_neu': 0.49, 'f1_neg': 0.603, 'eval_score': 0.572}\n",
      "{'C': 1.440315911248508, 'n': 250}                                   \n",
      "{'accuracy': 0.564, 'f1_pos': 0.577, 'f1_neu': 0.524, 'f1_neg': 0.592, 'eval_score': 0.578}\n",
      "100%|██████████| 20/20 [08:58<00:00, 26.94s/trial, best loss: -0.594]\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR_PCA(args):\n",
    "\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    \n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    "\n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    par_dict = {'C': C, 'n': n}\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(par_dict)\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,2),\n",
    "         'n': hp.choice('n', np.arange(100, 500, step =50))\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR_PCA = fmin(objective_func_LR_PCA, space, algo=tpe.suggest, max_evals=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With Bayesian Optimization, different class weights and LR\n",
    "\n",
    "# def objective_func_LR(args):\n",
    "\n",
    "#     C = args['C']\n",
    "#     balance = args['balance']\n",
    "#     # multi_class = args['multi_class']\n",
    "#     # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "#     clf = LogisticRegression(C= C,class_weight=balance,n_jobs =-1)\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "#     eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "#     print(eval_dict)\n",
    "#     return -(eval_score)\n",
    "\n",
    "\n",
    "# space = {'C': hp.uniform('C', 0,10),\n",
    "#          'balance': hp.choice('balance', [{1:3, 2:1, 3:2}, {1:15, 2:1, 3:10}, {1:20, 2:5, 3:10}, {1:100, 2:10, 3:100}])\n",
    "#         # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "#         # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "#         #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "#         }                        \n",
    "                                \n",
    "                                \n",
    "# best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 1\n",
    "# n = 3\n",
    "# kernel = 'rbf'\n",
    "\n",
    "# par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "# print(par_dict)\n",
    "\n",
    "# clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print(eval_dict)\n",
    "# print('\\n')    \n",
    "# return -(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 2\n",
    "# n = 3\n",
    "# kernel = 'rbf'\n",
    "\n",
    "# par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "# print(par_dict)\n",
    "\n",
    "# clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print(eval_dict)\n",
    "# print('\\n')    \n",
    "# return -(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C = 3\n",
    "# n = 3\n",
    "# kernel = 'rbf'\n",
    "\n",
    "# par_dict = {'C': C, 'n': n, 'kernel': kernel}\n",
    "# print(par_dict)\n",
    "\n",
    "# clf = SVC(C= C, kernel = kernel, class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print(eval_dict)\n",
    "# print('\\n')    \n",
    "# return -(eval_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "58UghlAQeUzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 0.2892192714238556, 'n': 603, 'kernel': 'rbf'}  \n",
      "{'accuracy': 0.576, 'f1_pos': 0.565, 'f1_neu': 0.561, 'f1_neg': 0.6, 'eval_score': 0.58}\n",
      "{'C': 0.5151888680639615, 'n': 203, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.57, 'f1_pos': 0.578, 'f1_neu': 0.547, 'f1_neg': 0.588, 'eval_score': 0.579}\n",
      "{'C': 0.8193955283313585, 'n': 503, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.588, 'f1_pos': 0.591, 'f1_neu': 0.565, 'f1_neg': 0.611, 'eval_score': 0.597}\n",
      "{'C': 0.8798157958089573, 'n': 303, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.579, 'f1_pos': 0.588, 'f1_neu': 0.557, 'f1_neg': 0.592, 'eval_score': 0.586}\n",
      "{'C': 0.8809658735583068, 'n': 603, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.588, 'f1_pos': 0.597, 'f1_neu': 0.567, 'f1_neg': 0.602, 'eval_score': 0.596}\n",
      "{'C': 0.7960948785793291, 'n': 803, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.593, 'f1_pos': 0.599, 'f1_neu': 0.567, 'f1_neg': 0.615, 'eval_score': 0.602}\n",
      "{'C': 0.4684465835596907, 'n': 203, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.563, 'f1_pos': 0.571, 'f1_neu': 0.541, 'f1_neg': 0.578, 'eval_score': 0.571}\n",
      "{'C': 0.6717208157343634, 'n': 3, 'kernel': 'rbf'}                   \n",
      "{'accuracy': 0.472, 'f1_pos': 0.503, 'f1_neu': 0.387, 'f1_neg': 0.519, 'eval_score': 0.498}\n",
      "{'C': 0.4084134725933031, 'n': 203, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.565, 'f1_pos': 0.571, 'f1_neu': 0.541, 'f1_neg': 0.586, 'eval_score': 0.574}\n",
      "{'C': 0.7677672784430264, 'n': 403, 'kernel': 'rbf'}                \n",
      "{'accuracy': 0.58, 'f1_pos': 0.592, 'f1_neu': 0.556, 'f1_neg': 0.598, 'eval_score': 0.59}\n",
      "{'C': 0.8600057204519864, 'n': 803, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.581, 'f1_pos': 0.594, 'f1_neu': 0.558, 'f1_neg': 0.594, 'eval_score': 0.59}\n",
      "{'C': 0.9816763611379441, 'n': 603, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.585, 'f1_pos': 0.595, 'f1_neu': 0.562, 'f1_neg': 0.602, 'eval_score': 0.594}\n",
      "{'C': 0.7938175563330934, 'n': 203, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.575, 'f1_pos': 0.591, 'f1_neu': 0.544, 'f1_neg': 0.595, 'eval_score': 0.587}\n",
      "{'C': 0.3324088844216141, 'n': 603, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.567, 'f1_pos': 0.554, 'f1_neu': 0.552, 'f1_neg': 0.595, 'eval_score': 0.572}\n",
      "{'C': 0.7989714280291288, 'n': 603, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.586, 'f1_pos': 0.592, 'f1_neu': 0.567, 'f1_neg': 0.603, 'eval_score': 0.594}\n",
      "{'C': 0.8580270807040742, 'n': 603, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.584, 'f1_pos': 0.59, 'f1_neu': 0.556, 'f1_neg': 0.609, 'eval_score': 0.594}\n",
      "{'C': 0.19676751676508875, 'n': 803, 'kernel': 'rbf'}                 \n",
      "{'accuracy': 0.548, 'f1_pos': 0.515, 'f1_neu': 0.546, 'f1_neg': 0.572, 'eval_score': 0.545}\n",
      "{'C': 0.7293541876576741, 'n': 303, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.576, 'f1_pos': 0.584, 'f1_neu': 0.55, 'f1_neg': 0.595, 'eval_score': 0.585}\n",
      "{'C': 0.1256321161341699, 'n': 403, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.549, 'f1_pos': 0.542, 'f1_neu': 0.529, 'f1_neg': 0.574, 'eval_score': 0.555}\n",
      "{'C': 0.4115650248223637, 'n': 103, 'kernel': 'rbf'}                  \n",
      "{'accuracy': 0.542, 'f1_pos': 0.553, 'f1_neu': 0.511, 'f1_neg': 0.565, 'eval_score': 0.553}\n",
      "100%|██████████| 20/20 [35:58<00:00, 107.90s/trial, best loss: -0.602]\n",
      "{'C': 0.7960948785793291, 'n': 8}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and SVM\n",
    "\n",
    "def objective_func_SVM_PCA(args):\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "#     kernel = args['kernel']\n",
    "#     gamma = args['gamma']\n",
    "#     degree = args['degree']\n",
    "\n",
    "    par_dict = {'C': C, 'n': n, 'kernel': 'rbf'}\n",
    "    print(par_dict)\n",
    " \n",
    "    clf = SVC(C= C, kernel = 'rbf', class_weight='balanced')\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,1),\n",
    "         'n': hp.choice('n', np.arange(3, 1003, step =100))\n",
    "#         'kernel': hp.choice('kernel', ['poly', 'rbf']) \n",
    "#         'gamma': hp.choice('gamma',range(1,4)),\n",
    "#          'degree' : hp.choice('degree',range(1,4))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_SVM_PCA = fmin(objective_func_SVM_PCA, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_classifier_SVM_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{'C': 2.645341878923651, 'n': 3, 'kernel': 'rbf'}                       \n",
    "{'accuracy': 0.597, 'f1_pos': 0.607, 'f1_neu': 0.553, 'f1_neg': 0.631, 'eval_score': 0.612}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 19, 'metric': 'manhattan'}            \n",
      "{'accuracy': 0.366, 'f1_pos': 0.488, 'f1_neu': 0.329, 'f1_neg': 0.041, 'eval_score': 0.298}\n",
      "{'n_neighbors': 13, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.371, 'f1_pos': 0.49, 'f1_neu': 0.298, 'f1_neg': 0.107, 'eval_score': 0.323}\n",
      "{'n_neighbors': 16, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.505, 'f1_pos': 0.561, 'f1_neu': 0.426, 'f1_neg': 0.5, 'eval_score': 0.522}\n",
      "{'n_neighbors': 16, 'metric': 'manhattan'}                           \n",
      "{'accuracy': 0.362, 'f1_pos': 0.485, 'f1_neu': 0.306, 'f1_neg': 0.064, 'eval_score': 0.304}\n",
      "{'n_neighbors': 13, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.503, 'f1_pos': 0.561, 'f1_neu': 0.424, 'f1_neg': 0.497, 'eval_score': 0.52}\n",
      "{'n_neighbors': 1, 'metric': 'euclidean'}                            \n",
      "{'accuracy': 0.486, 'f1_pos': 0.517, 'f1_neu': 0.446, 'f1_neg': 0.492, 'eval_score': 0.498}\n",
      "{'n_neighbors': 1, 'metric': 'euclidean'}                            \n",
      "{'accuracy': 0.481, 'f1_pos': 0.514, 'f1_neu': 0.443, 'f1_neg': 0.484, 'eval_score': 0.493}\n",
      "{'n_neighbors': 28, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.512, 'f1_pos': 0.563, 'f1_neu': 0.41, 'f1_neg': 0.532, 'eval_score': 0.536}\n",
      "{'n_neighbors': 22, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.514, 'f1_pos': 0.57, 'f1_neu': 0.419, 'f1_neg': 0.523, 'eval_score': 0.536}\n",
      "{'n_neighbors': 16, 'metric': 'euclidean'}                           \n",
      "{'accuracy': 0.506, 'f1_pos': 0.56, 'f1_neu': 0.427, 'f1_neg': 0.505, 'eval_score': 0.524}\n",
      "100%|██████████| 10/10 [20:51<00:00, 125.18s/trial, best loss: -0.536]\n",
      "{'metric': 0, 'n_neighbors': 9}\n"
     ]
    }
   ],
   "source": [
    "# WIth Bayesian Optimization and KNN\n",
    "def objective_func_KNN(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']  \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,31, step =3)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"])\n",
    "        }\n",
    "\n",
    "best_classifier_KNN = fmin(objective_func_KNN, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_neighbors': 3, 'n': 33, 'metric': 'euclidean'}    \n",
      "{'accuracy': 0.462, 'f1_pos': 0.474, 'f1_neu': 0.458, 'f1_neg': 0.451, 'eval_score': 0.462}\n",
      "{'n_neighbors': 6, 'n': 3, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.439, 'f1_pos': 0.477, 'f1_neu': 0.406, 'f1_neg': 0.435, 'eval_score': 0.45}\n",
      "{'n_neighbors': 3, 'n': 13, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.451, 'f1_pos': 0.479, 'f1_neu': 0.431, 'f1_neg': 0.442, 'eval_score': 0.457}\n",
      "{'n_neighbors': 1, 'n': 53, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.462, 'f1_pos': 0.463, 'f1_neu': 0.456, 'f1_neg': 0.467, 'eval_score': 0.464}\n",
      "{'n_neighbors': 8, 'n': 28, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.488, 'f1_pos': 0.506, 'f1_neu': 0.492, 'f1_neg': 0.464, 'eval_score': 0.486}\n",
      "{'n_neighbors': 5, 'n': 8, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.452, 'f1_pos': 0.48, 'f1_neu': 0.437, 'f1_neg': 0.438, 'eval_score': 0.457}\n",
      "{'n_neighbors': 1, 'n': 38, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.456, 'f1_pos': 0.456, 'f1_neu': 0.453, 'f1_neg': 0.461, 'eval_score': 0.458}\n",
      "{'n_neighbors': 5, 'n': 23, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.473, 'f1_pos': 0.499, 'f1_neu': 0.477, 'f1_neg': 0.442, 'eval_score': 0.471}\n",
      "{'n_neighbors': 9, 'n': 18, 'metric': 'euclidean'}                  \n",
      "{'accuracy': 0.485, 'f1_pos': 0.483, 'f1_neu': 0.491, 'f1_neg': 0.478, 'eval_score': 0.482}\n",
      "{'n_neighbors': 2, 'n': 38, 'metric': 'manhattan'}                  \n",
      "{'accuracy': 0.433, 'f1_pos': 0.483, 'f1_neu': 0.441, 'f1_neg': 0.342, 'eval_score': 0.419}\n",
      "{'n_neighbors': 6, 'n': 58, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.479, 'f1_pos': 0.501, 'f1_neu': 0.488, 'f1_neg': 0.443, 'eval_score': 0.474}\n",
      "{'n_neighbors': 4, 'n': 48, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.469, 'f1_pos': 0.487, 'f1_neu': 0.487, 'f1_neg': 0.426, 'eval_score': 0.461}\n",
      "{'n_neighbors': 5, 'n': 48, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.472, 'f1_pos': 0.492, 'f1_neu': 0.485, 'f1_neg': 0.435, 'eval_score': 0.466}\n",
      "{'n_neighbors': 9, 'n': 53, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.481, 'f1_pos': 0.491, 'f1_neu': 0.494, 'f1_neg': 0.454, 'eval_score': 0.475}\n",
      "{'n_neighbors': 5, 'n': 33, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.469, 'f1_pos': 0.495, 'f1_neu': 0.469, 'f1_neg': 0.438, 'eval_score': 0.467}\n",
      "{'n_neighbors': 1, 'n': 53, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.454, 'f1_pos': 0.462, 'f1_neu': 0.445, 'f1_neg': 0.456, 'eval_score': 0.457}\n",
      "{'n_neighbors': 1, 'n': 33, 'metric': 'euclidean'}                   \n",
      "{'accuracy': 0.454, 'f1_pos': 0.454, 'f1_neu': 0.45, 'f1_neg': 0.456, 'eval_score': 0.455}\n",
      "{'n_neighbors': 3, 'n': 53, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.458, 'f1_pos': 0.486, 'f1_neu': 0.436, 'f1_neg': 0.449, 'eval_score': 0.464}\n",
      "{'n_neighbors': 1, 'n': 38, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.447, 'f1_pos': 0.448, 'f1_neu': 0.436, 'f1_neg': 0.456, 'eval_score': 0.45}\n",
      "{'n_neighbors': 8, 'n': 38, 'metric': 'manhattan'}                   \n",
      "{'accuracy': 0.478, 'f1_pos': 0.488, 'f1_neu': 0.493, 'f1_neg': 0.448, 'eval_score': 0.471}\n",
      "100%|██████████| 20/20 [03:00<00:00,  9.05s/trial, best loss: -0.486]\n"
     ]
    }
   ],
   "source": [
    "# WIth PCA , Bayesian Optimization and KNN\n",
    "def objective_func_KNN_PCA(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']\n",
    "    n = args['n']    \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'n': n, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,10, step =1)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"]),\n",
    "        'n': hp.choice('n', np.arange(3,63, step =5))}\n",
    "\n",
    "best_classifier_KNN_PCA = fmin(objective_func_KNN_PCA, space, algo=tpe.suggest, max_evals=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-z9cSEeUzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.467, 'f1_pos': 0.527, 'f1_neu': 0.34, 'f1_neg': 0.497, 'eval_score': 0.497}\n"
     ]
    }
   ],
   "source": [
    "clf = GaussianNB()\n",
    "accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 18}                                             \n",
      "{'accuracy': 0.459, 'f1_pos': 0.426, 'f1_neu': 0.317, 'f1_neg': 0.556, 'eval_score': 0.48}\n",
      "{'n': 28}                                                          \n",
      "{'accuracy': 0.472, 'f1_pos': 0.427, 'f1_neu': 0.374, 'f1_neg': 0.56, 'eval_score': 0.486}\n",
      "{'n': 28}                                                           \n",
      "{'accuracy': 0.463, 'f1_pos': 0.431, 'f1_neu': 0.359, 'f1_neg': 0.548, 'eval_score': 0.481}\n",
      "{'n': 18}                                                           \n",
      "{'accuracy': 0.458, 'f1_pos': 0.426, 'f1_neu': 0.308, 'f1_neg': 0.557, 'eval_score': 0.48}\n",
      "{'n': 3}                                                            \n",
      "{'accuracy': 0.407, 'f1_pos': 0.341, 'f1_neu': 0.143, 'f1_neg': 0.541, 'eval_score': 0.43}\n",
      "{'n': 28}                                                           \n",
      "{'accuracy': 0.465, 'f1_pos': 0.433, 'f1_neu': 0.38, 'f1_neg': 0.534, 'eval_score': 0.477}\n",
      "{'n': 28}                                                           \n",
      "{'accuracy': 0.472, 'f1_pos': 0.435, 'f1_neu': 0.382, 'f1_neg': 0.547, 'eval_score': 0.485}\n",
      "{'n': 8}                                                            \n",
      "{'accuracy': 0.45, 'f1_pos': 0.43, 'f1_neu': 0.243, 'f1_neg': 0.555, 'eval_score': 0.478}\n",
      "{'n': 28}                                                           \n",
      "{'accuracy': 0.465, 'f1_pos': 0.425, 'f1_neu': 0.374, 'f1_neg': 0.545, 'eval_score': 0.478}\n",
      "{'n': 43}                                                           \n",
      "{'accuracy': 0.48, 'f1_pos': 0.436, 'f1_neu': 0.431, 'f1_neg': 0.548, 'eval_score': 0.488}\n",
      "100%|██████████| 10/10 [01:02<00:00,  6.29s/trial, best loss: -0.488]\n"
     ]
    }
   ],
   "source": [
    "# With PCA and Naive bayes\n",
    "def objective_func_NB_PCA(args):\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = {'n': n}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n': hp.choice('n', np.arange(3,63, step =5))}\n",
    "\n",
    "best_classifier_NB_PCA = fmin(objective_func_NB_PCA, space, algo=tpe.suggest, max_evals=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uqUrTlhaCiX",
    "outputId": "46220e5f-4754-4177-f6df-fc32cf0fa883"
   },
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnu2mBLwaCie"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "r-6Q8dRwaCig",
    "outputId": "b42504a9-9d98-49fd-e286-297bb9fcd020"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 61, 'max_depth': 43, 'min_samples_split': 3, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.562, 'f1_pos': 0.56, 'f1_neu': 0.529, 'f1_neg': 0.595, 'eval_score': 0.572}\n",
      "{'n_estimators': 74, 'max_depth': 84, 'min_samples_split': 3, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.543, 'f1_neu': 0.518, 'f1_neg': 0.588, 'eval_score': 0.561}\n",
      "{'n_estimators': 74, 'max_depth': 96, 'min_samples_split': 8, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.56, 'f1_neu': 0.526, 'f1_neg': 0.599, 'eval_score': 0.574}\n",
      "{'n_estimators': 98, 'max_depth': 71, 'min_samples_split': 11, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.566, 'f1_pos': 0.558, 'f1_neu': 0.534, 'f1_neg': 0.603, 'eval_score': 0.576}\n",
      "{'n_estimators': 20, 'max_depth': 74, 'min_samples_split': 4, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.551, 'f1_neu': 0.516, 'f1_neg': 0.583, 'eval_score': 0.561}\n",
      "{'n_estimators': 80, 'max_depth': 30, 'min_samples_split': 8, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.538, 'f1_neu': 0.532, 'f1_neg': 0.607, 'eval_score': 0.569}\n",
      "{'n_estimators': 77, 'max_depth': 69, 'min_samples_split': 11, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.518, 'f1_neu': 0.532, 'f1_neg': 0.588, 'eval_score': 0.552}\n",
      "{'n_estimators': 99, 'max_depth': 80, 'min_samples_split': 5, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.568, 'f1_pos': 0.562, 'f1_neu': 0.536, 'f1_neg': 0.604, 'eval_score': 0.578}\n",
      "{'n_estimators': 20, 'max_depth': 53, 'min_samples_split': 5, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.553, 'f1_neu': 0.513, 'f1_neg': 0.58, 'eval_score': 0.561}\n",
      "{'n_estimators': 89, 'max_depth': 36, 'min_samples_split': 9, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.548, 'f1_neu': 0.528, 'f1_neg': 0.608, 'eval_score': 0.573}\n",
      "{'n_estimators': 44, 'max_depth': 22, 'min_samples_split': 11, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.485, 'f1_neu': 0.523, 'f1_neg': 0.577, 'eval_score': 0.532}\n",
      "{'n_estimators': 97, 'max_depth': 76, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.568, 'f1_pos': 0.569, 'f1_neu': 0.534, 'f1_neg': 0.6, 'eval_score': 0.579}\n",
      "{'n_estimators': 27, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.53, 'f1_neu': 0.536, 'f1_neg': 0.587, 'eval_score': 0.556}\n",
      "{'n_estimators': 73, 'max_depth': 68, 'min_samples_split': 8, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.564, 'f1_pos': 0.56, 'f1_neu': 0.525, 'f1_neg': 0.603, 'eval_score': 0.576}\n",
      "{'n_estimators': 60, 'max_depth': 31, 'min_samples_split': 2, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.539, 'f1_neu': 0.533, 'f1_neg': 0.596, 'eval_score': 0.564}\n",
      "{'n_estimators': 39, 'max_depth': 22, 'min_samples_split': 3, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.519, 'f1_neu': 0.514, 'f1_neg': 0.592, 'eval_score': 0.552}\n",
      "{'n_estimators': 95, 'max_depth': 80, 'min_samples_split': 9, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.512, 'f1_neu': 0.519, 'f1_neg': 0.592, 'eval_score': 0.55}\n",
      "{'n_estimators': 97, 'max_depth': 77, 'min_samples_split': 8, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.565, 'f1_pos': 0.57, 'f1_neu': 0.531, 'f1_neg': 0.594, 'eval_score': 0.576}\n",
      "{'n_estimators': 95, 'max_depth': 77, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.573, 'f1_pos': 0.571, 'f1_neu': 0.538, 'f1_neg': 0.607, 'eval_score': 0.584}\n",
      "{'n_estimators': 32, 'max_depth': 60, 'min_samples_split': 7, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.51, 'f1_neu': 0.498, 'f1_neg': 0.581, 'eval_score': 0.542}\n",
      "{'n_estimators': 97, 'max_depth': 76, 'min_samples_split': 6, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.538, 'f1_neu': 0.525, 'f1_neg': 0.594, 'eval_score': 0.562}\n",
      "{'n_estimators': 68, 'max_depth': 77, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.574, 'f1_pos': 0.578, 'f1_neu': 0.544, 'f1_neg': 0.6, 'eval_score': 0.584}\n",
      "{'n_estimators': 68, 'max_depth': 62, 'min_samples_split': 10, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.561, 'f1_pos': 0.561, 'f1_neu': 0.526, 'f1_neg': 0.597, 'eval_score': 0.573}\n",
      "{'n_estimators': 100, 'max_depth': 77, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.567, 'f1_pos': 0.573, 'f1_neu': 0.532, 'f1_neg': 0.596, 'eval_score': 0.579}\n",
      "{'n_estimators': 68, 'max_depth': 70, 'min_samples_split': 6, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.558, 'f1_pos': 0.56, 'f1_neu': 0.521, 'f1_neg': 0.591, 'eval_score': 0.57}\n",
      "100%|██████████| 25/25 [09:35<00:00, 23.04s/trial, best loss: -0.584]\n",
      "{'max_depth': 57, 'min_samples_leaf': 1, 'min_samples_split': 7, 'n_estimators': 75}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "\n",
    "    par_dict = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF = fmin(objective_func_RF, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "AI0VHzOleUzi",
    "outputId": "de0a927c-95c9-439d-d3ff-06cd57c0aab6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 183, 'n_estimators': 87, 'max_depth': 61, 'min_samples_split': 8, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.542, 'f1_neu': 0.526, 'f1_neg': 0.585, 'eval_score': 0.56}\n",
      "{'n': 163, 'n_estimators': 21, 'max_depth': 32, 'min_samples_split': 7, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.517, 'f1_pos': 0.515, 'f1_neu': 0.498, 'f1_neg': 0.537, 'eval_score': 0.523}\n",
      "{'n': 43, 'n_estimators': 93, 'max_depth': 51, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.56, 'f1_neu': 0.514, 'f1_neg': 0.565, 'eval_score': 0.557}\n",
      "{'n': 43, 'n_estimators': 76, 'max_depth': 53, 'min_samples_split': 7, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.535, 'f1_pos': 0.539, 'f1_neu': 0.497, 'f1_neg': 0.569, 'eval_score': 0.548}\n",
      "{'n': 183, 'n_estimators': 28, 'max_depth': 74, 'min_samples_split': 9, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.527, 'f1_pos': 0.533, 'f1_neu': 0.495, 'f1_neg': 0.552, 'eval_score': 0.537}\n",
      "{'n': 103, 'n_estimators': 22, 'max_depth': 87, 'min_samples_split': 5, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.538, 'f1_pos': 0.539, 'f1_neu': 0.512, 'f1_neg': 0.562, 'eval_score': 0.546}\n",
      "{'n': 143, 'n_estimators': 89, 'max_depth': 68, 'min_samples_split': 9, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.541, 'f1_neu': 0.53, 'f1_neg': 0.578, 'eval_score': 0.557}\n",
      "{'n': 163, 'n_estimators': 51, 'max_depth': 71, 'min_samples_split': 5, 'min_samples_leaf': 2}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.542, 'f1_neu': 0.515, 'f1_neg': 0.565, 'eval_score': 0.549}\n",
      "{'n': 63, 'n_estimators': 55, 'max_depth': 77, 'min_samples_split': 6, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.549, 'f1_neu': 0.505, 'f1_neg': 0.57, 'eval_score': 0.553}\n",
      "{'n': 3, 'n_estimators': 88, 'max_depth': 21, 'min_samples_split': 2, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.478, 'f1_pos': 0.509, 'f1_neu': 0.433, 'f1_neg': 0.497, 'eval_score': 0.495}\n",
      "{'n': 123, 'n_estimators': 53, 'max_depth': 33, 'min_samples_split': 7, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.552, 'f1_neu': 0.523, 'f1_neg': 0.57, 'eval_score': 0.557}\n",
      "{'n': 83, 'n_estimators': 82, 'max_depth': 54, 'min_samples_split': 8, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.548, 'f1_neu': 0.517, 'f1_neg': 0.568, 'eval_score': 0.553}\n",
      "{'n': 183, 'n_estimators': 82, 'max_depth': 89, 'min_samples_split': 2, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.554, 'f1_neu': 0.523, 'f1_neg': 0.576, 'eval_score': 0.56}\n",
      "{'n': 83, 'n_estimators': 26, 'max_depth': 74, 'min_samples_split': 2, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.536, 'f1_pos': 0.544, 'f1_neu': 0.507, 'f1_neg': 0.556, 'eval_score': 0.545}\n",
      "{'n': 123, 'n_estimators': 24, 'max_depth': 77, 'min_samples_split': 6, 'min_samples_leaf': 11}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.533, 'f1_neu': 0.501, 'f1_neg': 0.553, 'eval_score': 0.538}\n",
      "{'n': 163, 'n_estimators': 40, 'max_depth': 83, 'min_samples_split': 10, 'min_samples_leaf': 9}\n",
      "{'accuracy': 0.542, 'f1_pos': 0.537, 'f1_neu': 0.518, 'f1_neg': 0.569, 'eval_score': 0.549}\n",
      "{'n': 143, 'n_estimators': 25, 'max_depth': 78, 'min_samples_split': 10, 'min_samples_leaf': 10}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.525, 'f1_neu': 0.508, 'f1_neg': 0.563, 'eval_score': 0.54}\n",
      "{'n': 3, 'n_estimators': 35, 'max_depth': 63, 'min_samples_split': 4, 'min_samples_leaf': 1}\n",
      "{'accuracy': 0.455, 'f1_pos': 0.489, 'f1_neu': 0.408, 'f1_neg': 0.473, 'eval_score': 0.472}\n",
      "{'n': 163, 'n_estimators': 37, 'max_depth': 62, 'min_samples_split': 2, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.548, 'f1_pos': 0.545, 'f1_neu': 0.531, 'f1_neg': 0.567, 'eval_score': 0.553}\n",
      "{'n': 183, 'n_estimators': 68, 'max_depth': 74, 'min_samples_split': 7, 'min_samples_leaf': 3}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.553, 'f1_neu': 0.514, 'f1_neg': 0.572, 'eval_score': 0.557}\n",
      "{'n': 183, 'n_estimators': 41, 'max_depth': 50, 'min_samples_split': 8, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.537, 'f1_pos': 0.53, 'f1_neu': 0.506, 'f1_neg': 0.572, 'eval_score': 0.546}\n",
      "{'n': 23, 'n_estimators': 82, 'max_depth': 25, 'min_samples_split': 3, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.547, 'f1_neu': 0.496, 'f1_neg': 0.56, 'eval_score': 0.547}\n",
      "{'n': 183, 'n_estimators': 87, 'max_depth': 89, 'min_samples_split': 11, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.539, 'f1_neu': 0.519, 'f1_neg': 0.577, 'eval_score': 0.554}\n",
      "{'n': 183, 'n_estimators': 60, 'max_depth': 47, 'min_samples_split': 8, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.545, 'f1_neu': 0.52, 'f1_neg': 0.568, 'eval_score': 0.552}\n",
      "{'n': 183, 'n_estimators': 74, 'max_depth': 75, 'min_samples_split': 3, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.55, 'f1_pos': 0.546, 'f1_neu': 0.529, 'f1_neg': 0.573, 'eval_score': 0.556}\n",
      "{'n': 23, 'n_estimators': 90, 'max_depth': 89, 'min_samples_split': 4, 'min_samples_leaf': 7}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.552, 'f1_neu': 0.499, 'f1_neg': 0.554, 'eval_score': 0.547}\n",
      "{'n': 63, 'n_estimators': 87, 'max_depth': 39, 'min_samples_split': 11, 'min_samples_leaf': 5}\n",
      "{'accuracy': 0.536, 'f1_pos': 0.549, 'f1_neu': 0.497, 'f1_neg': 0.564, 'eval_score': 0.55}\n",
      "{'n': 103, 'n_estimators': 52, 'max_depth': 61, 'min_samples_split': 8, 'min_samples_leaf': 6}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.548, 'f1_neu': 0.513, 'f1_neg': 0.574, 'eval_score': 0.556}\n",
      "{'n': 183, 'n_estimators': 62, 'max_depth': 61, 'min_samples_split': 2, 'min_samples_leaf': 4}\n",
      "{'accuracy': 0.549, 'f1_pos': 0.557, 'f1_neu': 0.524, 'f1_neg': 0.569, 'eval_score': 0.558}\n",
      "{'n': 183, 'n_estimators': 80, 'max_depth': 69, 'min_samples_split': 8, 'min_samples_leaf': 8}\n",
      "{'accuracy': 0.546, 'f1_pos': 0.542, 'f1_neu': 0.518, 'f1_neg': 0.576, 'eval_score': 0.555}\n",
      "100%|██████████| 30/30 [08:03<00:00, 16.11s/trial, best loss: -0.56]\n",
      "{'max_depth': 41, 'min_samples_leaf': 3, 'min_samples_split': 6, 'n': 9, 'n_estimators': 67}\n"
     ]
    }
   ],
   "source": [
    "# With PCA,  Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF_PCA(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = { 'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF_PCA = fmin(objective_func_RF_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_RF_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKP0NEj6eUzk"
   },
   "outputs": [],
   "source": [
    "# # WITH LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from math import sqrt\n",
    "# lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "# x_lda = lda.fit_transform(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQsPlUENeUzl",
    "outputId": "7a8a84e7-88c0-4bab-bf29-611d0cc98d98"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "#     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda, temp, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzCfxZ1GeUzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Y5ahpseUzp"
   },
   "outputs": [],
   "source": [
    "# obama_train2_pr_tfidf, obama_val_pr_tfidf, obama_train2_pr_class, obama_val_pr_class = train_test_split(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SUBRUJ8eUzq"
   },
   "outputs": [],
   "source": [
    "# x_lda2 = lda.fit_transform(obama_train2_pr_tfidf, obama_train2_pr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cz5e2KzeUzs",
    "outputId": "9d0f0738-440f-4c25-dcf2-79fc3f0dce22"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "# #     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda2, obama_train2_pr_class, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm0j2MPOeUzt"
   },
   "outputs": [],
   "source": [
    "# x_val_lda = lda.transform(obama_val_pr_tfidf)\n",
    "# x_val_lda_df = pd.DataFrame(data = x_val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkTc0libeUzy",
    "outputId": "e52d6004-e6f8-46bd-863b-90bb91bba6be"
   },
   "outputs": [],
   "source": [
    "# c = lambda b : 1 if b == 0 else b\n",
    "# d = lambda b : 2 if b <= 1 else b\n",
    "# bc = RandomForestClassifier(max_depth = c(best_classifier['max_depth']), min_samples_leaf = c(best_classifier['min_samples_leaf']), min_samples_split = d(best_classifier['min_samples_split'])\n",
    "#                             ,n_estimators = best_classifier['n_estimators'])\n",
    "\n",
    "# bc.fit(x_lda2, obama_train2_pr_class)\n",
    "\n",
    "# y_pred = bc.predict(x_val_lda)\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDPpRJ91eUz0",
    "outputId": "7338ec62-8947-4d33-a1ef-f2e2eee8843d"
   },
   "outputs": [],
   "source": [
    "# lda_clf = lda.fit(obama_train2_pr_tfidf, obama_train2_pr_class)\n",
    "# y_pred_lda_clf = lda_clf.predict(obama_val_pr_tfidf)\n",
    "# # y_pred_lda_clf\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred_lda_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBsD7WYFeUz4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "0UB056v0eUz6"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'boosting_type': 'goss', 'n_estimators': 170, 'max_depth': 7, 'learning_rate': 0.06041058628039278, 'subsample': 0.8666666666666667, 'min_split_gain': 4}\n",
      "{'accuracy': 0.525, 'f1_pos': 0.551, 'f1_neu': 0.481, 'f1_neg': 0.542, 'eval_score': 0.539}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 219, 'max_depth': 8, 'learning_rate': 0.115917144381938, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.531, 'f1_pos': 0.557, 'f1_neu': 0.466, 'f1_neg': 0.565, 'eval_score': 0.551}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 281, 'max_depth': 8, 'learning_rate': 0.10018895580687695, 'subsample': 0.9333333333333333, 'min_split_gain': 4}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.561, 'f1_neu': 0.453, 'f1_neg': 0.566, 'eval_score': 0.552}\n",
      "{'boosting_type': 'goss', 'n_estimators': 441, 'max_depth': 5, 'learning_rate': 0.11625015294913124, 'subsample': 1.0, 'min_split_gain': 0}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.538, 'f1_neu': 0.477, 'f1_neg': 0.533, 'eval_score': 0.529}\n",
      "{'boosting_type': 'dart', 'n_estimators': 284, 'max_depth': 4, 'learning_rate': 0.14386899686794283, 'subsample': 0.8666666666666667, 'min_split_gain': 4}\n",
      "{'accuracy': 0.535, 'f1_pos': 0.56, 'f1_neu': 0.466, 'f1_neg': 0.573, 'eval_score': 0.556}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 385, 'max_depth': 4, 'learning_rate': 0.11481816400770731, 'subsample': 1.0, 'min_split_gain': 2}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.567, 'f1_neu': 0.485, 'f1_neg': 0.577, 'eval_score': 0.562}\n",
      "{'boosting_type': 'goss', 'n_estimators': 468, 'max_depth': 4, 'learning_rate': 0.0565707419699016, 'subsample': 1.0, 'min_split_gain': 4}\n",
      "{'accuracy': 0.523, 'f1_pos': 0.546, 'f1_neu': 0.483, 'f1_neg': 0.541, 'eval_score': 0.537}\n",
      "{'boosting_type': 'goss', 'n_estimators': 142, 'max_depth': 9, 'learning_rate': 0.09601784767786954, 'subsample': 0.8, 'min_split_gain': 4}\n",
      "{'accuracy': 0.522, 'f1_pos': 0.547, 'f1_neu': 0.486, 'f1_neg': 0.534, 'eval_score': 0.534}\n",
      "{'boosting_type': 'dart', 'n_estimators': 237, 'max_depth': 6, 'learning_rate': 0.19499589920896007, 'subsample': 0.8666666666666667, 'min_split_gain': 0}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.571, 'f1_neu': 0.522, 'f1_neg': 0.563, 'eval_score': 0.562}\n",
      "{'boosting_type': 'goss', 'n_estimators': 217, 'max_depth': 8, 'learning_rate': 0.19592257227626822, 'subsample': 1.0, 'min_split_gain': 5}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.525, 'f1_neu': 0.463, 'f1_neg': 0.529, 'eval_score': 0.52}\n",
      "{'boosting_type': 'dart', 'n_estimators': 284, 'max_depth': 5, 'learning_rate': 0.14150577038154274, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.547, 'f1_pos': 0.572, 'f1_neu': 0.502, 'f1_neg': 0.569, 'eval_score': 0.563}\n",
      "{'boosting_type': 'goss', 'n_estimators': 107, 'max_depth': 4, 'learning_rate': 0.06673314162896074, 'subsample': 0.8, 'min_split_gain': 5}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.542, 'f1_neu': 0.462, 'f1_neg': 0.544, 'eval_score': 0.534}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 266, 'max_depth': 9, 'learning_rate': 0.18244090014904624, 'subsample': 1.0, 'min_split_gain': 2}\n",
      "{'accuracy': 0.542, 'f1_pos': 0.561, 'f1_neu': 0.5, 'f1_neg': 0.564, 'eval_score': 0.556}\n",
      "{'boosting_type': 'goss', 'n_estimators': 100, 'max_depth': 6, 'learning_rate': 0.15495214722404574, 'subsample': 0.8666666666666667, 'min_split_gain': 3}\n",
      "{'accuracy': 0.518, 'f1_pos': 0.548, 'f1_neu': 0.477, 'f1_neg': 0.53, 'eval_score': 0.532}\n",
      "{'boosting_type': 'goss', 'n_estimators': 285, 'max_depth': 4, 'learning_rate': 0.09306160519107548, 'subsample': 0.8, 'min_split_gain': 0}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.553, 'f1_neu': 0.485, 'f1_neg': 0.548, 'eval_score': 0.543}\n",
      "{'boosting_type': 'dart', 'n_estimators': 261, 'max_depth': 8, 'learning_rate': 0.17666940423469357, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.543, 'f1_pos': 0.568, 'f1_neu': 0.498, 'f1_neg': 0.563, 'eval_score': 0.558}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 390, 'max_depth': 4, 'learning_rate': 0.08097067697397729, 'subsample': 0.8666666666666667, 'min_split_gain': 4}\n",
      "{'accuracy': 0.535, 'f1_pos': 0.56, 'f1_neu': 0.463, 'f1_neg': 0.575, 'eval_score': 0.557}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 260, 'max_depth': 4, 'learning_rate': 0.009172455859492513, 'subsample': 0.8, 'min_split_gain': 1}\n",
      "{'accuracy': 0.512, 'f1_pos': 0.548, 'f1_neu': 0.427, 'f1_neg': 0.549, 'eval_score': 0.536}\n",
      "{'boosting_type': 'goss', 'n_estimators': 57, 'max_depth': 8, 'learning_rate': 0.07700172633553706, 'subsample': 0.8, 'min_split_gain': 5}\n",
      "{'accuracy': 0.512, 'f1_pos': 0.547, 'f1_neu': 0.457, 'f1_neg': 0.53, 'eval_score': 0.53}\n",
      "{'boosting_type': 'goss', 'n_estimators': 357, 'max_depth': 8, 'learning_rate': 0.14367066061922623, 'subsample': 0.8, 'min_split_gain': 0}\n",
      "{'accuracy': 0.51, 'f1_pos': 0.533, 'f1_neu': 0.47, 'f1_neg': 0.528, 'eval_score': 0.524}\n",
      "{'boosting_type': 'dart', 'n_estimators': 110, 'max_depth': 5, 'learning_rate': 0.16849339728407128, 'subsample': 0.9333333333333333, 'min_split_gain': 3}\n",
      "{'accuracy': 0.539, 'f1_pos': 0.561, 'f1_neu': 0.479, 'f1_neg': 0.573, 'eval_score': 0.558}\n",
      "{'boosting_type': 'dart', 'n_estimators': 147, 'max_depth': 3, 'learning_rate': 0.194601224838643, 'subsample': 0.9333333333333333, 'min_split_gain': 1}\n",
      "{'accuracy': 0.536, 'f1_pos': 0.565, 'f1_neu': 0.473, 'f1_neg': 0.566, 'eval_score': 0.556}\n",
      "{'boosting_type': 'gbdt', 'n_estimators': 144, 'max_depth': 5, 'learning_rate': 0.02937994929726749, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.55, 'f1_neu': 0.456, 'f1_neg': 0.572, 'eval_score': 0.55}\n",
      "{'boosting_type': 'dart', 'n_estimators': 406, 'max_depth': 3, 'learning_rate': 0.12301349326381474, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.569, 'f1_neu': 0.493, 'f1_neg': 0.571, 'eval_score': 0.562}\n",
      "{'boosting_type': 'dart', 'n_estimators': 84, 'max_depth': 3, 'learning_rate': 0.13436688706646716, 'subsample': 0.9333333333333333, 'min_split_gain': 2}\n",
      "{'accuracy': 0.526, 'f1_pos': 0.551, 'f1_neu': 0.452, 'f1_neg': 0.566, 'eval_score': 0.548}\n",
      "100%|██████████| 25/25 [01:48<00:00,  4.34s/trial, best loss: -0.563]\n",
      "{'boosting_type': 1, 'learning_rate': 0.14150577038154274, 'max_depth': 2, 'min_split_gain': 2, 'n_estimators': 234, 'subsample': 2}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "    subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "\n",
    "    par_dict = {'boosting_type': boosting_type, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': subsample, 'min_split_gain': min_split_gain}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=subsample, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,501, step =1)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "     'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB = fmin(objective_func_LGB, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n': 103, 'n_estimators': 63, 'max_depth': 8, 'learning_rate': 0.16383257483998023, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.524, 'f1_pos': 0.537, 'f1_neu': 0.495, 'f1_neg': 0.541, 'eval_score': 0.534}\n",
      "{'n': 143, 'n_estimators': 238, 'max_depth': 8, 'learning_rate': 0.007000130544041383, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.552, 'f1_neu': 0.497, 'f1_neg': 0.551, 'eval_score': 0.545}\n",
      "{'n': 143, 'n_estimators': 310, 'max_depth': 7, 'learning_rate': 0.08240929053331739, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.57, 'f1_neu': 0.523, 'f1_neg': 0.574, 'eval_score': 0.566}\n",
      "{'n': 123, 'n_estimators': 281, 'max_depth': 7, 'learning_rate': 0.1944260906828903, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.539, 'f1_neu': 0.507, 'f1_neg': 0.559, 'eval_score': 0.544}\n",
      "{'n': 23, 'n_estimators': 220, 'max_depth': 6, 'learning_rate': 0.17933083294492252, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.5, 'f1_pos': 0.51, 'f1_neu': 0.481, 'f1_neg': 0.509, 'eval_score': 0.506}\n",
      "{'n': 123, 'n_estimators': 57, 'max_depth': 5, 'learning_rate': 0.08541725413474424, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.564, 'f1_neu': 0.494, 'f1_neg': 0.544, 'eval_score': 0.547}\n",
      "{'n': 83, 'n_estimators': 324, 'max_depth': 3, 'learning_rate': 0.07434661733391844, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.539, 'f1_pos': 0.555, 'f1_neu': 0.504, 'f1_neg': 0.557, 'eval_score': 0.55}\n",
      "{'n': 183, 'n_estimators': 362, 'max_depth': 5, 'learning_rate': 0.005564792484479253, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.519, 'f1_pos': 0.548, 'f1_neu': 0.476, 'f1_neg': 0.535, 'eval_score': 0.534}\n",
      "{'n': 103, 'n_estimators': 303, 'max_depth': 9, 'learning_rate': 0.01251999321591537, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.552, 'f1_pos': 0.561, 'f1_neu': 0.52, 'f1_neg': 0.576, 'eval_score': 0.563}\n",
      "{'n': 3, 'n_estimators': 274, 'max_depth': 9, 'learning_rate': 0.1767751466299161, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.48, 'f1_pos': 0.536, 'f1_neu': 0.423, 'f1_neg': 0.481, 'eval_score': 0.499}\n",
      "{'n': 183, 'n_estimators': 423, 'max_depth': 4, 'learning_rate': 0.10269308779037521, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.578, 'f1_neu': 0.529, 'f1_neg': 0.583, 'eval_score': 0.575}\n",
      "{'n': 103, 'n_estimators': 130, 'max_depth': 8, 'learning_rate': 0.13450292048701473, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.542, 'f1_pos': 0.55, 'f1_neu': 0.511, 'f1_neg': 0.564, 'eval_score': 0.552}\n",
      "{'n': 163, 'n_estimators': 131, 'max_depth': 9, 'learning_rate': 0.1847786378174954, 'subsample': 1, 'min_split_gain': 4}\n",
      "{'accuracy': 0.551, 'f1_pos': 0.568, 'f1_neu': 0.52, 'f1_neg': 0.566, 'eval_score': 0.562}\n",
      "{'n': 83, 'n_estimators': 349, 'max_depth': 5, 'learning_rate': 0.014990937277015594, 'subsample': 1, 'min_split_gain': 2}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.563, 'f1_neu': 0.503, 'f1_neg': 0.565, 'eval_score': 0.557}\n",
      "{'n': 123, 'n_estimators': 367, 'max_depth': 5, 'learning_rate': 0.04872170243991039, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.557, 'f1_neu': 0.516, 'f1_neg': 0.561, 'eval_score': 0.554}\n",
      "{'n': 123, 'n_estimators': 317, 'max_depth': 8, 'learning_rate': 0.1378776684363382, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.556, 'f1_pos': 0.564, 'f1_neu': 0.532, 'f1_neg': 0.574, 'eval_score': 0.565}\n",
      "{'n': 163, 'n_estimators': 457, 'max_depth': 7, 'learning_rate': 0.014410421859797419, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.542, 'f1_pos': 0.567, 'f1_neu': 0.503, 'f1_neg': 0.556, 'eval_score': 0.555}\n",
      "{'n': 183, 'n_estimators': 190, 'max_depth': 5, 'learning_rate': 0.022311160548053533, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.545, 'f1_pos': 0.561, 'f1_neu': 0.513, 'f1_neg': 0.564, 'eval_score': 0.557}\n",
      "{'n': 63, 'n_estimators': 107, 'max_depth': 5, 'learning_rate': 0.13457800940919615, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.515, 'f1_pos': 0.532, 'f1_neu': 0.492, 'f1_neg': 0.523, 'eval_score': 0.523}\n",
      "{'n': 83, 'n_estimators': 143, 'max_depth': 8, 'learning_rate': 0.08140204003782721, 'subsample': 1, 'min_split_gain': 5}\n",
      "{'accuracy': 0.53, 'f1_pos': 0.552, 'f1_neu': 0.49, 'f1_neg': 0.549, 'eval_score': 0.544}\n",
      "{'n': 143, 'n_estimators': 350, 'max_depth': 4, 'learning_rate': 0.10842934628865775, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.547, 'f1_neu': 0.512, 'f1_neg': 0.562, 'eval_score': 0.55}\n",
      "{'n': 43, 'n_estimators': 176, 'max_depth': 4, 'learning_rate': 0.05422677429277256, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.533, 'f1_pos': 0.553, 'f1_neu': 0.499, 'f1_neg': 0.548, 'eval_score': 0.545}\n",
      "{'n': 183, 'n_estimators': 423, 'max_depth': 7, 'learning_rate': 0.11231408953567633, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.556, 'f1_neu': 0.505, 'f1_neg': 0.562, 'eval_score': 0.553}\n",
      "{'n': 143, 'n_estimators': 410, 'max_depth': 4, 'learning_rate': 0.05506213686210195, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.553, 'f1_pos': 0.563, 'f1_neu': 0.523, 'f1_neg': 0.575, 'eval_score': 0.564}\n",
      "{'n': 43, 'n_estimators': 310, 'max_depth': 3, 'learning_rate': 0.0969545292022026, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.528, 'f1_pos': 0.545, 'f1_neu': 0.5, 'f1_neg': 0.54, 'eval_score': 0.538}\n",
      "{'n': 3, 'n_estimators': 382, 'max_depth': 4, 'learning_rate': 0.12488957709391987, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.474, 'f1_pos': 0.521, 'f1_neu': 0.423, 'f1_neg': 0.479, 'eval_score': 0.491}\n",
      "{'n': 23, 'n_estimators': 173, 'max_depth': 6, 'learning_rate': 0.15094721571229297, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.532, 'f1_pos': 0.558, 'f1_neu': 0.488, 'f1_neg': 0.551, 'eval_score': 0.547}\n",
      "{'n': 183, 'n_estimators': 411, 'max_depth': 7, 'learning_rate': 0.037682659698052325, 'subsample': 1, 'min_split_gain': 3}\n",
      "{'accuracy': 0.559, 'f1_pos': 0.579, 'f1_neu': 0.53, 'f1_neg': 0.571, 'eval_score': 0.57}\n",
      "{'n': 183, 'n_estimators': 191, 'max_depth': 4, 'learning_rate': 0.044435516916856724, 'subsample': 1, 'min_split_gain': 1}\n",
      "{'accuracy': 0.544, 'f1_pos': 0.565, 'f1_neu': 0.516, 'f1_neg': 0.551, 'eval_score': 0.553}\n",
      "{'n': 183, 'n_estimators': 494, 'max_depth': 7, 'learning_rate': 0.028565771393559758, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.563, 'f1_pos': 0.561, 'f1_neu': 0.54, 'f1_neg': 0.589, 'eval_score': 0.571}\n",
      "{'n': 183, 'n_estimators': 494, 'max_depth': 3, 'learning_rate': 0.06345494214294103, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.556, 'f1_pos': 0.571, 'f1_neu': 0.533, 'f1_neg': 0.568, 'eval_score': 0.565}\n",
      "{'n': 63, 'n_estimators': 495, 'max_depth': 7, 'learning_rate': 0.1598984692340466, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.54, 'f1_pos': 0.548, 'f1_neu': 0.508, 'f1_neg': 0.563, 'eval_score': 0.55}\n",
      "{'n': 183, 'n_estimators': 118, 'max_depth': 4, 'learning_rate': 0.029895404478695462, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.534, 'f1_pos': 0.556, 'f1_neu': 0.497, 'f1_neg': 0.551, 'eval_score': 0.547}\n",
      "{'n': 183, 'n_estimators': 368, 'max_depth': 6, 'learning_rate': 0.10074621688540689, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.56, 'f1_pos': 0.556, 'f1_neu': 0.535, 'f1_neg': 0.588, 'eval_score': 0.568}\n",
      "{'n': 23, 'n_estimators': 398, 'max_depth': 7, 'learning_rate': 0.06931373517634437, 'subsample': 1, 'min_split_gain': 0}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.546, 'f1_neu': 0.499, 'f1_neg': 0.544, 'eval_score': 0.54}\n",
      "100%|██████████| 35/35 [17:05<00:00, 29.30s/trial, best loss: -0.575]\n",
      "{'boosting_type': 1, 'learning_rate': 0.10269308779037521, 'max_depth': 1, 'min_split_gain': 1, 'n': 9, 'n_estimators': 373}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB_PCA(args):\n",
    "    boosting_type= args['boosting_type']\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "#     subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "    n = args['n']\n",
    "    \n",
    "    par_dict = {'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': 1, 'min_split_gain': min_split_gain} \n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(boosting_type = boosting_type, class_weight  = 'balanced', num_leaves = 2**max_depth, n_estimators = n_estimators, max_depth = max_depth, subsample=1, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'boosting_type' : hp.choice('boosting_type', ['gbdt', 'dart', 'goss']),\n",
    "    'n_estimators': hp.choice('n_estimators', np.arange(50,501, step =1)),\n",
    "    'learning_rate':  hp.uniform('learning_rate', 0,0.2), \n",
    "    'max_depth': hp.choice('max_depth', np.arange(3, 10, step =1)),   \n",
    "#      'subsample': hp.choice('subsample', np.linspace(0.8, 1, num=4)),\n",
    "    'min_split_gain': hp.choice('min_split_gain' , np.arange(0,6, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB_PCA = fmin(objective_func_LGB_PCA, space, algo=tpe.suggest, max_evals=35)\n",
    "print(best_classifier_LGB_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 76, 'max_depth': 91, 'min_samples_split': 6, 'min_samples_leaf': 4}\n",
      "  0%|          | 0/30 [33:53<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-8c981e33a7c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mbest_classifier_GB\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobjective_func_GB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_classifier_GB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             )\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-8c981e33a7c5>\u001b[0m in \u001b[0;36mobjective_func_GB\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_leaf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_samples_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Neutral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Negative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_valid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobama_train_pr_tfidf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mobama_train_pr_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentiment'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[0meval_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf1_Negative\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[0meval_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_pos'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Positive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_neu'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Neutral\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'f1_neg'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf1_Negative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'eval_score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0meval_score\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-51e315430fd9>\u001b[0m in \u001b[0;36mcross_valid\u001b[1;34m(X, y, model)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Positive'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'Negative'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    496\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[1;31m# fit the boosting stages\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 498\u001b[1;33m         n_stages = self._fit_stages(\n\u001b[0m\u001b[0;32m    499\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m             \u001b[1;31m# fit next stage of trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m             raw_predictions = self._fit_stage(\n\u001b[0m\u001b[0;32m    556\u001b[0m                 \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m                 random_state, X_idx_sorted, X_csc, X_csr)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0m\u001b[0;32m    212\u001b[0m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1240\u001b[0m         \"\"\"\n\u001b[0;32m   1241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1242\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1243\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1244\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 375\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    376\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and GB\n",
    "\n",
    "def objective_func_GB(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "\n",
    "    par_dict = { 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = GradientBoostingClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_GB= fmin(objective_func_GB, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PCA, Bayesian Optimization, and GB\n",
    "\n",
    "def objective_func_GB(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = { 'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,203, step =20))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF_PCA = fmin(objective_func_RF_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_RF_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt3WGISKa9_U"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "romney_train_pr_tfidf_smote, romney_train_pr_class = sm.fit_sample(romney_train_pr_tfidf, romney_train_pr_df['Sentiment'] )\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "obama_train_pr_tfidf_smote, obama_train_pr_class = sm.fit_sample(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
