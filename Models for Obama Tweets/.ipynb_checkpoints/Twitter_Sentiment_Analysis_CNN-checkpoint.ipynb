{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ec00867a-7c6d-49bf-accc-f948d3859e04"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "e305700f-495b-4a9c-c72a-8ffb460266d2"
   },
   "outputs": [],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "698e7631-906c-41e2-8f8c-57f0217bbb7a"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "from statistics import mean, stdev, median, mode\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from mittens import GloVe, Mittens\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Conv3D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import PReLU\n",
    "from tensorflow.keras.layers import MaxPool3D, AveragePooling3D\n",
    "\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import AveragePooling3D\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "a0b2d8da-0ebb-4d47-c6ca-1398fdd3a7ed"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BALelbZyNU02",
    "outputId": "c87872db-d3e0-44f8-b6a0-9fa931a3715e"
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "### Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training Data.csv'), usecols = [1,2]).dropna()\n",
    "obama_val_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Validation Data.csv'), usecols = [1,2]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "AD9U00CVddNC"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>obama need b remove dangerous white house czar...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sidenote obama profile sexy back debate</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hillary take bullet obama n buck still fall</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>morgan freeman obama best commercial ever need...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bitch like obama bitch want food stamp lmao</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  obama need b remove dangerous white house czar...  Negative\n",
       "1            sidenote obama profile sexy back debate  Positive\n",
       "2        hillary take bullet obama n buck still fall  Negative\n",
       "3  morgan freeman obama best commercial ever need...  Positive\n",
       "4        bitch like obama bitch want food stamp lmao   Neutral"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hell obama can not even take care one border p...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama save auto industry enough say team obama</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>great obama hope take today sport section cheat</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>appreciate obama capitalisizes fact black</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ann romney michelle obama dress second debate ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  hell obama can not even take care one border p...  Negative\n",
       "1     obama save auto industry enough say team obama  Positive\n",
       "2    great obama hope take today sport section cheat  Negative\n",
       "3          appreciate obama capitalisizes fact black  Negative\n",
       "4  ann romney michelle obama dress second debate ...   Neutral"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_val_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "obama_train1_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2]).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "1  obama debate cracker as cracker tonight tune t...  Positive\n",
       "2  miss point afraid understand big picture dont ...   Neutral\n",
       "3  raise democrat leave party year ago lifetime n...  Negative\n",
       "4  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train1_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Building CNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "FiqzWfYwa-ir"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "# embedded_sequences = embedding_layer(int_sequences_input)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(embedded_sequences)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.MaxPooling1D(5)(x)\n",
    "# x = layers.Conv1D(128, 5, activation=\"relu\")(x)\n",
    "# x = layers.GlobalMaxPooling1D()(x)\n",
    "# x = layers.Dense(128, activation=\"relu\")(x)\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "# preds = layers.Dense(len(class_names), activation=\"softmax\")(x)\n",
    "# model = keras.Model(int_sequences_input, preds)\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Nt4Q-LLu_Nh6"
   },
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    data = np.asarray(data)\n",
    "    temp = np.zeros((len(data),3))\n",
    "#     print(data[0])\n",
    "    for i in range(len(temp)):\n",
    "        if data[i] == 'Negative':\n",
    "            temp[i][2] = 1 ## Negative sentiment third neuron\n",
    "        elif data[i] == 'Neutral':\n",
    "            temp[i][1] = 1 ## Neutral sentiment second neuron  \n",
    "        else:\n",
    "            temp[i][0] = 1 ## Positive sentiment first neuron \n",
    "\n",
    "    return temp\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "ETvx1WoKfrfk"
   },
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    temp = []\n",
    "    for i in x:\n",
    "        m = np.argmax(i)\n",
    "        if m == 0:\n",
    "            temp.append('Positive')\n",
    "        elif m == 1:\n",
    "            temp.append('Neutral')\n",
    "        else:\n",
    "            temp.append('Negative')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmGshHslpZeH"
   },
   "source": [
    "## Building Glove Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "YU-SxaOZ_Nh9"
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or3hmQvdpZeJ"
   },
   "source": [
    "## Embedding Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "wBG3Kq7EpZeJ"
   },
   "outputs": [],
   "source": [
    "def emb_matrix(t,embeddings, we_dim):\n",
    "    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n",
    "    vocab_size = len(t.word_index) + 1    \n",
    "    embedding_matrix = zeros((vocab_size, we_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kT9ZAXi_NiC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgK6d3Qy_NiF"
   },
   "source": [
    "## Fine tuning the word embeddings of 300 dimensions using mittens library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqwJmPT-_NiF"
   },
   "source": [
    "## Used the code for finetuning from the following link:\n",
    "### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "HRwhIKRT_NiF"
   },
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "    #training_tokens\n",
    "\n",
    "    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "    print(len(oov))\n",
    "\n",
    "    corp_vocab = list(set(oov))\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "    trr =''\n",
    "    for i in training_tokens:\n",
    "        for j in i:\n",
    "            trr+= j\n",
    "            trr += ' '\n",
    "\n",
    "    # print(trr)\n",
    "    # print(z)\n",
    "    X = cv.fit_transform([trr])\n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    coocc_ar = Xc.toarray()\n",
    "\n",
    "    mittens_model = Mittens(n=200, max_iter=len(oov)+200)\n",
    "\n",
    "    new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "    return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB2Nd-Kr_NiH",
    "outputId": "4773e17a-16dd-4855-a490-a5fa416fa28b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "Iteration 760: loss: 0.010516015812754631"
     ]
    }
   ],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(obama_train_pr_df)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spGvX_Cg_NiI",
    "outputId": "7efc2436-a554-40f2-8aa5-6a0932bf3d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir2jPLOS_NiK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3VcHx_CduFW"
   },
   "source": [
    "## Using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "t9xXo3kBdnDc"
   },
   "outputs": [],
   "source": [
    "def cross_valid_cnn(X,y,epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "\n",
    "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        tokenise_tf = Tokenizer()\n",
    "        tokenise_tf.fit_on_texts(X_train1) \n",
    "    \n",
    "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
    "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
    "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "        embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "        int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "        embedded_sequences = embedding_layer(int_sequences_input)\n",
    "        x = layers.Conv1D(128, 3)(embedded_sequences)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "        x = layers.Conv1D(128, 3)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "        x = layers.Conv1D(128, 3)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU()(x)\n",
    "        x = layers.GlobalMaxPooling1D()(x)\n",
    "        # x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        x = layers.Dense(128, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "        preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "        model = tf.keras.Model(int_sequences_input, preds)\n",
    "        # print(model.summary())\n",
    "        # print(z)\n",
    "        model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        y_pred_temp = model.predict(validation_padded)\n",
    "        y_pred = pred(y_pred_temp)\n",
    "        f1_temp = f1_score(y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[2])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[0])\n",
    "\n",
    "        acc.append(round(accuracy_score(y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "ZQ_WOJe96xsa"
   },
   "outputs": [],
   "source": [
    "def model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(64, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(128, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(256, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "    # x = layers.MaxPooling1D(2)(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "0-q5ICwzkjY8"
   },
   "outputs": [],
   "source": [
    "def model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(32, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.MaxPooling1D(2)(x)\n",
    "    x = layers.Conv1D(64, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ixABj5x5kj33"
   },
   "outputs": [],
   "source": [
    "def model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim):\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "    embedding_layer = Embedding(vocab_size, we_dim, embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),trainable=False)\n",
    "    int_sequences_input = layers.Input(shape=(None,), dtype=\"int64\")\n",
    "    embedded_sequences = embedding_layer(int_sequences_input)\n",
    "\n",
    "    x = layers.Conv1D(64, 3)(embedded_sequences)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = layers.GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"tanh\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    preds = layers.Dense(3, activation=\"softmax\")(x)\n",
    "\n",
    "    model = tf.keras.Model(int_sequences_input, preds)\n",
    "    # print(model.summary())\n",
    "    # print(z)\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "JebvbtfskGBZ"
   },
   "outputs": [],
   "source": [
    "# max_length = 31\n",
    "# epochs = 2\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "# we_dim = 200\n",
    "# par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "# print(par_dict)\n",
    "\n",
    "# tokenise_tf = Tokenizer()\n",
    "# tokenise_tf.fit_on_texts(romney_train_pr_df['Doc Text']) \n",
    "\n",
    "# encoded_train = tokenise_tf.texts_to_sequences(romney_train_pr_df['Doc Text'])\n",
    "# training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "# embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "# encoded_validation = tokenise_tf.texts_to_sequences(romney_val_pr_df['Doc Text'])\n",
    "# validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "# try: \n",
    "#   model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#   history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "# except:\n",
    "#   try: \n",
    "#     model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "#   except:\n",
    "\n",
    "#     model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     history = model.fit(training_padded, one_hot(romney_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "\n",
    "# y_pred_temp = model.predict(validation_padded)\n",
    "# y_pred = pred(y_pred_temp)\n",
    "# f1_temp = f1_score(romney_val_pr_df['Sentiment'], y_pred, average = None)\n",
    "\n",
    "# f1_Positive = f1_temp[2]\n",
    "# f1_Neutral = f1_temp[1]\n",
    "# f1_Negative = f1_temp[0]\n",
    "\n",
    "# accuracy = round(accuracy_score(romney_val_pr_df['Sentiment'], y_pred),3) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# print('\\n')\n",
    "# print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Cl2Z9PW8CEy",
    "outputId": "52564d69-79f7-460e-a7e1-35c4ad983a5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_length': 25, 'batch_size': 128, 'learning_rate': 0.007716406556550836, 'epochs': 11}\n",
      "{'accuracy': 0.523, 'f1_pos': 0.503, 'f1_neu': 0.517, 'f1_neg': 0.547, 'eval_score': 0.524}\n",
      "{'max_length': 43, 'batch_size': 128, 'learning_rate': 0.008876353984141813, 'epochs': 22}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.514, 'f1_neu': 0.503, 'f1_neg': 0.504, 'eval_score': 0.508}\n",
      "{'max_length': 56, 'batch_size': 128, 'learning_rate': 0.0063845762649313445, 'epochs': 7}\n",
      "{'accuracy': 0.541, 'f1_pos': 0.555, 'f1_neu': 0.479, 'f1_neg': 0.59, 'eval_score': 0.562}\n",
      "{'max_length': 38, 'batch_size': 64, 'learning_rate': 0.00933631742826214, 'epochs': 22}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.602, 'f1_neu': 0.463, 'f1_neg': 0.454, 'eval_score': 0.524}\n",
      "{'max_length': 47, 'batch_size': 64, 'learning_rate': 0.0075821287597799426, 'epochs': 15}\n",
      "{'accuracy': 0.529, 'f1_pos': 0.547, 'f1_neu': 0.442, 'f1_neg': 0.58, 'eval_score': 0.552}\n",
      "{'max_length': 41, 'batch_size': 32, 'learning_rate': 0.005839951047673778, 'epochs': 9}\n",
      "{'accuracy': 0.56, 'f1_pos': 0.586, 'f1_neu': 0.493, 'f1_neg': 0.597, 'eval_score': 0.581}\n",
      "{'max_length': 42, 'batch_size': 128, 'learning_rate': 0.008142195819721901, 'epochs': 12}\n",
      "{'accuracy': 0.51, 'f1_pos': 0.454, 'f1_neu': 0.445, 'f1_neg': 0.589, 'eval_score': 0.518}\n",
      "{'max_length': 31, 'batch_size': 128, 'learning_rate': 0.003878967582088312, 'epochs': 25}\n",
      "{'accuracy': 0.511, 'f1_pos': 0.568, 'f1_neu': 0.411, 'f1_neg': 0.532, 'eval_score': 0.537}\n",
      "{'max_length': 22, 'batch_size': 32, 'learning_rate': 0.007023742626801312, 'epochs': 12}\n",
      "{'accuracy': 0.506, 'f1_pos': 0.503, 'f1_neu': 0.482, 'f1_neg': 0.535, 'eval_score': 0.515}\n",
      "{'max_length': 56, 'batch_size': 32, 'learning_rate': 0.0030626773047653256, 'epochs': 7}\n",
      "{'accuracy': 0.547, 'f1_pos': 0.604, 'f1_neu': 0.403, 'f1_neg': 0.595, 'eval_score': 0.582}\n",
      "{'max_length': 41, 'batch_size': 32, 'learning_rate': 0.008196299307356015, 'epochs': 26}\n",
      "{'accuracy': 0.52, 'f1_pos': 0.567, 'f1_neu': 0.518, 'f1_neg': 0.476, 'eval_score': 0.521}\n",
      "{'max_length': 19, 'batch_size': 32, 'learning_rate': 0.008558437074789543, 'epochs': 22}\n",
      "{'accuracy': 0.52, 'f1_pos': 0.542, 'f1_neu': 0.439, 'f1_neg': 0.572, 'eval_score': 0.545}\n",
      "{'max_length': 19, 'batch_size': 64, 'learning_rate': 0.003374122335295826, 'epochs': 6}\n",
      "{'accuracy': 0.493, 'f1_pos': 0.58, 'f1_neu': 0.366, 'f1_neg': 0.518, 'eval_score': 0.53}\n",
      "{'max_length': 49, 'batch_size': 32, 'learning_rate': 0.004398141834033248, 'epochs': 7}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.575, 'f1_neu': 0.449, 'f1_neg': 0.505, 'eval_score': 0.532}\n",
      "{'max_length': 32, 'batch_size': 32, 'learning_rate': 0.0022945788309202473, 'epochs': 15}\n",
      "{'accuracy': 0.497, 'f1_pos': 0.445, 'f1_neu': 0.477, 'f1_neg': 0.543, 'eval_score': 0.495}\n",
      "{'max_length': 4, 'batch_size': 32, 'learning_rate': 0.0028246814928072797, 'epochs': 20}\n",
      "{'accuracy': 0.471, 'f1_pos': 0.447, 'f1_neu': 0.457, 'f1_neg': 0.508, 'eval_score': 0.475}\n",
      "{'max_length': 20, 'batch_size': 64, 'learning_rate': 0.0050343347418224296, 'epochs': 21}\n",
      "{'accuracy': 0.521, 'f1_pos': 0.561, 'f1_neu': 0.448, 'f1_neg': 0.547, 'eval_score': 0.543}\n",
      "{'max_length': 23, 'batch_size': 128, 'learning_rate': 0.0009521620152417466, 'epochs': 23}\n",
      "{'accuracy': 0.509, 'f1_pos': 0.529, 'f1_neu': 0.451, 'f1_neg': 0.549, 'eval_score': 0.529}\n",
      "{'max_length': 50, 'batch_size': 128, 'learning_rate': 0.008469328945600844, 'epochs': 22}\n",
      "{'accuracy': 0.537, 'f1_pos': 0.556, 'f1_neu': 0.476, 'f1_neg': 0.571, 'eval_score': 0.555}\n",
      "{'max_length': 12, 'batch_size': 32, 'learning_rate': 0.005777382633112505, 'epochs': 18}\n",
      "{'accuracy': 0.499, 'f1_pos': 0.476, 'f1_neu': 0.434, 'f1_neg': 0.564, 'eval_score': 0.513}\n",
      "100%|██████████| 20/20 [08:44<00:00, 26.24s/trial, best loss: -0.582]\n",
      "{'batch_size': 0, 'epochs': 2, 'learning_rate': 0.0030626773047653256, 'max_length': 52}\n"
     ]
    }
   ],
   "source": [
    "def objective_func_CNN(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "    print(par_dict)\n",
    "\n",
    "    tokenise_tf = Tokenizer()\n",
    "    tokenise_tf.fit_on_texts(obama_train_pr_df['Doc Text']) \n",
    "\n",
    "    encoded_train = tokenise_tf.texts_to_sequences(obama_train_pr_df['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings, we_dim)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    try: \n",
    "        model = model_cnn1(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "        history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "    except:\n",
    "        try:            \n",
    "            model = model_cnn2(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "            history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        except:\n",
    "\n",
    "            model = model_cnn3(embedding_matrix, vocab_size, epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "            history = model.fit(training_padded, one_hot(obama_train_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "\n",
    "\n",
    "    y_pred_temp = model.predict(validation_padded)\n",
    "    y_pred = pred(y_pred_temp)\n",
    "    f1_temp = np.round(f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None),3)\n",
    "\n",
    "    f1_Positive = f1_temp[2]\n",
    "    f1_Neutral = f1_temp[1]\n",
    "    f1_Negative = f1_temp[0]\n",
    "\n",
    "    accuracy = round(accuracy_score(obama_val_pr_df['Sentiment'], y_pred),3) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)   \n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print('\\n')\n",
    "    print(eval_dict)\n",
    "\n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.01)\n",
    "        }                  \n",
    "                                    \n",
    "we_dim = 200                               \n",
    "best_CNN = fmin(objective_func_CNN, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_length': 31,\n",
       " 'batch_size': 128,\n",
       " 'learning_rate': 0.00043843544096825674,\n",
       " 'epochs': 7}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'max_length': 31, 'batch_size': 128, 'learning_rate': 0.00043843544096825674, 'epochs': 7}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "Hfm558S9d2RH"
   },
   "outputs": [],
   "source": [
    "# max_length = 30\n",
    "# epochs = 20\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "# we_dim = 200\n",
    "\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "# f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "# print('\\n')\n",
    "# print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "X0w0bGDBrC9x"
   },
   "outputs": [],
   "source": [
    "# def objective_func_CNN_CV(args):\n",
    "#     max_length = args['max_length']\n",
    "#     batch_size = args['batch_size']\n",
    "#     learning_rate = args['learning_rate']\n",
    "#     epochs = args['epochs']\n",
    "\n",
    "#     par_dict = {'max_length': max_length, 'batch_size': batch_size, 'learning_rate': learning_rate, 'epochs': epochs}\n",
    "#     print(par_dict)\n",
    "\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative= cross_valid_cnn(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, learning_rate, we_dim)\n",
    "#     f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "#     print('\\n')\n",
    "#     print(eval_dict)\n",
    "\n",
    "#     return -(f1)\n",
    "\n",
    "# space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "#         'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "#          'epochs': hp.choice('epochs',range(5,30)), \n",
    "#          'learning_rate': hp.uniform('learning_rate', 0,0.01)\n",
    "#         }                  \n",
    "                                    \n",
    "# we_dim = 200                               \n",
    "# best_CNN_CV = fmin(objective_func_CNN_CV, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_CNN_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5FhdoX1kd2Y4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldH6dPIM_NiL"
   },
   "source": [
    "## Custom F1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "9Q5Q1HVs_NiM"
   },
   "outputs": [],
   "source": [
    "def f1_value(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_CNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
