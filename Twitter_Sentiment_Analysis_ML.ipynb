{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "ce569851-00f4-4d5e-885f-09017ba06d94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "33c0b826-09cd-46a9-d064-b3d2e3f29dd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "3daebdaa-b3bf-4e0e-db83-54f15377e157"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mittens\n",
      "  Downloading https://files.pythonhosted.org/packages/ce/c0/6e4fce5b3cb88edde2e657bb4da9885c0aeac232981706beed7f43773b00/mittens-0.2-py3-none-any.whl\n",
      "Requirement already satisfied, skipping upgrade: numpy in /usr/local/lib/python3.6/dist-packages (from mittens) (1.18.5)\n",
      "Installing collected packages: mittens\n",
      "Successfully installed mittens-0.2\n"
     ]
    }
   ],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "e0d02290-c4fe-42de-a584-b148f9e7ea94"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "BALelbZyNU02"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CCe7BteZpZdj"
   },
   "outputs": [],
   "source": [
    "# # Load your favorite word2vec model\n",
    "# cont = Contractions('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "# text = \"we're\"\n",
    "# text = list(cont.expand_texts([text], precise=True))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBMsohUhpZdl"
   },
   "outputs": [],
   "source": [
    "def conv_dataframes(pos_path, neg_path):\n",
    "    with open(pos_path,'r',encoding='latin1') as f:\n",
    "        data_p = f.readlines()\n",
    "#     print(data_p[11])\n",
    "    with open(neg_path,'r',encoding='latin1') as f:\n",
    "        data_n = f.readlines()\n",
    "    pos_data = shuffle(pd.DataFrame(data_p, columns = [\"Doc Text\"]))\n",
    "#     pos_data['Sentiment'] = 1\n",
    "#     pos_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    neg_data = shuffle(pd.DataFrame(data_n, columns = [\"Doc Text\"]))\n",
    "#     neg_data['Sentiment'] = -1\n",
    "#     neg_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    return pos_data, neg_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hr-TspWpZdn"
   },
   "source": [
    "## The code for pos tagging and lemmatize sentence is fron the following link:\n",
    " ### https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmfT_d2dpZdn"
   },
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJso8xCYpZdq"
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     print(wordnet_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#         if tag is not None:\n",
    "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eioAEw_bpZdr"
   },
   "outputs": [],
   "source": [
    "# print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "# print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "# print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "# print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0yxaZ3DHpZdt"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    # This method replaces two or more consecutive letters with the same character to something shorter. For example, gooooooood becomes good.\n",
    "    def replaceTwoOrMore(s):\n",
    "        #look for 2 or more repetitions of character\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "        return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "    # This method converts camel cased words into space delimited words.\n",
    "    # For example: ThisIsASentence will be changed to This Is A Sentence\n",
    "    def convertCamelCase(word):\n",
    "        return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",word)\n",
    "\n",
    "    # Read a flat file containing some abbreviations and their expansions in pipe separated format\n",
    "    # Use these abbreviations to replace text in the tweets as part of Preprocessing\n",
    "    \n",
    "    def readAbbrFile(abb_path):\n",
    "        global abbr_dict\n",
    "        abbr_dict ={}\n",
    "        f = open(abb_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            tmp = i.split('|')\n",
    "            abbr_dict[tmp[0]] = tmp[1]\n",
    "\n",
    "        return abbr_dict\n",
    "  \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replaceAbbr(s):\n",
    "#         temp =[]\n",
    "#         for word in s.split():\n",
    "#             if word.lower() in abbr_dict.keys():\n",
    "# #                 print('t')                \n",
    "#                 temp.append(abbr_dict[word.lower()])\n",
    "#             else:\n",
    "#                 temp.append(word)\n",
    "        temp = \" \".join([abbr_dict[word.lower()] if word.lower() in abbr_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end\n",
    "\n",
    "    def readcontractions(contra_path):\n",
    "        global contra_dict\n",
    "        contra_dict ={}\n",
    "        f = open(contra_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            try: \n",
    "                tmp = i.replace('\"', '').replace(',', '').replace('\\n', ' ').split(':')\n",
    "                contra_dict[tmp[0]] = tmp[1]\n",
    "            except:\n",
    "                print(tmp)\n",
    "                print(z)\n",
    "\n",
    "        return contra_dict\n",
    "    \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replacecontra(s):\n",
    "        temp = \" \".join([contra_dict[word.lower()] if word.lower() in contra_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end    \n",
    "    \n",
    "\n",
    "    abb_path = os.path.join(data_path,\"abbrevations.txt\")\n",
    "    abbr_dict = readAbbrFile(abb_path)\n",
    "    \n",
    "    contra_path = os.path.join(data_path,\"contractions.txt\")\n",
    "    contra_dict = readcontractions(contra_path)\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: x.lower())\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceAbbr(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replacecontra(x))\n",
    "\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[<>!#@$:.,%\\?-]+', ' ') #remove punctuation and special characters\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n\\n', ' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n', ' ') # removing new line characters\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[^\\w\\s]',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+[a-zA-Z]\\s+',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\^[a-zA-Z]\\s+',' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+',' ')  \n",
    "\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceTwoOrMore(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: convertCamelCase(x))\n",
    "\n",
    "    # Remove stop words from text\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_list]))\n",
    "    \n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(y) for y in x.split()]))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7duAi2av_Ng-",
    "outputId": "df5b9919-8587-4983-8e8a-3d25d94e71fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k k  k'"
      ]
     },
     "execution_count": 15,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = 'k k  k'\n",
    "st.replace('\\s+','')\n",
    "st.replace('\\n\\n', '')\n",
    "st.replace('[^\\w\\s]','')\n",
    "st.replace('\\s+[a-zA-Z]\\s+','')\n",
    "st.replace('\\^[a-zA-Z]\\s+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgyAcMZRpZdv"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data1, data2):\n",
    "    temp1 = text_preprocessing(data1)\n",
    "    temp2 = text_preprocessing(data2)\n",
    "#     temp_pos2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_pos['Doc Text']])\n",
    "#     temp_neg2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_neg['Doc Text']]) \n",
    "#     temp_pos[\"Doc Text Tokens\"] = temp_pos2 \n",
    "#     temp_neg[\"Doc Text Tokens\"] = temp_neg2\n",
    "    return temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcHqAMoZeUyG"
   },
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "id": "YcIgGnBHpZd1",
    "outputId": "185586a2-72e7-46f6-8858-9c66f561fdad"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-255fb4ac6532>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mr'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrainFile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"training-Obama-Romney-tweets.xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mobama_train_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Obama'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mromney_train_temp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainFile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msheet_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Romney'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskiprows\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0musecols\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Doc Text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 )\n\u001b[1;32m    295\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, comment, skipfooter, convert_float, mangle_dupe_cols)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0mio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         raise ValueError(\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_buffer, engine)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engines\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_io\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__fspath__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0merr_msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Install xlrd >= 1.0.0 for Excel support\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mimport_optional_dependency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"xlrd\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextra\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m    351\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 353\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    354\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/excel/_xlrd.py\u001b[0m in \u001b[0;36mload_workbook\u001b[0;34m(self, filepath_or_buffer)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_contents\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen_workbook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/xlrd/__init__.py\u001b[0m in \u001b[0;36mopen_workbook\u001b[0;34m(filename, logfile, verbosity, use_mmap, file_contents, encoding_override, formatting_info, on_demand, ragged_rows)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_contents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mpeek\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpeeksz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpeek\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34mb\"PK\\x03\\x04\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# a ZIP file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\kalya\\\\OneDrive - University of Illinois at Chicago\\\\!UIC\\\\!Semesters\\\\3rd Sem\\\\CS 583 Data Mining and Text Mining\\\\Research Project\\\\Data/training-Obama-Romney-tweets.xlsx'"
     ]
    }
   ],
   "source": [
    "\n",
    "trainFile = os.path.join(data_path,\"training-Obama-Romney-tweets.xlsx\")\n",
    "obama_train_temp = pd.read_excel(trainFile, sheet_name = 'Obama', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "romney_train_temp = pd.read_excel(trainFile, sheet_name = 'Romney', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Aoo2NCbGpZd3"
   },
   "outputs": [],
   "source": [
    "obama_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ESw9o5h7_NhH"
   },
   "outputs": [],
   "source": [
    "romney_train_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3fSFxgW_NhI"
   },
   "source": [
    "## Removing datapoints with mixed sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W-c-9PNk_NhJ"
   },
   "outputs": [],
   "source": [
    "obama_train = obama_train_temp[obama_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "obama_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8-cpEizR_NhL"
   },
   "outputs": [],
   "source": [
    "romney_train = romney_train_temp[romney_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "romney_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ0Xu4VZ_NhN"
   },
   "source": [
    "# Dropping missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6WQ-4zw_NhO"
   },
   "outputs": [],
   "source": [
    "obama_train = obama_train.dropna()\n",
    "romney_train = romney_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epYfA3Vg_NhQ"
   },
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gW_pnLsm_NhQ",
    "outputId": "8c8f3c85-5317-474d-a0a8-a82f5c280779"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1922\n",
       " 0    1895\n",
       " 1    1653\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 71,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_DA = obama_train['Sentiment'].value_counts()\n",
    "obama_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RE1FsDeu_NhT",
    "outputId": "273ddcd9-24eb-4537-9c26-4b2c4fab26ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 72,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = obama_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T1_gyDU9_NhV",
    "outputId": "4ca273ed-1f9e-4d6e-c9fb-a55f743de3fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2893\n",
       " 0    1680\n",
       " 1    1075\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_DA = romney_train['Sentiment'].value_counts()\n",
    "romney_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Alvh5jk_NhW",
    "outputId": "aeb017c0-982e-4669-aab5-f8781e9a77ee"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = romney_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LagE7x_6_NhY"
   },
   "source": [
    "# Romney data is very imbalanced\n",
    "# So, its better to select the best model with respect to F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05yRKXM4_NhY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j9EdPb9_Nha"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qsh4tjZk_Nhb"
   },
   "outputs": [],
   "source": [
    "# hm_lines = 5331\n",
    "\n",
    "# tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# spell = Speller(lang='en')\n",
    "# # spell = SpellChecker()\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list.extend(['rt', 'retweet', 'e'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZD3LoJ1_Nhe"
   },
   "outputs": [],
   "source": [
    "obama_train_pr, romney_train_pr = create_vocab(obama_train, romney_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dt3d1UQV_Nhg",
    "outputId": "7e1f41e2-e776-4a8b-a822-74d9076321af"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...         0\n",
       "2  obama debate cracker as cracker tonight tune t...         1\n",
       "4  miss point afraid understand big picture dont ...         0\n",
       "6  raise democrat leave party year ago lifetime n...        -1\n",
       "7  obama camp can not afford low expectation toni...         0"
      ]
     },
     "execution_count": 77,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dIaA7p2_Nhh"
   },
   "outputs": [],
   "source": [
    "obama_train_pr['Sentiment'] = obama_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvFTC2P0_Nhj",
    "outputId": "db43eeb5-5682-47b0-b9d7-08261a27422a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "2  obama debate cracker as cracker tonight tune t...  Positive\n",
       "4  miss point afraid understand big picture dont ...   Neutral\n",
       "6  raise democrat leave party year ago lifetime n...  Negative\n",
       "7  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 79,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ngjoZsf1_Nhk",
    "outputId": "f4faedde-ebea-4669-c4ae-8b3167a116da"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...        -1\n",
       "2                     mean like romney cheat primary        -1\n",
       "3          mitt romney still believe black president        -1\n",
       "4  romney tax plan deserve nd look secret one dif...        -1\n",
       "5        hope romney debate prepped people last time         1"
      ]
     },
     "execution_count": 80,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX_1EBR3_Nhm"
   },
   "outputs": [],
   "source": [
    "romney_train_pr['Sentiment'] = romney_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PT1k9oGj_Nho",
    "outputId": "432925e6-6ad6-4ed8-ac06-4e63bf4f003d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...  Negative\n",
       "2                     mean like romney cheat primary  Negative\n",
       "3          mitt romney still believe black president  Negative\n",
       "4  romney tax plan deserve nd look secret one dif...  Negative\n",
       "5        hope romney debate prepped people last time  Positive"
      ]
     },
     "execution_count": 82,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBTlpT5I_Nhp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8nyt2ltYpZd5"
   },
   "source": [
    "\n",
    "## Building Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dugwt7lTaCiA"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr.to_csv(os.path.join(data_path, 'Obama Training1 Data.csv'))\n",
    "# romney_train_pr.to_csv(os.path.join(data_path, 'Romney Training1 Data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96eAfNsDaiVd"
   },
   "outputs": [],
   "source": [
    "# data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "kljiyU8TaCiF"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2])\n",
    "romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mcLnGHdPaCiI",
    "outputId": "a08bc803-0a20-4a28-f538-77929db4d5df"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "1  obama debate cracker as cracker tonight tune t...  Positive\n",
       "2  miss point afraid understand big picture dont ...   Neutral\n",
       "3  raise democrat leave party year ago lifetime n...  Negative\n",
       "4  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "XORvLV2iaCiM"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = obama_train_pr_df.dropna()\n",
    "romney_train_pr_df = romney_train_pr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mXJwrtOtaCiQ"
   },
   "outputs": [],
   "source": [
    "def tf_idf(x):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(x)\n",
    "    #print(tfidf_matrix)\n",
    "    x1 = tfidf_matrix.toarray()\n",
    "#     print(x1)\n",
    "    return x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "hM-Q2KTBaCiU"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_tfidf = tf_idf(obama_train_pr_df['Doc Text'])\n",
    "romney_train_pr_tfidf = tf_idf(romney_train_pr_df['Doc Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NimCgs87eUzF",
    "outputId": "c6ecaa8e-c556-4bb8-d907-fe29294689b7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5470, 6945)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWPQASu9eUzG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm66hwR6eUzJ"
   },
   "source": [
    "### Cross Valid Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "a6F0wDNPeUzJ"
   },
   "outputs": [],
   "source": [
    "def cross_valid(X,y,model):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(X_train1, temp)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Valid func with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_valid_PCA(X,y,model, n):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    \n",
    "    pca = PCA(n_components=n)\n",
    "    \n",
    "    \n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "        # print(1)\n",
    "        X_train1, X_val, y_train1, y_val = X[train_index], X[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        x_pca1 = pca.fit_transform(X_train1)\n",
    "        temp = y_train1.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        model = model.fit(X_train1, temp)\n",
    "\n",
    "        y_pred = model.predict(X_val)\n",
    "        temp_y_val = y_val.apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "        f1_temp = f1_score(temp_y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[0])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[2])\n",
    "\n",
    "        acc.append(round(accuracy_score(temp_y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rmhRyNUshSYd"
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "-PbiFcztf_NY"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wP6ste5OD_9a",
    "outputId": "2a7a71e4-4f73-4130-ec7d-c6c80e0f44e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # clf = SVC(kernel=\"rbf\", gamma=1, class_weight='balanced')\n",
    "# #     error = 1-(cross_val_score(clf, x_pca1, temp, cv = 5, scoring = 'f1_macro'))\n",
    "# #     f1 = mean(error) + stdev(error) \n",
    "# #print(f1)\n",
    "\n",
    "\n",
    "# clf = LogisticRegression(class_weight='balanced')\n",
    "# accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "# eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "# eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "# # print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "\n",
    "# print(eval_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "id": "JoDEYnJDhX6a",
    "outputId": "4721cafd-2b1b-47fd-a178-a47f10f631cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.566, 'f1_pos': 0.583, 'f1_neu': 0.516, 'f1_neg': 0.599, 'eval_score': 0.583}\n",
      "{'accuracy': 0.565, 'f1_pos': 0.579, 'f1_neu': 0.517, 'f1_neg': 0.599, 'eval_score': 0.581}\n",
      "{'accuracy': 0.566, 'f1_pos': 0.575, 'f1_neu': 0.519, 'f1_neg': 0.601, 'eval_score': 0.581}\n",
      "{'accuracy': 0.585, 'f1_pos': 0.601, 'f1_neu': 0.54, 'f1_neg': 0.614, 'eval_score': 0.6}\n",
      "{'accuracy': 0.576, 'f1_pos': 0.598, 'f1_neu': 0.525, 'f1_neg': 0.605, 'eval_score': 0.593}\n",
      "{'accuracy': 0.569, 'f1_pos': 0.589, 'f1_neu': 0.526, 'f1_neg': 0.594, 'eval_score': 0.584}\n",
      "{'accuracy': 0.571, 'f1_pos': 0.589, 'f1_neu': 0.52, 'f1_neg': 0.606, 'eval_score': 0.589}\n",
      "{'accuracy': 0.568, 'f1_pos': 0.584, 'f1_neu': 0.526, 'f1_neg': 0.594, 'eval_score': 0.582}\n",
      "{'accuracy': 0.589, 'f1_pos': 0.604, 'f1_neu': 0.539, 'f1_neg': 0.623, 'eval_score': 0.605}\n",
      "{'accuracy': 0.578, 'f1_pos': 0.588, 'f1_neu': 0.535, 'f1_neg': 0.612, 'eval_score': 0.593}\n",
      "100%|██████████| 10/10 [22:16<00:00, 133.68s/trial, best loss: -0.605]\n",
      "{'C': 0.5774084451499195}\n"
     ]
    }
   ],
   "source": [
    "# With Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR(args):\n",
    "\n",
    "    C = args['C']\n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,10),\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 3.2792115779089426, 'n': 6}                     \n",
      "{'accuracy': 0.576, 'f1_pos': 0.593, 'f1_neu': 0.521, 'f1_neg': 0.613, 'eval_score': 0.594}\n",
      "{'C': 0.8713058148642638, 'n': 7}                                    \n",
      "{'accuracy': 0.581, 'f1_pos': 0.597, 'f1_neu': 0.534, 'f1_neg': 0.613, 'eval_score': 0.597}\n",
      "{'C': 7.170176595228483, 'n': 7}                                     \n",
      "{'accuracy': 0.571, 'f1_pos': 0.585, 'f1_neu': 0.518, 'f1_neg': 0.608, 'eval_score': 0.588}\n",
      "{'C': 1.0224592220990114, 'n': 5}                                    \n",
      "{'accuracy': 0.59, 'f1_pos': 0.609, 'f1_neu': 0.535, 'f1_neg': 0.627, 'eval_score': 0.609}\n",
      "{'C': 2.3663156354792845, 'n': 7}                                    \n",
      "{'accuracy': 0.581, 'f1_pos': 0.601, 'f1_neu': 0.535, 'f1_neg': 0.61, 'eval_score': 0.597}\n",
      "{'C': 6.069194911250428, 'n': 5}                                     \n",
      "{'accuracy': 0.569, 'f1_pos': 0.583, 'f1_neu': 0.523, 'f1_neg': 0.6, 'eval_score': 0.584}\n",
      "{'C': 3.9398854245370374, 'n': 8}                                    \n",
      "{'accuracy': 0.582, 'f1_pos': 0.605, 'f1_neu': 0.535, 'f1_neg': 0.607, 'eval_score': 0.598}\n",
      "{'C': 6.155850029577314, 'n': 9}                                     \n",
      "{'accuracy': 0.568, 'f1_pos': 0.581, 'f1_neu': 0.52, 'f1_neg': 0.604, 'eval_score': 0.584}\n",
      "{'C': 4.990489122517268, 'n': 9}                                     \n",
      "{'accuracy': 0.582, 'f1_pos': 0.593, 'f1_neu': 0.537, 'f1_neg': 0.616, 'eval_score': 0.597}\n",
      "{'C': 9.88492392568257, 'n': 3}                                      \n",
      "{'accuracy': 0.563, 'f1_pos': 0.581, 'f1_neu': 0.513, 'f1_neg': 0.595, 'eval_score': 0.58}\n",
      "100%|██████████| 10/10 [25:09<00:00, 150.99s/trial, best loss: -0.609]\n",
      "{'C': 1.0224592220990114, 'n': 2}\n"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and LR\n",
    "\n",
    "def objective_func_LR_PCA(args):\n",
    "\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    \n",
    "    # penalty = args['penalty']\n",
    "    # multi_class = args['multi_class']\n",
    "    # l1_ratio = args['l1_ratio']\n",
    "\n",
    "\n",
    "    clf = LogisticRegression(C= C,class_weight='balanced',n_jobs =-1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    par_dict = {'C': C, 'n': n}\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(par_dict)\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0,10),\n",
    "         'n': hp.choice('n', np.arange(3,10, step =1))\n",
    "        # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "        # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "        #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "        }                        \n",
    "                                \n",
    "                                \n",
    "best_classifier_LR_PCA = fmin(objective_func_LR_PCA, space, algo=tpe.suggest, max_evals=10)\n",
    "print(best_classifier_LR_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # With Bayesian Optimization, different class weights and LR\n",
    "\n",
    "# def objective_func_LR(args):\n",
    "\n",
    "#     C = args['C']\n",
    "#     balance = args['balance']\n",
    "#     # multi_class = args['multi_class']\n",
    "#     # l1_ratio = args['l1_ratio']\n",
    "\n",
    " \n",
    "#     clf = LogisticRegression(C= C,class_weight=balance,n_jobs =-1)\n",
    "#     accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "#     eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "#     eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "#     print(eval_dict)\n",
    "#     return -(eval_score)\n",
    "\n",
    "\n",
    "# space = {'C': hp.uniform('C', 0,10),\n",
    "#          'balance': hp.choice('balance', [{1:3, 2:1, 3:2}, {1:15, 2:1, 3:10}, {1:20, 2:5, 3:10}, {1:100, 2:10, 3:100}])\n",
    "#         # 'penalty': hp.choice('penalty', ['l1', 'l2']), \n",
    "#         # 'multi_class': hp.choice('multi_class',['ovr', 'multinomial']),\n",
    "\n",
    "#         #  'l1_ratio' : hp.uniform('l1_ratio',0,1)\n",
    "#         }                        \n",
    "                                \n",
    "                                \n",
    "# best_classifier_LR = fmin(objective_func_LR, space, algo=tpe.suggest, max_evals=20)\n",
    "# print(best_classifier_LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58UghlAQeUzP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 10.75099542755227, 'n': 4, 'degree': 3, 'kernel': 'poly'}\n",
      "  0%|          | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "# With PCA, Bayesian Optimization, and SVM\n",
    "\n",
    "def objective_func_SVM_PCA(args):\n",
    "    C = args['C']\n",
    "    n = args['n']\n",
    "    kernel = args['kernel']\n",
    "#     gamma = args['gamma']\n",
    "    degree = args['degree']\n",
    "\n",
    "    par_dict = {'C': C, 'n': n, 'degree': degree, 'kernel': kernel}\n",
    "    print(par_dict)\n",
    " \n",
    "    clf = SVC(C= C, kernel = kernel, degree=degree,class_weight='balanced')\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "\n",
    "space = {'C': hp.uniform('C', 0.1,20),\n",
    "         'n': hp.choice('n', np.arange(3,6, step =1)),\n",
    "        'kernel': hp.choice('kernel', ['poly', 'rbf']), \n",
    "#         'gamma': hp.choice('gamma',range(1,4)),\n",
    "         'degree' : hp.choice('degree',range(1,4))}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_SVM_PCA = fmin(objective_func_SVM_PCA, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_classifier_SVM_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WIth PCA , Bayesian Optimization and KNN\n",
    "def objective_func_KNN_PCA(args):\n",
    "    n_neighbors = args['n_neighbors']\n",
    "    metric = args['metric']\n",
    "    n = args['n']    \n",
    "\n",
    "    par_dict = {'n_neighbors': n_neighbors, 'n': n, 'metric': metric}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=n_neighbors, metric=metric, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_neighbors': hp.choice('n_neighbors',np.arange(1,10, step =1)),\n",
    "        'metric':hp.choice('metric', [\"euclidean\",\"manhattan\"]),\n",
    "        'n': hp.choice('n', np.arange(3,6, step =1))}\n",
    "\n",
    "best_classifier_KNN_PCA = fmin(objective_func_KNN_PCA, space, algo=tpe.suggest, max_evals=15)\n",
    "print(best_classifier_KNN_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vy-z9cSEeUzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GaussianNB()\n",
    "accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "print(eval_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PCA and Naive bayes\n",
    "def objective_func_NB_PCA(args):\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = {'n': n}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n': hp.choice('n', np.arange(3,10, step =1))}\n",
    "\n",
    "best_classifier_NB_PCA = fmin(objective_func_NB_PCA, space, algo=tpe.suggest, max_evals=15)\n",
    "print(best_classifier_NB_PCA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9uqUrTlhaCiX",
    "outputId": "46220e5f-4754-4177-f6df-fc32cf0fa883"
   },
   "source": [
    "## Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnu2mBLwaCie"
   },
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-6Q8dRwaCig",
    "outputId": "b42504a9-9d98-49fd-e286-297bb9fcd020"
   },
   "outputs": [],
   "source": [
    "# With Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "\n",
    "    par_dict = {'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF = fmin(objective_func_RF, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AI0VHzOleUzi",
    "outputId": "de0a927c-95c9-439d-d3ff-06cd57c0aab6"
   },
   "outputs": [],
   "source": [
    "# With PCA,  Bayesian Optimization, and Random Forest\n",
    "\n",
    "def objective_func_RF_PCA(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    min_samples_split = args['min_samples_split']\n",
    "    min_samples_leaf = args['min_samples_leaf']\n",
    "    n = args['n']\n",
    "\n",
    "    par_dict = { 'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'min_samples_split': min_samples_split, 'min_samples_leaf': min_samples_leaf}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "         'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "#         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "#         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "        'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "         'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1)),\n",
    "        'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_RF_PCA = fmin(objective_func_RF_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_RF_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TKP0NEj6eUzk"
   },
   "outputs": [],
   "source": [
    "# # WITH LDA\n",
    "# from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "# from math import sqrt\n",
    "# lda = LinearDiscriminantAnalysis(n_components = None)\n",
    "# x_lda = lda.fit_transform(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NQsPlUENeUzl",
    "outputId": "7a8a84e7-88c0-4bab-bf29-611d0cc98d98"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "#     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda, temp, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MzCfxZ1GeUzn"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5Y5ahpseUzp"
   },
   "outputs": [],
   "source": [
    "# obama_train2_pr_tfidf, obama_val_pr_tfidf, obama_train2_pr_class, obama_val_pr_class = train_test_split(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'], test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4SUBRUJ8eUzq"
   },
   "outputs": [],
   "source": [
    "# x_lda2 = lda.fit_transform(obama_train2_pr_tfidf, obama_train2_pr_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6cz5e2KzeUzs",
    "outputId": "9d0f0738-440f-4c25-dcf2-79fc3f0dce22"
   },
   "outputs": [],
   "source": [
    "# # With LDA, Bayesian Optimization, and Random Forest\n",
    "\n",
    "# def objective_func(args):\n",
    "#     n_estimators = args['n_estimators']\n",
    "#     max_depth = args['max_depth']\n",
    "#     min_samples_split = args['min_samples_split']\n",
    "#     min_samples_leaf = args['min_samples_leaf']\n",
    "# #     pca = PCA(n_components=args['n'])\n",
    "# #     x_pca1 = pca.fit_transform(obama_train_pr_tfidf)\n",
    "\n",
    " \n",
    "#     clf = RandomForestClassifier(n_estimators = n_estimators, max_depth = max_depth, min_samples_leaf=min_samples_leaf, min_samples_split=min_samples_split, n_jobs = -1)\n",
    "    \n",
    "# #     temp = obama_train_pr_df['Sentiment'].apply(lambda x: 1 if x == 'Positive' else (3 if x == 'Negative' else 2))\n",
    "#     temp_f1 = cross_val_score(clf, x_lda2, obama_train2_pr_class, cv = 5, scoring = 'f1_macro')\n",
    "#     f1 = mean(temp_f1)\n",
    "#     return -(f1)\n",
    "# space = {'n_estimators': hp.choice('n_estimators', np.arange(20,101, step =1)), \n",
    "#          'max_depth': hp.choice('max_depth', np.arange(20,101, step =1)), \n",
    "# #         'criterion': hp.choice('criterion',['gini', 'entropy']),\n",
    "# #         'max_features': hp.choice('max_features', ['auto', 'sqrt']),\n",
    "#         'min_samples_split': hp.choice('min_samples_split', np.arange(2,12, step =1)),\n",
    "#          'min_samples_leaf' : hp.choice('min_samples_leaf',np.arange(1,12, step =1))\n",
    "# #         'n': hp.choice('n', np.arange(3,20, step =1))\n",
    "#         }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "# best_classifier = fmin(objective_func, space, algo=tpe.suggest, max_evals=30)\n",
    "# print(best_classifier)# With Bayesian Optimization, and Random Forest\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pm0j2MPOeUzt"
   },
   "outputs": [],
   "source": [
    "# x_val_lda = lda.transform(obama_val_pr_tfidf)\n",
    "# x_val_lda_df = pd.DataFrame(data = x_val_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GkTc0libeUzy",
    "outputId": "e52d6004-e6f8-46bd-863b-90bb91bba6be"
   },
   "outputs": [],
   "source": [
    "# c = lambda b : 1 if b == 0 else b\n",
    "# d = lambda b : 2 if b <= 1 else b\n",
    "# bc = RandomForestClassifier(max_depth = c(best_classifier['max_depth']), min_samples_leaf = c(best_classifier['min_samples_leaf']), min_samples_split = d(best_classifier['min_samples_split'])\n",
    "#                             ,n_estimators = best_classifier['n_estimators'])\n",
    "\n",
    "# bc.fit(x_lda2, obama_train2_pr_class)\n",
    "\n",
    "# y_pred = bc.predict(x_val_lda)\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDPpRJ91eUz0",
    "outputId": "7338ec62-8947-4d33-a1ef-f2e2eee8843d"
   },
   "outputs": [],
   "source": [
    "# lda_clf = lda.fit(obama_train2_pr_tfidf, obama_train2_pr_class)\n",
    "# y_pred_lda_clf = lda_clf.predict(obama_val_pr_tfidf)\n",
    "# # y_pred_lda_clf\n",
    "\n",
    "# accuracy_score(obama_val_pr_class, y_pred_lda_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBsD7WYFeUz4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UB056v0eUz6"
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgbm\n",
    "from lightgbm.sklearn import LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "    subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "\n",
    "    par_dict = {'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': subsample, 'min_split_gain': min_split_gain}\n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(n_estimators = n_estimators, max_depth = max_depth, subsample=subsample, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'n_estimators':np.arange(50,201, step =1),\n",
    "    'learning_rate': np.linspace(0.1,0.7, num=7), \n",
    "    'max_depth': np.arange(5,40, step =1),   \n",
    "     'subsample': np.linspace(0.5,0.8, num=4),\n",
    "    'min_split_gain': np.arange(0,6, step =1)}\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB = fmin(objective_func_LGB, space, algo=tpe.suggest, max_evals=25)\n",
    "print(best_classifier_LGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With PCA, Bayesian Optimization, and Light GB\n",
    "\n",
    "def objective_func_LGB_PCA(args):\n",
    "    n_estimators = args['n_estimators']\n",
    "    max_depth = args['max_depth']\n",
    "    learning_rate = args['learning_rate']\n",
    "    subsample = args['subsample']\n",
    "    min_split_gain = args['min_split_gain']\n",
    "    n = args['n']\n",
    "    \n",
    "    par_dict = {'n': n, 'n_estimators': n_estimators, 'max_depth': max_depth, 'learning_rate': learning_rate, 'subsample': subsample, 'min_split_gain': min_split_gain} \n",
    "    print(par_dict)\n",
    "    \n",
    "    clf = LGBMClassifier(n_estimators = n_estimators, max_depth = max_depth, subsample=subsample, learning_rate=learning_rate, min_split_gain = min_split_gain)\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_PCA(obama_train_pr_tfidf,obama_train_pr_df['Sentiment'], clf, n) \n",
    "    eval_score = round(mean([accuracy, f1_Positive, f1_Negative]),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'eval_score': eval_score}\n",
    "\n",
    "    print(eval_dict)\n",
    "    print('\\n')    \n",
    "    return -(eval_score)\n",
    "\n",
    "space = {\n",
    "    'n_estimators':np.arange(50,201, step =1),\n",
    "    'learning_rate': np.linspace(0.1,0.7, num=7), \n",
    "    'max_depth': np.arange(5,40, step =1),   \n",
    "     'subsample': np.linspace(0.5,0.8, num=4),\n",
    "    'min_split_gain': np.arange(0,6, step =1),\n",
    "    'n': hp.choice('n', np.arange(3, 6, step =1))\n",
    "    }\n",
    "                                \n",
    "                                \n",
    "                                \n",
    "best_classifier_LGB_PCA = fmin(objective_func_LGB_PCA, space, algo=tpe.suggest, max_evals=30)\n",
    "print(best_classifier_LGB_PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tt3WGISKa9_U"
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "romney_train_pr_tfidf_smote, romney_train_pr_class = sm.fit_sample(romney_train_pr_tfidf, romney_train_pr_df['Sentiment'] )\n",
    "\n",
    "\n",
    "sm = SMOTE(sampling_strategy='not majority',random_state=42)\n",
    "obama_train_pr_tfidf_smote, obama_train_pr_class = sm.fit_sample(obama_train_pr_tfidf, obama_train_pr_df['Sentiment'])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_ML.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
