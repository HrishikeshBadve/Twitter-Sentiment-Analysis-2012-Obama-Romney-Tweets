{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "78PhOL0jpZdY",
    "outputId": "5ef6feb8-a6f7-48dc-baf1-5109902d3f24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEM1ZS6vpu6b",
    "outputId": "df0ec57a-c45b-49e9-ef06-43d5ab03195c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Research Project\n"
     ]
    }
   ],
   "source": [
    "# %cd /content/drive/My Drive/Research Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QsHo75fupasE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "# import src\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4ePRqI5M3qPi",
    "outputId": "7166afbe-e2ef-45ca-b8a9-d3552fd9274d"
   },
   "outputs": [],
   "source": [
    "# pip install -U mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "hgktrAVlpZdb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from statistics import mean, stdev, median, mode\n",
    "# With PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "# KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from mittens import GloVe, Mittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NO6oNPy7px_h",
    "outputId": "2d590ef1-c608-4dd5-dbe2-119dc543ea04"
   },
   "outputs": [],
   "source": [
    "# pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "qXIBsqXjpZdd"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "# nltk.download()\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "import random\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from autocorrect import Speller\n",
    "# from pycontractions import Contractions\n",
    "\n",
    "# from spellchecker import SpellChecker\n",
    "\n",
    "import re\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "from hyperopt import fmin, tpe, hp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BALelbZyNU02",
    "outputId": "b7e47a1f-4fb1-47a0-e855-f10034934aa2"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lim7c1rpZdf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "w8gGSNnE_Ngv"
   },
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa\n",
    "\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten, Dropout, MaxPool1D\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, GRU, Bidirectional\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "IUnMZe9zpZdh"
   },
   "outputs": [],
   "source": [
    "# nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "CCe7BteZpZdj"
   },
   "outputs": [],
   "source": [
    "# # Load your favorite word2vec model\n",
    "# cont = Contractions('https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz')\n",
    "# text = \"we're\"\n",
    "# text = list(cont.expand_texts([text], precise=True))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "jBMsohUhpZdl"
   },
   "outputs": [],
   "source": [
    "def conv_dataframes(pos_path, neg_path):\n",
    "    with open(pos_path,'r',encoding='latin1') as f:\n",
    "        data_p = f.readlines()\n",
    "#     print(data_p[11])\n",
    "    with open(neg_path,'r',encoding='latin1') as f:\n",
    "        data_n = f.readlines()\n",
    "    pos_data = shuffle(pd.DataFrame(data_p, columns = [\"Doc Text\"]))\n",
    "#     pos_data['Sentiment'] = 1\n",
    "#     pos_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    neg_data = shuffle(pd.DataFrame(data_n, columns = [\"Doc Text\"]))\n",
    "#     neg_data['Sentiment'] = -1\n",
    "#     neg_data.columns = [\"Doc Text\", \"Sentiment\"]\n",
    "    return pos_data, neg_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3hr-TspWpZdn"
   },
   "source": [
    "## The code for pos tagging and lemmatize sentence is fron the following link:\n",
    " ### https://medium.com/@gaurav5430/using-nltk-for-lemmatizing-sentences-c1bfff963258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "CmfT_d2dpZdn"
   },
   "outputs": [],
   "source": [
    "# function to convert nltk tag to wordnet tag\n",
    "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:          \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "UJso8xCYpZdq"
   },
   "outputs": [],
   "source": [
    "def lemmatize_sentence(sentence):\n",
    "    #tokenize the sentence and find the POS tag for each token\n",
    "    nltk_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))  \n",
    "    #tuple of (token, wordnet_tag)\n",
    "    wordnet_tagged = map(lambda x: (x[0], nltk_tag_to_wordnet_tag(x[1])), nltk_tagged)\n",
    "#     print(wordnet_tagged)\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in wordnet_tagged:\n",
    "        if tag is None:\n",
    "            #if there is no available tag, append the token as is\n",
    "            lemmatized_sentence.append(word)\n",
    "        else:        \n",
    "            #else use the tag to lemmatize the token\n",
    "            lemmatized_sentence.append(lemmatizer.lemmatize(word, tag))\n",
    "#         if tag is not None:\n",
    "#             lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "\n",
    "    return \" \".join(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "eioAEw_bpZdr"
   },
   "outputs": [],
   "source": [
    "# print(lemmatizer.lemmatize(\"I am loving it\")) #I am loving it\n",
    "# print(lemmatizer.lemmatize(\"loving\")) #loving\n",
    "# print(lemmatizer.lemmatize(\"loving\", \"v\")) #love\n",
    "# print(lemmatize_sentence(\"I am loving it\")) #I be love it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "4lDD2sOAM2m6",
    "outputId": "c7d4445f-8b36-4603-b4bc-989a85c5d74f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Goodd'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def replaceTwoOrMore(s):\n",
    "    #look for 2 or more repetitions of character\n",
    "    pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "    return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "replaceTwoOrMore('Goooddd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "HP97V6pkO85M",
    "outputId": "69234644-93c0-4d85-b52f-2679bc69a77f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'haha'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"\\b(?:a{0,2}h{1,2}a{0,2}){2,}h?\\b\", \"haha\", \"hahahahahahahhhahaha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fGN7mzmrP47t",
    "outputId": "c0e27ade-dac5-4bc8-ef5c-cf13ff74457d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lol'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(r\"\\b(?:o{0,2}l{1,2}o{0,2}){2,}l?\\b\", \"lol\", \"lolololl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SZg8bpzDSHdq",
    "outputId": "c1892b82-5309-45c6-859b-5c879943896b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#Obama'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str = '#Obama'\n",
    "\n",
    "str.replace('\\S+', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "5mqGu-vhAZwm"
   },
   "outputs": [],
   "source": [
    "def text_preprocessing(data):\n",
    "    \n",
    "    # This method replaces two or more consecutive letters with the same character to something shorter. For example, gooooooood becomes good.\n",
    "    def replaceTwoOrMore(s):\n",
    "        #look for 2 or more repetitions of character\n",
    "        pattern = re.compile(r\"(.)\\1{1,}\", re.DOTALL) \n",
    "        return pattern.sub(r\"\\1\\1\", s)\n",
    "\n",
    "    # This method converts camel cased words into space delimited words.\n",
    "    # For example: ThisIsASentence will be changed to This Is A Sentence\n",
    "    def convertCamelCase(word):\n",
    "        return re.sub(\"([a-z])([A-Z])\",\"\\g<1> \\g<2>\",word)\n",
    "\n",
    "    # Read a flat file containing some abbreviations and their expansions in pipe separated format\n",
    "    # Use these abbreviations to replace text in the tweets as part of Preprocessing\n",
    "    \n",
    "    def readAbbrFile(abb_path):\n",
    "        global abbr_dict\n",
    "        abbr_dict ={}\n",
    "        f = open(abb_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            tmp = i.split('|')\n",
    "            abbr_dict[tmp[0]] = tmp[1]\n",
    "\n",
    "        return abbr_dict\n",
    "  \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replaceAbbr(s):\n",
    "#         temp =[]\n",
    "#         for word in s.split():\n",
    "#             if word.lower() in abbr_dict.keys():\n",
    "# #                 print('t')                \n",
    "#                 temp.append(abbr_dict[word.lower()])\n",
    "#             else:\n",
    "#                 temp.append(word)\n",
    "        temp = \" \".join([abbr_dict[word.lower()] if word.lower() in abbr_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end\n",
    "\n",
    "    def readcontractions(contra_path):\n",
    "        global contra_dict\n",
    "        contra_dict ={}\n",
    "        f = open(contra_path)\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        for i in lines:\n",
    "            try: \n",
    "                tmp = i.replace('\"', '').replace(',', '').replace('\\n', ' ').split(':')\n",
    "                contra_dict[tmp[0]] = tmp[1]\n",
    "            except:\n",
    "                print(tmp)\n",
    "                print(z)\n",
    "\n",
    "        return contra_dict\n",
    "    \n",
    "    # This function checks the dictionary containing abbreviations and their meanings as (key,value) pairs\n",
    "    # and replaces the key with the corresponding value\n",
    "    def replacecontra(s):\n",
    "        temp = \" \".join([contra_dict[word.lower()] if word.lower() in contra_dict.keys() else word for word in s.split()])\n",
    "        return temp\n",
    "    #end    \n",
    "    \n",
    "\n",
    "    abb_path = os.path.join(data_path,\"abbrevations.txt\")\n",
    "    abbr_dict = readAbbrFile(abb_path)\n",
    "    \n",
    "    contra_path = os.path.join(data_path,\"contractions.txt\")\n",
    "    contra_dict = readcontractions(contra_path)\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[^\\x00-\\x7f]', ' ') # remove non ascii characters\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: x.lower())\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceAbbr(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replacecontra(x))\n",
    "\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*',' ') #remove URL\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('(\\s)@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('@\\w+', ' ') #remove usernames\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('<[^<]+?>', ' ') #remove HTML tags\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[<>!#@$:.,%\\?-]+', ' ') #remove punctuation and special characters\n",
    "    # data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('(#\\S+)', ' ') # to remove words with hashtags #Obama\n",
    "\n",
    "\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\d+', ' ') # removing the words with more than 1 digit\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n\\n', ' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\n', ' ') # removing new line characters\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('[^\\w\\s]',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+[a-zA-Z]\\s+',' ')\n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\^[a-zA-Z]\\s+',' ')\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].str.replace('\\s+',' ')  \n",
    "\n",
    "    \n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: replaceTwoOrMore(x))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: convertCamelCase(x))\n",
    "\n",
    "    # Remove stop words from text\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([word for word in x.split() if word not in stop_list]))\n",
    "\n",
    "\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([re.sub(r\"\\b(?:a{0,2}h{1,2}a{0,2}){2,}h?\\b\", \"haha\", word) for word in x.split()])) # REPLACING HAHAHAH WITH HAHA\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([re.sub(r\"\\b(?:o{0,2}l{1,2}o{0,2}){2,}l?\\b\", \"lol\", word) for word in x.split()])) # REPLACING lOLOLOL WITH LOL\n",
    "                                                  \n",
    "#     data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: \" \".join([lemmatizer.lemmatize(y) for y in x.split()]))\n",
    "    data[\"Doc Text\"] = data[\"Doc Text\"].apply(lambda x: lemmatize_sentence(x))\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "7duAi2av_Ng-",
    "outputId": "7b01f659-6b9f-49ff-9bb6-a115373641d8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'k k  k'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "st = 'k k  k'\n",
    "st.replace('\\s+','')\n",
    "st.replace('\\n\\n', '')\n",
    "st.replace('[^\\w\\s]','')\n",
    "st.replace('\\s+[a-zA-Z]\\s+','')\n",
    "st.replace('\\^[a-zA-Z]\\s+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "DgyAcMZRpZdv"
   },
   "outputs": [],
   "source": [
    "def create_vocab(data1, data2):\n",
    "    temp1 = text_preprocessing(data1)\n",
    "    temp2 = text_preprocessing(data2)\n",
    "#     temp_pos2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_pos['Doc Text']])\n",
    "#     temp_neg2 = np.asarray([word_tokenize(re.sub(r\"\\b[a-zA-Z]\\b\", \" \",i)) for i in temp_neg['Doc Text']]) \n",
    "#     temp_pos[\"Doc Text Tokens\"] = temp_pos2 \n",
    "#     temp_neg[\"Doc Text Tokens\"] = temp_neg2\n",
    "    return temp1, temp2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "z2ekzmr7Brl-"
   },
   "outputs": [],
   "source": [
    "data_path = r'/content/drive/My Drive/Research Project/Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YcIgGnBHpZd1"
   },
   "outputs": [],
   "source": [
    "\n",
    "trainFile = os.path.join(data_path,\"training-Obama-Romney-tweets.xlsx\")\n",
    "obama_train_temp = pd.read_excel(trainFile, sheet_name = 'Obama', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "romney_train_temp = pd.read_excel(trainFile, sheet_name = 'Romney', header = None, skiprows =[0,1], usecols= [3,4], names =['Doc Text', 'Sentiment'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Aoo2NCbGpZd3",
    "outputId": "57b3002f-2473-4b60-d3ec-9212db034de2"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Question: If &lt;e&gt;Romney&lt;/e&gt; and &lt;e&gt;Obama&lt;/e&gt; ha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @davewiner Slate: Blame &lt;e&gt;Obama&lt;/e&gt; for fo...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
       "1  Question: If <e>Romney</e> and <e>Obama</e> ha...         2\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
       "3  RT @davewiner Slate: Blame <e>Obama</e> for fo...         2\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...         0"
      ]
     },
     "execution_count": 271,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ESw9o5h7_NhH",
    "outputId": "52096dfd-309d-47c8-8a1d-d66a7a1f53c5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Senior &lt;e&gt;Romney&lt;/e&gt; Advisor Claims &lt;e&gt;Obama&lt;/...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
       "1  Senior <e>Romney</e> Advisor Claims <e>Obama</...         2\n",
       "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
       "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
       "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1"
      ]
     },
     "execution_count": 272,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3fSFxgW_NhI"
   },
   "source": [
    "## Removing datapoints with mixed sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "W-c-9PNk_NhJ",
    "outputId": "505786b5-a97f-45a7-96db-df91392578e1"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kirkpatrick, who wore a baseball cap embroider...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#&lt;e&gt;obama&lt;/e&gt; debates that Cracker Ass Cracker...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@Hollivan @hereistheanswer  Youre missing the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I was raised as a Democrat  left the party yea...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The &lt;e&gt;Obama camp&lt;/e&gt; can't afford to lower ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Kirkpatrick, who wore a baseball cap embroider...         0\n",
       "2  #<e>obama</e> debates that Cracker Ass Cracker...         1\n",
       "4  @Hollivan @hereistheanswer  Youre missing the ...         0\n",
       "6  I was raised as a Democrat  left the party yea...        -1\n",
       "7  The <e>Obama camp</e> can't afford to lower ex...         0"
      ]
     },
     "execution_count": 273,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train = obama_train_temp[obama_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "obama_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "8-cpEizR_NhL",
    "outputId": "3441438f-7138-4c05-bbe8-c9f66d207d5c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Insidious!&lt;e&gt;Mitt Romney&lt;/e&gt;'s Bain Helped Phi...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>.@WardBrenda @shortwave8669 @allanbourdius you...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;e&gt;Mitt Romney&lt;/e&gt; still doesn't &lt;a&gt;believe&lt;/a...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;e&gt;Romney&lt;/e&gt;'s &lt;a&gt;tax plan&lt;/a&gt; deserves a 2nd...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hope &lt;e&gt;Romney&lt;/e&gt; debate prepped w/ the same ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  Insidious!<e>Mitt Romney</e>'s Bain Helped Phi...        -1\n",
       "2  .@WardBrenda @shortwave8669 @allanbourdius you...        -1\n",
       "3  <e>Mitt Romney</e> still doesn't <a>believe</a...        -1\n",
       "4  <e>Romney</e>'s <a>tax plan</a> deserves a 2nd...        -1\n",
       "5  Hope <e>Romney</e> debate prepped w/ the same ...         1"
      ]
     },
     "execution_count": 274,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train = romney_train_temp[romney_train_temp['Sentiment'] .isin((1,-1,0))]\n",
    "romney_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZ0Xu4VZ_NhN"
   },
   "source": [
    "# Dropping missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6WQ-4zw_NhO"
   },
   "outputs": [],
   "source": [
    "obama_train = obama_train.dropna()\n",
    "romney_train = romney_train.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "epYfA3Vg_NhQ"
   },
   "source": [
    "## Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "gW_pnLsm_NhQ",
    "outputId": "96167183-77ea-4a83-cdd5-389c85b2ecf5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    1922\n",
       " 0    1895\n",
       " 1    1653\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 276,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_DA = obama_train['Sentiment'].value_counts()\n",
    "obama_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "RE1FsDeu_NhT",
    "outputId": "5c8bd61a-0e50-4c2b-c8c6-baa34ea4da30"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 277,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = obama_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "id": "T1_gyDU9_NhV",
    "outputId": "f9506f30-a720-49b9-880e-5549bf393e83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1    2893\n",
       " 0    1680\n",
       " 1    1075\n",
       "Name: Sentiment, dtype: int64"
      ]
     },
     "execution_count": 278,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_DA = romney_train['Sentiment'].value_counts()\n",
    "romney_train_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "id": "7Alvh5jk_NhW",
    "outputId": "6991c6c0-1ecc-4f27-a4bf-49ad013918e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 279,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.bar(height = romney_train_DA, x = ['-1', '0', '1'], color = ['r', 'c', 'g'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LagE7x_6_NhY"
   },
   "source": [
    "# Romney data is very imbalanced\n",
    "# So, its better to select the best model with respect to F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "05yRKXM4_NhY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2j9EdPb9_Nha"
   },
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qsh4tjZk_Nhb"
   },
   "outputs": [],
   "source": [
    "# hm_lines = 5331\n",
    "\n",
    "# tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "# spell = Speller(lang='en')\n",
    "# # spell = SpellChecker()\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "stop_list = stopwords.words('english')\n",
    "stop_list.extend(['rt', 'retweet', 'e'])\n",
    "\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZD3LoJ1_Nhe"
   },
   "outputs": [],
   "source": [
    "obama_train_pr, romney_train_pr = create_vocab(obama_train, romney_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "Dt3d1UQV_Nhg",
    "outputId": "0c33fec8-de92-4e4b-f1f8-913d1c7a7c1b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...         0\n",
       "2       obama debate cracker as cracker tonight tune         1\n",
       "4  miss point afraid understand big picture dont ...         0\n",
       "6  raise democrat leave party year ago lifetime n...        -1\n",
       "7  obama camp can not afford low expectation toni...         0"
      ]
     },
     "execution_count": 282,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7dIaA7p2_Nhh"
   },
   "outputs": [],
   "source": [
    "obama_train_pr['Sentiment'] = obama_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "cvFTC2P0_Nhj",
    "outputId": "66e8431e-c51e-4716-d059-f0ecd2df739c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>obama debate cracker as cracker tonight tune</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "2       obama debate cracker as cracker tonight tune  Positive\n",
       "4  miss point afraid understand big picture dont ...   Neutral\n",
       "6  raise democrat leave party year ago lifetime n...  Negative\n",
       "7  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 284,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "XrmRhgvaRRfL",
    "outputId": "b4255f38-890f-4e98-b958-5914db7796e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Doc Text    obama priority k gaydar research security emba...\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 286,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr.iloc[10,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "ngjoZsf1_Nhk",
    "outputId": "b6447eb8-2bbb-4c77-c95d-a5c793d1e9f7"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...        -1\n",
       "2                     mean like romney cheat primary        -1\n",
       "3          mitt romney still believe black president        -1\n",
       "4  romney tax plan deserve nd look secret one dif...        -1\n",
       "5        hope romney debate prepped people last time         1"
      ]
     },
     "execution_count": 246,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BX_1EBR3_Nhm"
   },
   "outputs": [],
   "source": [
    "romney_train_pr['Sentiment'] = romney_train_pr['Sentiment'].apply(lambda x: 'Positive' if x == 1 else ('Negative' if x == -1 else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "PT1k9oGj_Nho",
    "outputId": "a09ce86a-78af-4ba3-a3b0-c33ca6bfb3d5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>insidious mitt romney bain help philip morris ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mean like romney cheat primary</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mitt romney still believe black president</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>romney tax plan deserve nd look secret one dif...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hope romney debate prepped people last time</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  insidious mitt romney bain help philip morris ...  Negative\n",
       "2                     mean like romney cheat primary  Negative\n",
       "3          mitt romney still believe black president  Negative\n",
       "4  romney tax plan deserve nd look secret one dif...  Negative\n",
       "5        hope romney debate prepped people last time  Positive"
      ]
     },
     "execution_count": 248,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "romney_train_pr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cBTlpT5I_Nhp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dugwt7lTaCiA"
   },
   "outputs": [],
   "source": [
    "# obama_train_pr.to_csv(os.path.join(data_path, 'Obama Training1 Data.csv'))\n",
    "# romney_train_pr.to_csv(os.path.join(data_path, 'Romney Training1 Data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "AD9U00CVddNC"
   },
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\kalya\\OneDrive - University of Illinois at Chicago\\!UIC\\!Semesters\\3rd Sem\\CS 583 Data Mining and Text Mining\\Research Project\\Data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "96eAfNsDaiVd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "kljiyU8TaCiF"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = pd.read_csv(os.path.join(data_path, 'Obama Training1 Data.csv'), usecols = [1,2])\n",
    "romney_train_pr_df = pd.read_csv(os.path.join(data_path, 'Romney Training1 Data.csv'), usecols = [1,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "mcLnGHdPaCiI",
    "outputId": "173fe46d-f71f-4d97-f16b-d26cdad70c7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Doc Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>kirkpatrick wear baseball cap embroider obama ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>obama debate cracker as cracker tonight tune t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>miss point afraid understand big picture dont ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>raise democrat leave party year ago lifetime n...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>obama camp can not afford low expectation toni...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Doc Text Sentiment\n",
       "0  kirkpatrick wear baseball cap embroider obama ...   Neutral\n",
       "1  obama debate cracker as cracker tonight tune t...  Positive\n",
       "2  miss point afraid understand big picture dont ...   Neutral\n",
       "3  raise democrat leave party year ago lifetime n...  Negative\n",
       "4  obama camp can not afford low expectation toni...   Neutral"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obama_train_pr_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "XORvLV2iaCiM"
   },
   "outputs": [],
   "source": [
    "obama_train_pr_df = obama_train_pr_df.dropna()\n",
    "romney_train_pr_df = romney_train_pr_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l4qdpS83daOj"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "Nt4Q-LLu_Nh6"
   },
   "outputs": [],
   "source": [
    "def one_hot(data):\n",
    "    data = np.asarray(data)\n",
    "    temp = np.zeros((len(data),3))\n",
    "#     print(data[0])\n",
    "    for i in range(len(temp)):\n",
    "        if data[i] == 'Negative':\n",
    "            temp[i][2] = 1 ## Negative sentiment third neuron\n",
    "        elif data[i] == 'Neutral':\n",
    "            temp[i][1] = 1 ## Neutral sentiment second neuron  \n",
    "        else:\n",
    "            temp[i][0] = 1 ## Positive sentiment first neuron \n",
    "\n",
    "    return temp\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "ETvx1WoKfrfk"
   },
   "outputs": [],
   "source": [
    "def pred(x):\n",
    "    temp = []\n",
    "    for i in x:\n",
    "        m = np.argmax(i)\n",
    "        if m == 0:\n",
    "            temp.append('Positive')\n",
    "        elif m == 1:\n",
    "            temp.append('Neutral')\n",
    "        else:\n",
    "            temp.append('Negative')\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmGshHslpZeH"
   },
   "source": [
    "## Building Glove Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "YU-SxaOZ_Nh9"
   },
   "outputs": [],
   "source": [
    "embeddings = {}\n",
    "with open(os.path.join(data_path,\"glove.twitter.27B.200d.txt\"), 'r', encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = asarray(values[1:], dtype='float32')\n",
    "        embeddings[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or3hmQvdpZeJ"
   },
   "source": [
    "## Embedding Matrix Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "wBG3Kq7EpZeJ"
   },
   "outputs": [],
   "source": [
    "def emb_matrix(t,embeddings, we_dim):\n",
    "    # creating a embedding matrix for the words in training data, which will be used as weight matrix for embedding layer\n",
    "    vocab_size = len(t.word_index) + 1    \n",
    "    embedding_matrix = zeros((vocab_size, we_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kT9ZAXi_NiC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgK6d3Qy_NiF"
   },
   "source": [
    "## Fine tuning the word embeddings of 300 dimensions using mittens library\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqwJmPT-_NiF"
   },
   "source": [
    "## Used the code for finetuning from the following link:\n",
    "### https://towardsdatascience.com/fine-tune-glove-embeddings-using-mittens-89b5f3fe4c39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "HRwhIKRT_NiF"
   },
   "outputs": [],
   "source": [
    "def finetune(training): \n",
    "    training_tokens = [word_tokenize(i) for i in training['Doc Text']]\n",
    "    #training_tokens\n",
    "\n",
    "    oov = [j for i in training_tokens for j in i if j not in embeddings.keys()]\n",
    "    print(len(oov))\n",
    "\n",
    "    corp_vocab = list(set(oov))\n",
    "\n",
    "    cv = CountVectorizer(ngram_range=(1,1), vocabulary=corp_vocab)\n",
    "    trr =''\n",
    "    for i in training_tokens:\n",
    "        for j in i:\n",
    "            trr+= j\n",
    "            trr += ' '\n",
    "\n",
    "    # print(trr)\n",
    "    # print(z)\n",
    "    X = cv.fit_transform([trr])\n",
    "    Xc = (X.T * X)\n",
    "    Xc.setdiag(0)\n",
    "    coocc_ar = Xc.toarray()\n",
    "\n",
    "    mittens_model = Mittens(n=200, max_iter=1000)\n",
    "\n",
    "    new_embeddings = mittens_model.fit(\n",
    "      coocc_ar,\n",
    "      vocab=corp_vocab,\n",
    "      initial_embedding_dict= embeddings)\n",
    "\n",
    "    new_embeddings = dict(zip(corp_vocab, new_embeddings))\n",
    "    return training_tokens, new_embeddings\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MB2Nd-Kr_NiH",
    "outputId": "f67a7ffe-aa55-497d-8a84-5f0438889b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kalya\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1751: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "Iteration 1000: loss: 0.016246378421783447"
     ]
    }
   ],
   "source": [
    "embeddings2= embeddings.copy()\n",
    "\n",
    "training_tokens, new_embeddings = finetune(obama_train_pr_df)\n",
    "embeddings2.update(new_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spGvX_Cg_NiI",
    "outputId": "1034e7f8-712b-4ca7-94cf-1ce609bf1493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "oov2 = [j for i in training_tokens for j in i if j not in embeddings2.keys()]\n",
    "print(len(oov2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ir2jPLOS_NiK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldH6dPIM_NiL"
   },
   "source": [
    "## Custom F1 value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "9Q5Q1HVs_NiM"
   },
   "outputs": [],
   "source": [
    "def f1_value(y_true, y_pred): #taken from old keras source code\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3VHQfx6SpZeN"
   },
   "source": [
    "## Building Vanilla RNN, LSTM, and GRU models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "sNbzJtMWpZeN"
   },
   "outputs": [],
   "source": [
    "def model_vanilla_rnn(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(SimpleRNN(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "D69lMKSLpZeP"
   },
   "outputs": [],
   "source": [
    "def model_lstm(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "tDL6HFQYpZeQ"
   },
   "outputs": [],
   "source": [
    "def model_gru(embedding_matrix, noh,  we_dim, vocab_size, max_length, adam_optimizer, activation, Dropout_rate, kernel_regularizer_coef, activity_regularizer_coef):\n",
    "\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, we_dim, weights=[embedding_matrix], input_length=max_length, trainable = False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(GRU(units = noh, activation = activation, dropout=0.2, kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef))))\n",
    "    model.add(Dropout(Dropout_rate))\n",
    "    model.add(Dense(3, activation='softmax', kernel_regularizer=tf.keras.regularizers.L1(kernel_regularizer_coef), activity_regularizer=tf.keras.regularizers.L2(activity_regularizer_coef)))\n",
    "    model.compile(optimizer=adam_optimizer, loss='categorical_crossentropy', metrics=[f1_value])\n",
    "#     print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "42FDB3zmpZeS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5ZHXpH9_Tg9Q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "ZsQN3txgThFD"
   },
   "outputs": [],
   "source": [
    "obama_train2_pr_df, obama_val_pr_df = train_test_split(obama_train_pr_df, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5oc67yzQ2-Z",
    "outputId": "0b374181-2608-46f3-de3b-6750eb36e2bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4376 entries, 3018 to 939\n",
      "Data columns (total 2 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   Doc Text   4376 non-null   object\n",
      " 1   Sentiment  4376 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 102.6+ KB\n"
     ]
    }
   ],
   "source": [
    "obama_train2_pr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z9AxyCspT10S",
    "outputId": "038f97e2-1608-4b3e-b003-c64c230c704e"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'obama_val_pr_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-a909fb7cdcb6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mobama_val_pr_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'obama_val_pr_df' is not defined"
     ]
    }
   ],
   "source": [
    "obama_val_pr_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YR6ArxHZUA0H",
    "outputId": "16604f19-aa08-4d11-f63c-a3dbff428785"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4376 samples\n",
      "Epoch 1/2\n",
      "4376/4376 - 6s - loss: 6.4114 - f1_value: 0.3803\n",
      "Epoch 2/2\n",
      "4376/4376 - 5s - loss: 3.8594 - f1_value: 0.4854\n",
      "{'f1_pos': 0.551, 'f1_neu': 0.345, 'f1_neg': 0.634}\n"
     ]
    }
   ],
   "source": [
    "max_length = 20\n",
    "epochs = 2\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "we_dim = 200\n",
    "\n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(obama_train2_pr_df['Doc Text'])   \n",
    "encoded_train = tokenise_tf.texts_to_sequences(obama_train2_pr_df['Doc Text'])\n",
    "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n",
    "\n",
    "encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n",
    "validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model = model_vanilla_rnn(embedding_matrix, 300, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "history = model.fit(training_padded, one_hot(obama_train2_pr_df['Sentiment']), epochs=epochs, verbose=2, batch_size=batch_size, shuffle =True)\n",
    "y_pred_temp = model.predict(validation_padded)\n",
    "y_pred = pred(y_pred_temp)\n",
    "f1_list= np.round(f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None),3)\n",
    "accuracy = accuracy_score(obama_val_pr_df['Sentiment'], y_pred)\n",
    "f1_dict = {'f1_pos': f1_list[2], 'f1_neu': f1_list[1], 'f1_neg': f1_list[0]}\n",
    "# print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "print(f1_dict)\n",
    "f1 = mean([accuracy, f1_list[2], f1_list[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Dzh3dE1gU0Yf",
    "outputId": "4d5b15db-2941-4a20-8456-bb03c052e18f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "{'accuracy': 0.338, 'f1_pos': 0.101, 'f1_neu': 0.495, 'f1_neg': 0.0}\n",
      "  7%|         | 1/15 [01:32<21:30, 92.15s/it, best loss: -0.14633333333333334]WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "{'accuracy': 0.551, 'f1_pos': 0.584, 'f1_neu': 0.533, 'f1_neg': 0.542}\n",
      " 13%|        | 2/15 [06:03<31:35, 145.84s/it, best loss: -0.559]             WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "{'accuracy': 0.565, 'f1_pos': 0.598, 'f1_neu': 0.505, 'f1_neg': 0.588}\n",
      " 20%|        | 3/15 [08:32<29:23, 146.93s/it, best loss: -0.5836666666666667]WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "WARNING:tensorflow:Layer gru_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    }
   ],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "    em_dim = args['em_dim']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(obama_train2_pr_df['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(obama_val_pr_df['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_gru(embedding_matrix, em_dim, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "    history = model.fit(training_padded, one_hot(obama_train2_pr_df['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "    y_pred_temp = model.predict(validation_padded)\n",
    "    y_pred = pred(y_pred_temp)\n",
    "    f1_list= np.round(f1_score(obama_val_pr_df['Sentiment'], y_pred, average = None),3)\n",
    "    accuracy = round(accuracy_score(obama_val_pr_df['Sentiment'], y_pred),3)\n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_list[2], 'f1_neu': f1_list[1], 'f1_neg': f1_list[0]}\n",
    "    # print([f1_list[2],f1_list[1],f1_list[0]])\n",
    "    print('\\n')\n",
    "    print(eval_dict)\n",
    "    f1 = mean([accuracy, f1_list[2], f1_list[0]])\n",
    "\n",
    "\n",
    "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
    "#     f1 = history.history['f1_value'][-1]\n",
    "   \n",
    "    return - (f1)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n",
    "         'em_dim': hp.choice('em_dim', [50, 100, 200])\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(obama_train2_pr_df['Doc Text'])                                \n",
    "best_gru = fmin(objective_func, space, algo=tpe.suggest, max_evals=15)\n",
    "print(best_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hnDrYVDC_Nib"
   },
   "source": [
    "## Building models for obama tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLDi0elvjHkC"
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SBp3ntnMpZeX"
   },
   "source": [
    "## Using Hyperopt library to tune the Hyperparameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iD2A19Iy42R1"
   },
   "source": [
    "### Cross Valid Function for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "GTx5aKtXaCkP"
   },
   "outputs": [],
   "source": [
    "def cross_valid_lstm(X,y,epochs,batch_size,max_length, we_dim, learning_rate):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "\n",
    "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        tokenise_tf = Tokenizer()\n",
    "        tokenise_tf.fit_on_texts(X_train1) \n",
    "    \n",
    "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
    "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n",
    "\n",
    "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
    "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model = model_lstm(embedding_matrix, 300, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)        \n",
    "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        y_pred_temp = model.predict(validation_padded)\n",
    "        y_pred = pred(y_pred_temp)\n",
    "\n",
    "        f1_temp = f1_score(y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[2])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[0])\n",
    "\n",
    "        acc.append(round(accuracy_score(y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JECfQFe446Ke"
   },
   "source": [
    "### Cross Valid Function for GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "MHbmJeGS4Uan"
   },
   "outputs": [],
   "source": [
    "def cross_valid_gru(X,y,epochs,batch_size,max_length, em_dim, we_dim, learning_rate):\n",
    "    f1_Positive  =[]\n",
    "    f1_Neutral =[]\n",
    "    f1_Negative =[]\n",
    "    acc =[]\n",
    "    cv = KFold(n_splits=5,shuffle=True)\n",
    "    for train_index, val_index in cv.split(X):\n",
    "    #     print(\"Train Index: \", train_index, \"\\n\")\n",
    "    #     print(\"Test Index: \", test_index)\n",
    "    #     print(X.iloc[train_index])\n",
    "    #     print(f)\n",
    "\n",
    "        X_train1, X_val, y_train1, y_val = X.iloc[train_index], X.iloc[val_index], y.iloc[train_index], y.iloc[val_index]\n",
    "        tokenise_tf = Tokenizer()\n",
    "        tokenise_tf.fit_on_texts(X_train1) \n",
    "    \n",
    "        encoded_train = tokenise_tf.texts_to_sequences(X_train1)\n",
    "        training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2,we_dim)\n",
    "\n",
    "        encoded_validation = tokenise_tf.texts_to_sequences(X_val)\n",
    "        validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "        adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "        model = model_gru(embedding_matrix, em_dim, we_dim, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)        \n",
    "        history = model.fit(training_padded, one_hot(y_train1), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "        y_pred_temp = model.predict(validation_padded)\n",
    "        y_pred = pred(y_pred_temp)\n",
    "\n",
    "        f1_temp = f1_score(y_val, y_pred, average = None)\n",
    "\n",
    "        f1_Positive.append(f1_temp[2])\n",
    "        f1_Neutral.append(f1_temp[1])\n",
    "        f1_Negative.append(f1_temp[0])\n",
    "\n",
    "        acc.append(round(accuracy_score(y_val, y_pred),3))\n",
    "\n",
    "    return round(mean(acc),3), round(mean(f1_Positive),3), round(mean(f1_Neutral),3), round(mean(f1_Negative),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "r6qsr1PKaCkS"
   },
   "outputs": [],
   "source": [
    "# max_length = 8\n",
    "# epochs = 1\n",
    "# batch_size = 64\n",
    "# learning_rate = 0.001\n",
    "# we_dim = 200\n",
    "# em_dim = 200\n",
    "\n",
    "# # accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n",
    "# temp = cross_valid(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n",
    "# temp\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "TitWsucZ6C18"
   },
   "outputs": [],
   "source": [
    "# import warnings\n",
    "# warnings.filterwarnings('ignore');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wa5Tzpa85DTd"
   },
   "source": [
    "### Tuning Hyperparameters of GRU with Cross Valid GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "id": "6PBOzWHwaCkU",
    "outputId": "134aefe2-a325-40eb-fe6c-17e1bd465c56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.425, 'f1_pos': 0.367, 'f1_neu': 0.311, 'f1_neg': 0.403, 'f1_eval': 0.398}\n",
      "{'accuracy': 0.575, 'f1_pos': 0.59, 'f1_neu': 0.493, 'f1_neg': 0.621, 'f1_eval': 0.595}\n",
      "{'accuracy': 0.516, 'f1_pos': 0.522, 'f1_neu': 0.448, 'f1_neg': 0.565, 'f1_eval': 0.534}\n",
      "{'accuracy': 0.555, 'f1_pos': 0.566, 'f1_neu': 0.514, 'f1_neg': 0.577, 'f1_eval': 0.566}\n",
      "{'accuracy': 0.474, 'f1_pos': 0.419, 'f1_neu': 0.398, 'f1_neg': 0.471, 'f1_eval': 0.455}\n",
      "{'accuracy': 0.477, 'f1_pos': 0.413, 'f1_neu': 0.366, 'f1_neg': 0.52, 'f1_eval': 0.47}\n",
      " 30%|       | 6/20 [7:12:29<26:21:45, 6779.00s/trial, best loss: -0.595]"
     ]
    }
   ],
   "source": [
    "def objective_func_GRU(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "    em_dim = args['em_dim']\n",
    "\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_gru(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n",
    "    f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "    print('\\n')\n",
    "    print(eval_dict)\n",
    "\n",
    "    return -(f1)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n",
    "         'em_dim': hp.choice('em_dim', [50, 100, 200])\n",
    "        }                     \n",
    "                                \n",
    "we_dim = 200                               \n",
    "best_GRU_cv = fmin(objective_func_GRU, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_GRU_cv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxq3SGbN5bST"
   },
   "source": [
    "### Tuning Hyperparameters of LSTM with Cross Valid LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jx7bgJXn5eu_"
   },
   "outputs": [],
   "source": [
    "def objective_func_LSTM(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "    em_dim = args['em_dim']\n",
    "\n",
    "    accuracy, f1_Positive, f1_Neutral, f1_Negative = cross_valid_lstm(obama_train_pr_df['Doc Text'],obama_train_pr_df['Sentiment'], epochs,batch_size,max_length, em_dim, we_dim, learning_rate)\n",
    "    f1 = round(mean([accuracy, f1_Positive, f1_Negative]),3)    \n",
    "    eval_dict = {'accuracy': accuracy, 'f1_pos': f1_Positive, 'f1_neu': f1_Neutral, 'f1_neg': f1_Negative, 'f1_eval': f1}\n",
    "    print('\\n')\n",
    "    print(eval_dict)\n",
    "\n",
    "    return -(f1)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,60)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(5,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.1),\n",
    "         'em_dim': hp.choice('em_dim', [50, 100, 200])\n",
    "        }                       \n",
    "                                \n",
    "we_dim = 200                                \n",
    "best_LSTM_cv = fmin(objective_func_LSTM, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_LSTM_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhZTgQGmaCkW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "Am8LqqY96ghg",
    "outputId": "dc3efc76-9e4e-40fa-dc3e-de60f686d3fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 20/20 [29:00<00:00, 87.03s/it, best loss: -0.55] \n",
      "{'batch_size': 1, 'epochs': 1, 'learning_rate': 0.0018559993912219612, 'max_length': 3}\n"
     ]
    }
   ],
   "source": [
    "def objective_func(args):\n",
    "    max_length = args['max_length']\n",
    "    batch_size = args['batch_size']\n",
    "    learning_rate = args['learning_rate']\n",
    "    epochs = args['epochs']\n",
    "\n",
    "    \n",
    "    encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
    "    training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "    embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "    encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
    "    validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "    adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "    model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "    history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=0, batch_size=batch_size, shuffle =True)\n",
    "    y_pred_temp = model.predict(validation_padded)\n",
    "    y_pred = pred(y_pred_temp)\n",
    "    f1= f1_score(obama_val['Sentiment'], y_pred, average = 'macro')\n",
    "    # loss, f1 = model.evaluate(validation_padded, one_hot(obama_val['Sentiment']))\n",
    "#     f1 = history.history['f1_value'][-1]\n",
    "   \n",
    "    return -round(f1,2)\n",
    "\n",
    "space = {'max_length': hp.choice('max_length',range(4,20)),  \n",
    "        'batch_size': hp.choice('batch_size', [32, 64, 128]),\n",
    "         'epochs': hp.choice('epochs',range(10,30)), \n",
    "         'learning_rate': hp.uniform('learning_rate', 0,0.002)\n",
    "        }\n",
    "                                \n",
    "                                \n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(obama_train['Doc Text'])                                \n",
    "best_vanilla_rnn = fmin(objective_func, space, algo=tpe.suggest, max_evals=20)\n",
    "print(best_vanilla_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "jfD4GJ3tNQbo",
    "outputId": "676ee689-d0f4-4169-a819-990b7d3e9d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4376 samples\n",
      "Epoch 1/5\n",
      "4376/4376 [==============================] - 2s 431us/sample - loss: 8.9841\n",
      "Epoch 2/5\n",
      "4376/4376 [==============================] - 2s 418us/sample - loss: 6.2251\n",
      "Epoch 3/5\n",
      "4376/4376 [==============================] - 2s 417us/sample - loss: 4.1734\n",
      "Epoch 4/5\n",
      "4376/4376 [==============================] - 2s 424us/sample - loss: 2.7909\n",
      "Epoch 5/5\n",
      "4376/4376 [==============================] - 2s 428us/sample - loss: 2.0136\n",
      "0.49177330895795246\n"
     ]
    }
   ],
   "source": [
    "max_length = 5\n",
    "epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.0007711038235329859\n",
    "\n",
    "tokenise_tf = Tokenizer()\n",
    "tokenise_tf.fit_on_texts(obama_train['Doc Text'])   \n",
    "encoded_train = tokenise_tf.texts_to_sequences(obama_train['Doc Text'])\n",
    "training_padded = pad_sequences(encoded_train, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "embedding_matrix, vocab_size = emb_matrix(tokenise_tf, embeddings2)\n",
    "\n",
    "encoded_validation = tokenise_tf.texts_to_sequences(obama_val['Doc Text'])\n",
    "validation_padded = pad_sequences(encoded_validation, maxlen=max_length, padding='post', truncating = 'pre')\n",
    "\n",
    "adam_optimizer = optimizers.Adam(learning_rate=learning_rate)\n",
    "model = model_vanilla_rnn(embedding_matrix, 300, vocab_size, max_length, adam_optimizer, 'tanh', 0.2, 0.001, 0.001)\n",
    "\n",
    "history = model.fit(training_padded, one_hot(obama_train['Sentiment']), epochs=epochs, verbose=1, batch_size=batch_size, shuffle =True)\n",
    "y_pred_temp = model.predict(validation_padded)\n",
    "y_pred = pred(y_pred_temp)\n",
    "acc= accuracy_score(obama_val['Sentiment'], y_pred)\n",
    "print(acc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6vO3cE8h-Np"
   },
   "source": [
    "# For LSTM rnn, the best hyper parameters are:\n",
    "# 'batch_size': 128, 'epochs': 10, 'learning_rate': 0.002, 'max_length': 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cf5oOkX5h_15"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "illGwOwGiO_y"
   },
   "source": [
    "# For GRU rnn, the best hyper parameters are:\n",
    "# 'batch_size': 64, 'epochs': 7, 'learning_rate': 0.0006, 'max_length': 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTooPqJ3pZej"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmX0YNPUQDjL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter_Sentiment_Analysis_RNN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
